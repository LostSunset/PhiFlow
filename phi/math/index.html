<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>phi.math API documentation</title>
<meta name="description" content="Vectorized operations, tensors with named dimensions …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>phi.math</code></h1>
</header>
<section id="section-intro">
<p>Vectorized operations, tensors with named dimensions.</p>
<p>This package provides a common interface for tensor operations.
Is internally uses NumPy, TensorFlow or PyTorch.</p>
<p>Main classes: <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>, <code><a title="phi.math.DType" href="#phi.math.DType">DType</a></code>, <code>Extrapolation</code>.</p>
<p>The provided operations are not implemented directly.
Instead, they delegate the actual computation to either NumPy, TensorFlow or PyTorch, depending on the configuration.
This allows the user to write simulation code once and have it run with various computation backends.</p>
<p>See the documentation at <a href="https://tum-pbs.github.io/PhiFlow/Math.html">https://tum-pbs.github.io/PhiFlow/Math.html</a></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Vectorized operations, tensors with named dimensions.

This package provides a common interface for tensor operations.
Is internally uses NumPy, TensorFlow or PyTorch.

Main classes: `Tensor`, `Shape`, `DType`, `Extrapolation`.

The provided operations are not implemented directly.
Instead, they delegate the actual computation to either NumPy, TensorFlow or PyTorch, depending on the configuration.
This allows the user to write simulation code once and have it run with various computation backends.

See the documentation at https://tum-pbs.github.io/PhiFlow/Math.html
&#34;&#34;&#34;

from .backend._dtype import DType
from .backend import NUMPY, precision, set_global_precision, get_precision

from ._shape import (
    shape, Shape, EMPTY_SHAPE, DimFilter,
    spatial, channel, batch, instance,
    non_batch, non_spatial, non_instance, non_channel,
    merge_shapes, concat_shapes, IncompatibleShapes
)
from ._magic_ops import unstack, stack, concat, expand, rename_dims, pack_dims, unpack_dim, unpack_dim as unpack_dims, flatten, copy_with
from ._tensors import wrap, tensor, layout, Tensor, Dict, to_dict, from_dict, is_scalar
from .extrapolation import Extrapolation
from ._ops import (
    choose_backend_t as choose_backend, all_available, convert, seed,
    native, numpy, reshaped_native, reshaped_tensor, reshaped_numpy, copy, native_call,
    print_ as print,
    map_ as map,
    zeros, ones, fftfreq, random_normal, random_uniform, meshgrid, linspace, arange as range, range_tensor,  # creation operators (use default backend)
    zeros_like, ones_like,
    pad,
    transpose,  # reshape operations
    divide_no_nan,
    where, nonzero,
    sum_ as sum, finite_sum, mean, finite_mean, std, prod, max_ as max, finite_max, min_ as min, finite_min, any_ as any, all_ as all, quantile, median,  # reduce
    dot,
    abs_ as abs, sign,
    round_ as round, ceil, floor,
    maximum, minimum, clip,
    sqrt, exp, sin, cos, tan, log, log2, log10, sigmoid, arcsin, arccos,
    to_float, to_int32, to_int64, to_complex, imag, real, conjugate,
    degrees,
    boolean_mask,
    is_finite, is_finite as isfinite,
    closest_grid_values, grid_sample, scatter, gather,
    fft, ifft, convolve, cumulative_sum,
    dtype, cast,
    close, assert_close,
    stop_gradient
)
from ._nd import (
    shift,
    vec, const_vec, vec_abs, vec_abs as vec_length, vec_squared, vec_normalize, cross_product, rotate_vector, dim_mask,
    normalize_to,
    l1_loss, l2_loss, frequency_loss,
    spatial_gradient, laplace,
    fourier_laplace, fourier_poisson, abs_square,
    downsample2x, upsample2x, sample_subgrid,
    masked_fill, finite_fill,
)
from ._functional import (
    LinearFunction, jit_compile_linear, jit_compile,
    jacobian, jacobian as gradient, functional_gradient, custom_gradient, print_gradient, hessian,
    solve_linear, solve_nonlinear, minimize, Solve, SolveInfo, ConvergenceException, NotConverged, Diverged, SolveTape,
    map_types, map_s2b, map_i2b,
    iterate,
)


PI = 3.14159265358979323846
&#34;&#34;&#34;Value of π to double precision &#34;&#34;&#34;
pi = PI  # intentionally undocumented, use PI instead. Exists only as an anlog to numpy.pi

INF = float(&#34;inf&#34;)
&#34;&#34;&#34; Floating-point representation of positive infinity. &#34;&#34;&#34;
inf = INF  # intentionally undocumented, use INF instead. Exists only as an anlog to numpy.inf


NAN = float(&#34;nan&#34;)
&#34;&#34;&#34; Floating-point representation of NaN (not a number). &#34;&#34;&#34;
nan = NAN  # intentionally undocumented, use NAN instead. Exists only as an anlog to numpy.nan

NUMPY = NUMPY  # to show up in pdoc
&#34;&#34;&#34;Default backend for NumPy arrays and SciPy objects.&#34;&#34;&#34;

__all__ = [key for key in globals().keys() if not key.startswith(&#39;_&#39;)]

__pdoc__ = {
    &#39;Extrapolation&#39;: False,
    &#39;Shape.__init__&#39;: False,
    &#39;SolveInfo.__init__&#39;: False,
    &#39;TensorDim.__init__&#39;: False,
    &#39;ConvergenceException.__init__&#39;: False,
    &#39;Diverged.__init__&#39;: False,
    &#39;NotConverged.__init__&#39;: False,
    &#39;LinearFunction.__init__&#39;: False,
}</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="phi.math.backend" href="backend/index.html">phi.math.backend</a></code></dt>
<dd>
<div class="desc"><p>Low-level library wrappers for delegating vector operations.</p></div>
</dd>
<dt><code class="name"><a title="phi.math.extrapolation" href="extrapolation.html">phi.math.extrapolation</a></code></dt>
<dd>
<div class="desc"><p>Extrapolations are used for padding tensors and sampling coordinates lying outside the tensor bounds.
Standard extrapolations are listed as global …</p></div>
</dd>
<dt><code class="name"><a title="phi.math.magic" href="magic.html">phi.math.magic</a></code></dt>
<dd>
<div class="desc"><p>Magic methods allow custom classes to be compatible with various functions defined in <code><a title="phi.math" href="#phi.math">phi.math</a></code>, analogous to how implementing <code>__hash__</code> allows …</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="phi.math.INF"><code class="name">var <span class="ident">INF</span></code></dt>
<dd>
<div class="desc"><p>Floating-point representation of positive infinity.</p></div>
</dd>
<dt id="phi.math.NAN"><code class="name">var <span class="ident">NAN</span></code></dt>
<dd>
<div class="desc"><p>Floating-point representation of NaN (not a number).</p></div>
</dd>
<dt id="phi.math.NUMPY"><code class="name">var <span class="ident">NUMPY</span></code></dt>
<dd>
<div class="desc"><p>Default backend for NumPy arrays and SciPy objects.</p></div>
</dd>
<dt id="phi.math.PI"><code class="name">var <span class="ident">PI</span></code></dt>
<dd>
<div class="desc"><p>Value of π to double precision</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="phi.math.abs"><code class="name flex">
<span>def <span class="ident">abs</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>||x||<sub>1</sub></em>.
Complex <code>x</code> result in matching precision float values.</p>
<p><em>Note</em>: The gradient of this operation is undefined for <em>x=0</em>.
TensorFlow and PyTorch return 0 while Jax returns 1.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Absolute value of <code>x</code> of same type as <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def abs_(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34;
    Computes *||x||&lt;sub&gt;1&lt;/sub&gt;*.
    Complex `x` result in matching precision float values.

    *Note*: The gradient of this operation is undefined for *x=0*.
    TensorFlow and PyTorch return 0 while Jax returns 1.

    Args:
        x: `Tensor` or `PhiTreeNode`

    Returns:
        Absolute value of `x` of same type as `x`.
    &#34;&#34;&#34;
    return _backend_op1(x, Backend.abs)</code></pre>
</details>
</dd>
<dt id="phi.math.abs_square"><code class="name flex">
<span>def <span class="ident">abs_square</span></span>(<span>complex_values: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Squared magnitude of complex values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>complex_values</code></strong></dt>
<dd>complex <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dt>
<dd>real valued magnitude squared</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def abs_square(complex_values: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;
    Squared magnitude of complex values.

    Args:
      complex_values: complex `Tensor`

    Returns:
        Tensor: real valued magnitude squared

    &#34;&#34;&#34;
    return math.imag(complex_values) ** 2 + math.real(complex_values) ** 2</code></pre>
</details>
</dd>
<dt id="phi.math.all"><code class="name flex">
<span>def <span class="ident">all</span></span>(<span>boolean_tensor: phi.math._tensors.Tensor, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Tests whether all entries of <code>boolean_tensor</code> are <code>True</code> along the specified dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boolean_tensor</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_(boolean_tensor: Tensor or list or tuple, dim: DimFilter = non_batch) -&gt; Tensor:
    &#34;&#34;&#34;
    Tests whether all entries of `boolean_tensor` are `True` along the specified dimensions.

    Args:
        boolean_tensor: `Tensor` or `list` / `tuple` of Tensors.
        dim: Dimension or dimensions to be reduced. One of

            * `None` to reduce all non-batch dimensions
            * `str` containing single dimension or comma-separated list of dimensions
            * `Tuple[str]` or `List[str]`
            * `Shape`
            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
            * `&#39;0&#39;` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors

    Returns:
        `Tensor` without the reduced dimensions.
    &#34;&#34;&#34;
    return _reduce(boolean_tensor, dim, bool, native_function=lambda backend, native, dim: backend.all(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.all_available"><code class="name flex">
<span>def <span class="ident">all_available</span></span>(<span>*values: phi.math._tensors.Tensor) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Tests if the values of all given tensors are known and can be read at this point.
Tracing placeholders are considered not available, even when they hold example values.</p>
<p>Tensors are not available during <code><a title="phi.math.jit_compile" href="#phi.math.jit_compile">jit_compile()</a></code>, <code><a title="phi.math.jit_compile_linear" href="#phi.math.jit_compile_linear">jit_compile_linear()</a></code> or while using TensorFlow's legacy graph mode.</p>
<p>Tensors are typically available when the backend operates in eager mode and is not currently tracing a function.</p>
<p>This can be used instead of the native checks</p>
<ul>
<li>PyTorch: <code>torch._C._get_tracing_state()</code></li>
<li>TensorFlow: <code>tf.executing_eagerly()</code></li>
<li>Jax: <code>isinstance(x, jax.core.Tracer)</code></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensors to check.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>True</code> if no value is a placeholder or being traced, <code>False</code> otherwise.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_available(*values: Tensor) -&gt; bool:
    &#34;&#34;&#34;
    Tests if the values of all given tensors are known and can be read at this point.
    Tracing placeholders are considered not available, even when they hold example values.

    Tensors are not available during `jit_compile()`, `jit_compile_linear()` or while using TensorFlow&#39;s legacy graph mode.
    
    Tensors are typically available when the backend operates in eager mode and is not currently tracing a function.

    This can be used instead of the native checks

    * PyTorch: `torch._C._get_tracing_state()`
    * TensorFlow: `tf.executing_eagerly()`
    * Jax: `isinstance(x, jax.core.Tracer)`

    Args:
      values: Tensors to check.

    Returns:
        `True` if no value is a placeholder or being traced, `False` otherwise.
    &#34;&#34;&#34;
    from phi.math._functional import ShiftLinTracer
    for value in values:
        if isinstance(value, ShiftLinTracer):
            return False
        natives = value._natives()
        natives_available = [choose_backend(native).is_available(native) for native in natives]
        if not all(natives_available):
            return False
    return True</code></pre>
</details>
</dd>
<dt id="phi.math.any"><code class="name flex">
<span>def <span class="ident">any</span></span>(<span>boolean_tensor: phi.math._tensors.Tensor, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Tests whether any entry of <code>boolean_tensor</code> is <code>True</code> along the specified dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>boolean_tensor</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def any_(boolean_tensor: Tensor or list or tuple, dim: DimFilter = non_batch) -&gt; Tensor:
    &#34;&#34;&#34;
    Tests whether any entry of `boolean_tensor` is `True` along the specified dimensions.

    Args:
        boolean_tensor: `Tensor` or `list` / `tuple` of Tensors.
        dim: Dimension or dimensions to be reduced. One of

            * `None` to reduce all non-batch dimensions
            * `str` containing single dimension or comma-separated list of dimensions
            * `Tuple[str]` or `List[str]`
            * `Shape`
            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
            * `&#39;0&#39;` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors

    Returns:
        `Tensor` without the reduced dimensions.
    &#34;&#34;&#34;
    return _reduce(boolean_tensor, dim, bool, native_function=lambda backend, native, dim: backend.any(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.arccos"><code class="name flex">
<span>def <span class="ident">arccos</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the inverse of <em>cos(x)</em> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code>.
For real arguments, the result lies in the range [0, π].</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def arccos(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Computes the inverse of *cos(x)* of the `Tensor` or `PhiTreeNode` `x`.
    For real arguments, the result lies in the range [0, π].
    &#34;&#34;&#34;
    return _backend_op1(x, Backend.cos)</code></pre>
</details>
</dd>
<dt id="phi.math.arcsin"><code class="name flex">
<span>def <span class="ident">arcsin</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the inverse of <em>sin(x)</em> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code>.
For real arguments, the result lies in the range [-π/2, π/2].</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def arcsin(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Computes the inverse of *sin(x)* of the `Tensor` or `PhiTreeNode` `x`.
    For real arguments, the result lies in the range [-π/2, π/2].
    &#34;&#34;&#34;
    return _backend_op1(x, Backend.arcsin)</code></pre>
</details>
</dd>
<dt id="phi.math.assert_close"><code class="name flex">
<span>def <span class="ident">assert_close</span></span>(<span>*values, rel_tolerance: float = 1e-05, abs_tolerance: float = 0, msg: str = '', verbose: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks that all given tensors have equal values within the specified tolerance.
Raises an AssertionError if the values of this tensor are not within tolerance of any of the other tensors.</p>
<p>Does not check that the shapes match as long as they can be broadcast to a common shape.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensors or native tensors or numbers or sequences of numbers.</dd>
<dt><strong><code>rel_tolerance</code></strong></dt>
<dd>Relative tolerance.</dd>
<dt><strong><code>abs_tolerance</code></strong></dt>
<dd>Absolute tolerance.</dd>
<dt><strong><code>msg</code></strong></dt>
<dd>Optional error message.</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>Whether to print conflicting values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assert_close(*values,
                 rel_tolerance: float = 1e-5,
                 abs_tolerance: float = 0,
                 msg: str = &#34;&#34;,
                 verbose: bool = True):
    &#34;&#34;&#34;
    Checks that all given tensors have equal values within the specified tolerance.
    Raises an AssertionError if the values of this tensor are not within tolerance of any of the other tensors.
    
    Does not check that the shapes match as long as they can be broadcast to a common shape.

    Args:
      values: Tensors or native tensors or numbers or sequences of numbers.
      rel_tolerance: Relative tolerance.
      abs_tolerance: Absolute tolerance.
      msg: Optional error message.
      verbose: Whether to print conflicting values.
    &#34;&#34;&#34;
    if not values:
        return
    phi_tensors = [t for t in values if isinstance(t, Tensor)]
    if phi_tensors:
        values = [compatible_tensor(t, phi_tensors[0].shape)._simplify() for t in values]  # use Tensor to infer dimensions
        for other in values[1:]:
            _assert_close(values[0], other, rel_tolerance, abs_tolerance, msg, verbose)
    elif all(isinstance(v, PhiTreeNode) for v in values):
        tree0, tensors0 = disassemble_tree(values[0])
        for value in values[1:]:
            tree, tensors_ = disassemble_tree(value)
            assert tree0 == tree, f&#34;Tree structures do not match: {tree0} and {tree}&#34;
            for t0, t in zip(tensors0, tensors_):
                _assert_close(t0, t, rel_tolerance, abs_tolerance, msg, verbose)
    else:
        np_values = [choose_backend(t).numpy(t) for t in values]
        for other in np_values[1:]:
            np.testing.assert_allclose(np_values[0], other, rel_tolerance, abs_tolerance, err_msg=msg, verbose=verbose)</code></pre>
</details>
</dd>
<dt id="phi.math.batch"><code class="name flex">
<span>def <span class="ident">batch</span></span>(<span>*args, **dims: int) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the batch dimensions of an existing <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or creates a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> with only batch dimensions.</p>
<p>Usage for filtering batch dimensions:</p>
<pre><code class="language-python">batch_dims = batch(shape)
batch_dims = batch(tensor)
</code></pre>
<p>Usage for creating a <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> with only batch dimensions:</p>
<pre><code class="language-python">batch_shape = batch('undef', batch=2)
# Out: (batch=2, undef=None)
</code></pre>
<p>Here, the dimension <code>undef</code> is created with an undefined size of <code>None</code>.
Undefined sizes are automatically filled in by <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code>, <code><a title="phi.math.wrap" href="#phi.math.wrap">wrap()</a></code>, <code><a title="phi.math.stack" href="#phi.math.stack">stack()</a></code> and <code><a title="phi.math.concat" href="#phi.math.concat">concat()</a></code>.</p>
<p>To create a shape with multiple types, use <code><a title="phi.math.merge_shapes" href="#phi.math.merge_shapes">merge_shapes()</a></code>, <code><a title="phi.math.concat_shapes" href="#phi.math.concat_shapes">concat_shapes()</a></code> or the syntax <code>shape1 &amp; shape2</code>.</p>
<p>See Also:
<code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>
<p>Either</p>
<ul>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> to filter or</li>
<li>Names of dimensions with undefined sizes as <code>str</code>.</li>
</ul>
</dd>
<dt><strong><code>**dims</code></strong></dt>
<dd>Dimension sizes and names. Must be empty when used as a filter operation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> containing only dimensions of type batch.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch(*args, **dims: int or str or tuple or list or Shape) -&gt; Shape:
    &#34;&#34;&#34;
    Returns the batch dimensions of an existing `Shape` or creates a new `Shape` with only batch dimensions.

    Usage for filtering batch dimensions:
    ```python
    batch_dims = batch(shape)
    batch_dims = batch(tensor)
    ```

    Usage for creating a `Shape` with only batch dimensions:
    ```python
    batch_shape = batch(&#39;undef&#39;, batch=2)
    # Out: (batch=2, undef=None)
    ```
    Here, the dimension `undef` is created with an undefined size of `None`.
    Undefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.

    To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 &amp; shape2`.

    See Also:
        `channel`, `spatial`, `instance`

    Args:
        *args: Either

            * `Shape` or `Tensor` to filter or
            * Names of dimensions with undefined sizes as `str`.

        **dims: Dimension sizes and names. Must be empty when used as a filter operation.

    Returns:
        `Shape` containing only dimensions of type batch.
    &#34;&#34;&#34;
    from .magic import Shaped
    if all(isinstance(arg, str) for arg in args) or dims:
        return _construct_shape(BATCH_DIM, *args, **dims)
    elif len(args) == 1 and isinstance(args[0], Shape):
        return args[0].batch
    elif len(args) == 1 and isinstance(args[0], Shaped):
        return shape(args[0]).batch
    else:
        raise AssertionError(f&#34;batch() must be called either as a selector batch(Shape) or batch(Tensor) or as a constructor batch(*names, **dims). Got *args={args}, **dims={dims}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.boolean_mask"><code class="name flex">
<span>def <span class="ident">boolean_mask</span></span>(<span>x: phi.math._tensors.Tensor, dim: str, mask: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Discards values <code>x.dim[i]</code> where <code>mask.dim[i]=False</code>.
All dimensions of <code>mask</code> that are not <code>dim</code> are treated as batch dimensions.</p>
<p>Alternative syntax: <code>x.dim[mask]</code>.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: Slicing</li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.masked_select.html"><code>masked_select</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/boolean_mask"><code>tf.boolean_mask</code></a></li>
<li>Jax: Slicing</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of values.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension of <code>x</code> to along which to discard slices.</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>Boolean <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> marking which values to keep. Must have the dimension <code>dim</code> matching `x´.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Selected values of <code>x</code> as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with dimensions from <code>x</code> and <code>mask</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def boolean_mask(x: Tensor, dim: str, mask: Tensor):
    &#34;&#34;&#34;
    Discards values `x.dim[i]` where `mask.dim[i]=False`.
    All dimensions of `mask` that are not `dim` are treated as batch dimensions.

    Alternative syntax: `x.dim[mask]`.

    Implementations:

    * NumPy: Slicing
    * PyTorch: [`masked_select`](https://pytorch.org/docs/stable/generated/torch.masked_select.html)
    * TensorFlow: [`tf.boolean_mask`](https://www.tensorflow.org/api_docs/python/tf/boolean_mask)
    * Jax: Slicing

    Args:
        x: `Tensor` of values.
        dim: Dimension of `x` to along which to discard slices.
        mask: Boolean `Tensor` marking which values to keep. Must have the dimension `dim` matching `x´.

    Returns:
        Selected values of `x` as `Tensor` with dimensions from `x` and `mask`.
    &#34;&#34;&#34;
    assert dim in mask.shape, f&#34;mask dimension &#39;{dim}&#39; must be present on the mask but got {mask.shape}&#34;
    
    def uniform_boolean_mask(x: Tensor, mask_1d: Tensor):
        if dim in x.shape:
            x_native = x.native(x.shape.names)  # order does not matter
            mask_native = mask_1d.native()  # only has 1 dim
            backend = choose_backend(x_native, mask_native)
            result_native = backend.boolean_mask(x_native, mask_native, axis=x.shape.index(dim))
            new_shape = x.shape.with_sizes(backend.staticshape(result_native))
            return NativeTensor(result_native, new_shape)
        else:
            total = int(sum_(to_int64(mask_1d), mask_1d.shape))
            new_shape = mask_1d.shape.with_sizes([total])
            return expand(x, new_shape)

    return broadcast_op(uniform_boolean_mask, [x, mask], iter_dims=mask.shape.without(dim))</code></pre>
</details>
</dd>
<dt id="phi.math.cast"><code class="name flex">
<span>def <span class="ident">cast</span></span>(<span>x: ~OtherMagicType, dtype: phi.math.backend._dtype.DType) ‑> ~OtherMagicType</span>
</code></dt>
<dd>
<div class="desc"><p>Casts <code>x</code> to a different data type.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="numpy.ndarray.astype"><code>x.astype()</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to"><code>x.to()</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/cast"><code>tf.cast</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html"><code>jax.numpy.array</code></a></li>
</ul>
<p>See Also:
<code><a title="phi.math.to_float" href="#phi.math.to_float">to_float()</a></code>, <code><a title="phi.math.to_int32" href="#phi.math.to_int32">to_int32()</a></code>, <code><a title="phi.math.to_int64" href="#phi.math.to_int64">to_int64()</a></code>, <code><a title="phi.math.to_complex" href="#phi.math.to_complex">to_complex()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>New data type as <code><a title="phi.math.DType" href="#phi.math.DType">DType</a></code>, e.g. <code><a title="phi.math.DType" href="#phi.math.DType">DType</a>(int, 16)</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with data type <code><a title="phi.math.dtype" href="#phi.math.dtype">dtype()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cast(x: OtherMagicType, dtype: DType or type) -&gt; OtherMagicType:
    &#34;&#34;&#34;
    Casts `x` to a different data type.

    Implementations:

    * NumPy: [`x.astype()`](numpy.ndarray.astype)
    * PyTorch: [`x.to()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to)
    * TensorFlow: [`tf.cast`](https://www.tensorflow.org/api_docs/python/tf/cast)
    * Jax: [`jax.numpy.array`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html)

    See Also:
        `to_float`, `to_int32`, `to_int64`, `to_complex`.

    Args:
        x: `Tensor`
        dtype: New data type as `phi.math.DType`, e.g. `DType(int, 16)`.

    Returns:
        `Tensor` with data type `dtype`
    &#34;&#34;&#34;
    if not isinstance(dtype, DType):
        dtype = DType.as_dtype(dtype)
    if hasattr(x, &#39;__cast__&#39;):
        return x.__cast__(dtype)
    elif isinstance(x, (Number, bool)):
        return dtype.kind(x)
    elif isinstance(x, PhiTreeNode):
        attrs = {key: getattr(x, key) for key in value_attributes(x)}
        new_attrs = {k: cast(v, dtype) for k, v in attrs.items()}
        return copy_with(x, **new_attrs)
    try:
        backend = choose_backend(x)
        return backend.cast(x, dtype)
    except NoBackendFound:
        if dtype.kind == bool:
            return bool(x)
        raise ValueError(f&#34;Cannot cast object of type &#39;{type(x).__name__}&#39;&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.ceil"><code class="name flex">
<span>def <span class="ident">ceil</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>⌈x⌉</em> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ceil(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Computes *⌈x⌉* of the `Tensor` or `PhiTreeNode` `x`. &#34;&#34;&#34;
    return _backend_op1(x, Backend.ceil)</code></pre>
</details>
</dd>
<dt id="phi.math.channel"><code class="name flex">
<span>def <span class="ident">channel</span></span>(<span>*args, **dims: int) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the channel dimensions of an existing <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or creates a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> with only channel dimensions.</p>
<p>Usage for filtering channel dimensions:</p>
<pre><code class="language-python">channel_dims = channel(shape)
channel_dims = channel(tensor)
</code></pre>
<p>Usage for creating a <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> with only channel dimensions:</p>
<pre><code class="language-python">channel_shape = channel('undef', vector=2)
# Out: (vector=2, undef=None)
</code></pre>
<p>Here, the dimension <code>undef</code> is created with an undefined size of <code>None</code>.
Undefined sizes are automatically filled in by <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code>, <code><a title="phi.math.wrap" href="#phi.math.wrap">wrap()</a></code>, <code><a title="phi.math.stack" href="#phi.math.stack">stack()</a></code> and <code><a title="phi.math.concat" href="#phi.math.concat">concat()</a></code>.</p>
<p>To create a shape with multiple types, use <code><a title="phi.math.merge_shapes" href="#phi.math.merge_shapes">merge_shapes()</a></code>, <code><a title="phi.math.concat_shapes" href="#phi.math.concat_shapes">concat_shapes()</a></code> or the syntax <code>shape1 &amp; shape2</code>.</p>
<p>See Also:
<code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>
<p>Either</p>
<ul>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> to filter or</li>
<li>Names of dimensions with undefined sizes as <code>str</code>.</li>
</ul>
</dd>
<dt><strong><code>**dims</code></strong></dt>
<dd>Dimension sizes and names. Must be empty when used as a filter operation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> containing only dimensions of type channel.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def channel(*args, **dims: int or str or tuple or list or Shape) -&gt; Shape:
    &#34;&#34;&#34;
    Returns the channel dimensions of an existing `Shape` or creates a new `Shape` with only channel dimensions.

    Usage for filtering channel dimensions:
    ```python
    channel_dims = channel(shape)
    channel_dims = channel(tensor)
    ```

    Usage for creating a `Shape` with only channel dimensions:
    ```python
    channel_shape = channel(&#39;undef&#39;, vector=2)
    # Out: (vector=2, undef=None)
    ```
    Here, the dimension `undef` is created with an undefined size of `None`.
    Undefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.

    To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 &amp; shape2`.

    See Also:
        `spatial`, `batch`, `instance`

    Args:
        *args: Either

            * `Shape` or `Tensor` to filter or
            * Names of dimensions with undefined sizes as `str`.

        **dims: Dimension sizes and names. Must be empty when used as a filter operation.

    Returns:
        `Shape` containing only dimensions of type channel.
    &#34;&#34;&#34;
    from .magic import Shaped
    if all(isinstance(arg, str) for arg in args) or dims:
        return _construct_shape(CHANNEL_DIM, *args, **dims)
    elif len(args) == 1 and isinstance(args[0], Shape):
        return args[0].channel
    elif len(args) == 1 and isinstance(args[0], Shaped):
        return shape(args[0]).channel
    else:
        raise AssertionError(f&#34;channel() must be called either as a selector channel(Shape) or channel(Tensor) or as a constructor channel(*names, **dims). Got *args={args}, **dims={dims}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.choose_backend"><code class="name flex">
<span>def <span class="ident">choose_backend</span></span>(<span>*values, prefer_default=False) ‑> phi.math.backend._backend.Backend</span>
</code></dt>
<dd>
<div class="desc"><p>Choose backend for given <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or native tensor values.
Backends need to be registered to be available, e.g. via the global import <code>phi.&lt;backend&gt;</code> or <code><a title="phi.detect_backends" href="../index.html#phi.detect_backends">detect_backends()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*values</code></strong></dt>
<dd>Sequence of <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>s, native tensors or constants.</dd>
<dt><strong><code>prefer_default</code></strong></dt>
<dd>Whether to always select the default backend if it can work with <code>values</code>, see <code>default_backend()</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The selected <code><a title="phi.math.backend.Backend" href="backend/index.html#phi.math.backend.Backend">Backend</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def choose_backend_t(*values, prefer_default=False) -&gt; Backend:
    &#34;&#34;&#34;
    Choose backend for given `Tensor` or native tensor values.
    Backends need to be registered to be available, e.g. via the global import `phi.&lt;backend&gt;` or `phi.detect_backends()`.

    Args:
        *values: Sequence of `Tensor`s, native tensors or constants.
        prefer_default: Whether to always select the default backend if it can work with `values`, see `default_backend()`.

    Returns:
        The selected `phi.math.backend.Backend`
    &#34;&#34;&#34;
    natives = sum([v._natives() if isinstance(v, Tensor) else (v,) for v in values], ())
    return choose_backend(*natives, prefer_default=prefer_default)</code></pre>
</details>
</dd>
<dt id="phi.math.clip"><code class="name flex">
<span>def <span class="ident">clip</span></span>(<span>x: phi.math._tensors.Tensor, lower_limit: float, upper_limit: float)</span>
</code></dt>
<dd>
<div class="desc"><p>Limits the values of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> <code>x</code> to lie between <code>lower_limit</code> and <code>upper_limit</code> (inclusive).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clip(x: Tensor, lower_limit: float or Tensor, upper_limit: float or Tensor):
    &#34;&#34;&#34; Limits the values of the `Tensor` `x` to lie between `lower_limit` and `upper_limit` (inclusive). &#34;&#34;&#34;
    if isinstance(lower_limit, Number) and isinstance(upper_limit, Number):

        def clip_(x):
            return x._op1(lambda native: choose_backend(native).clip(native, lower_limit, upper_limit))

        return broadcast_op(clip_, [x])
    else:
        return maximum(lower_limit, minimum(x, upper_limit))</code></pre>
</details>
</dd>
<dt id="phi.math.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>*tensors, rel_tolerance=1e-05, abs_tolerance=0) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Checks whether all tensors have equal values within the specified tolerance.</p>
<p>Does not check that the shapes exactly match.
Tensors with different shapes are reshaped before comparing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*tensors</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or tensor-like (constant) each</dd>
<dt><strong><code>rel_tolerance</code></strong></dt>
<dd>relative tolerance (Default value = 1e-5)</dd>
<dt><strong><code>abs_tolerance</code></strong></dt>
<dd>absolute tolerance (Default value = 0)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Whether all given tensors are equal to the first tensor within the specified tolerance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def close(*tensors, rel_tolerance=1e-5, abs_tolerance=0) -&gt; bool:
    &#34;&#34;&#34;
    Checks whether all tensors have equal values within the specified tolerance.
    
    Does not check that the shapes exactly match.
    Tensors with different shapes are reshaped before comparing.

    Args:
        *tensors: `Tensor` or tensor-like (constant) each
        rel_tolerance: relative tolerance (Default value = 1e-5)
        abs_tolerance: absolute tolerance (Default value = 0)

    Returns:
        Whether all given tensors are equal to the first tensor within the specified tolerance.
    &#34;&#34;&#34;
    tensors = [wrap(t) for t in tensors]
    for other in tensors[1:]:
        if not _close(tensors[0], other, rel_tolerance=rel_tolerance, abs_tolerance=abs_tolerance):
            return False
    return True</code></pre>
</details>
</dd>
<dt id="phi.math.closest_grid_values"><code class="name flex">
<span>def <span class="ident">closest_grid_values</span></span>(<span>grid: phi.math._tensors.Tensor, coordinates: phi.math._tensors.Tensor, extrap: e_.Extrapolation, stack_dim_prefix='closest_', **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the neighboring grid points in all spatial directions and returns their values.
The result will have 2^d values for each vector in coordiantes in d dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>grid data. The grid is spanned by the spatial dimensions of the tensor</dd>
<dt><strong><code>coordinates</code></strong></dt>
<dd>tensor with 1 channel dimension holding vectors pointing to locations in grid index space</dd>
<dt><strong><code>extrap</code></strong></dt>
<dd>grid extrapolation</dd>
<dt><strong><code>stack_dim_prefix</code></strong></dt>
<dd>For each spatial dimension <code>dim</code>, stacks lower and upper closest values along dimension <code>stack_dim_prefix+dim</code>.</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>Additional information for the extrapolation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor of shape (batch, coord_spatial, grid_spatial=(2, 2,&hellip;), grid_channel)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def closest_grid_values(grid: Tensor,
                        coordinates: Tensor,
                        extrap: &#39;e_.Extrapolation&#39;,
                        stack_dim_prefix=&#39;closest_&#39;,
                        **kwargs):
    &#34;&#34;&#34;
    Finds the neighboring grid points in all spatial directions and returns their values.
    The result will have 2^d values for each vector in coordiantes in d dimensions.

    Args:
      grid: grid data. The grid is spanned by the spatial dimensions of the tensor
      coordinates: tensor with 1 channel dimension holding vectors pointing to locations in grid index space
      extrap: grid extrapolation
      stack_dim_prefix: For each spatial dimension `dim`, stacks lower and upper closest values along dimension `stack_dim_prefix+dim`.
      kwargs: Additional information for the extrapolation.

    Returns:
      Tensor of shape (batch, coord_spatial, grid_spatial=(2, 2,...), grid_channel)

    &#34;&#34;&#34;
    return broadcast_op(functools.partial(_closest_grid_values, extrap=extrap, stack_dim_prefix=stack_dim_prefix, pad_kwargs=kwargs), [grid, coordinates])</code></pre>
</details>
</dd>
<dt id="phi.math.concat"><code class="name flex">
<span>def <span class="ident">concat</span></span>(<span>values: tuple, dim: str, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Concatenates a sequence of <code><a title="phi.math.magic.Shapable" href="magic.html#phi.math.magic.Shapable">Shapable</a></code> objects, e.g. <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, along one dimension.
All values must have the same spatial, instance and channel dimensions and their sizes must be equal, except for <code>dim</code>.
Batch dimensions will be added as needed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tuple or list of <code><a title="phi.math.magic.Shapable" href="magic.html#phi.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Concatenation dimension, must be present in all <code>values</code>.
The size along <code>dim</code> is determined from <code>values</code> and can be set to undefined (<code>None</code>).</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Concatenated <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">concat([math.zeros(batch(b=10)), math.ones(batch(b=10))], 'b')
# Out: (bᵇ=20) 0.500 ± 0.500 (0e+00...1e+00)

concat([vec(x=1, y=0), vec(z=2.)], 'vector')
# Out: (x=1.000, y=0.000, z=2.000) float64
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def concat(values: tuple or list, dim: str or Shape, **kwargs):
    &#34;&#34;&#34;
    Concatenates a sequence of `phi.math.magic.Shapable` objects, e.g. `Tensor`, along one dimension.
    All values must have the same spatial, instance and channel dimensions and their sizes must be equal, except for `dim`.
    Batch dimensions will be added as needed.

    Args:
        values: Tuple or list of `phi.math.magic.Shapable`, such as `phi.math.Tensor`
        dim: Concatenation dimension, must be present in all `values`.
            The size along `dim` is determined from `values` and can be set to undefined (`None`).
        **kwargs: Additional keyword arguments required by specific implementations.
            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
            Adding batch dimensions must always work without keyword arguments.

    Returns:
        Concatenated `Tensor`

    Examples:

        ```python
        concat([math.zeros(batch(b=10)), math.ones(batch(b=10))], &#39;b&#39;)
        # Out: (bᵇ=20) 0.500 ± 0.500 (0e+00...1e+00)

        concat([vec(x=1, y=0), vec(z=2.)], &#39;vector&#39;)
        # Out: (x=1.000, y=0.000, z=2.000) float64
        ```
    &#34;&#34;&#34;
    assert len(values) &gt; 0, f&#34;concat() got empty sequence {values}&#34;
    if isinstance(dim, Shape):
        dim = dim.name
    assert isinstance(dim, str), f&#34;dim must be a str or Shape but got &#39;{dim}&#39; of type {type(dim)}&#34;
    for v in values:
        assert dim in shape(v), f&#34;dim must be present in the shapes of all values bot got value {type(v).__name__} with shape {shape(v)}&#34;
    for v in values[1:]:
        assert set(non_batch(v).names) == set(non_batch(values[0]).names), f&#34;Concatenated values must have the same non-batch dimensions but got {non_batch(values[0])} and {non_batch(v)}&#34;
    # Add missing batch dimensions
    all_batch_dims = merge_shapes(*[batch(v) for v in values])
    values = [expand(v, all_batch_dims) for v in values]
    # --- First try __concat__ ---
    for v in values:
        if isinstance(v, Shapable):
            if hasattr(v, &#39;__concat__&#39;):
                result = v.__concat__(values, dim, **kwargs)
                if result is not NotImplemented:
                    assert isinstance(result, Shapable), &#34;__concat__ must return a Shapable object&#34;
                    return result
    # --- Next: try concat attributes for tree nodes ---
    if all(isinstance(v, PhiTreeNode) for v in values):
        attributes = all_attributes(values[0])
        if attributes and all(all_attributes(v) == attributes for v in values):
            new_attrs = {}
            for a in attributes:
                common_shape = merge_shapes(*[shape(getattr(v, a)).without(dim) for v in values])
                a_values = [expand(getattr(v, a), common_shape &amp; shape(v).only(dim)) for v in values]  # expand by dim if missing, and dims of others
                new_attrs[a] = concat(a_values, dim, **kwargs)
            return copy_with(values[0], **new_attrs)
        else:
            warnings.warn(f&#34;Failed to concat values using value attributes because attributes differ among values {values}&#34;)
    # --- Fallback: slice and stack ---
    try:
        unstacked = sum([unstack(v, dim) for v in values], ())
    except MagicNotImplemented:
        raise MagicNotImplemented(f&#34;concat: No value implemented __concat__ and not all values were Sliceable along {dim}. values = {[type(v) for v in values]}&#34;)
    if len(unstacked) &gt; 8:
        warnings.warn(f&#34;concat() default implementation is slow on large dimensions ({dim}={len(unstacked)}). Please implement __concat__()&#34;, RuntimeWarning, stacklevel=2)
    dim = shape(values[0])[dim].with_size(None)
    try:
        return stack(unstacked, dim, **kwargs)
    except MagicNotImplemented:
        raise MagicNotImplemented(f&#34;concat: No value implemented __concat__ and slices could not be stacked. values = {[type(v) for v in values]}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.concat_shapes"><code class="name flex">
<span>def <span class="ident">concat_shapes</span></span>(<span>*shapes: phi.math._shape.Shape) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> listing the dimensions of all <code>shapes</code> in the given order.</p>
<p>See Also:
<code><a title="phi.math.merge_shapes" href="#phi.math.merge_shapes">merge_shapes()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*shapes</code></strong></dt>
<dd>Shapes to concatenate. No two shapes must contain a dimension with the same name.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Combined <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def concat_shapes(*shapes: Shape or Any) -&gt; Shape:
    &#34;&#34;&#34;
    Creates a `Shape` listing the dimensions of all `shapes` in the given order.

    See Also:
        `merge_shapes()`.

    Args:
        *shapes: Shapes to concatenate. No two shapes must contain a dimension with the same name.

    Returns:
        Combined `Shape`.
    &#34;&#34;&#34;
    shapes = [obj if isinstance(obj, Shape) else shape(obj) for obj in shapes]
    names = sum([s.names for s in shapes], ())
    if len(set(names)) != len(names):
        raise IncompatibleShapes(f&#34;Cannot concatenate shapes {list(shapes)}. Duplicate dimension names are not allowed.&#34;)
    sizes = sum([s.sizes for s in shapes], ())
    types = sum([s.types for s in shapes], ())
    item_names = sum([s.item_names for s in shapes], ())
    return Shape(sizes, names, types, item_names)</code></pre>
</details>
</dd>
<dt id="phi.math.conjugate"><code class="name flex">
<span>def <span class="ident">conjugate</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>See Also:
<code><a title="phi.math.imag" href="#phi.math.imag">imag()</a></code>, <code><a title="phi.math.real" href="#phi.math.real">real()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Real or complex <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> or native tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Complex conjugate of <code>x</code> if <code>x</code> is complex, else <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conjugate(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34;
    See Also:
        `imag()`, `real()`.

    Args:
        x: Real or complex `Tensor` or `PhiTreeNode` or native tensor.

    Returns:
        Complex conjugate of `x` if `x` is complex, else `x`.
    &#34;&#34;&#34;
    return _backend_op1(x, Backend.conj)</code></pre>
</details>
</dd>
<dt id="phi.math.const_vec"><code class="name flex">
<span>def <span class="ident">const_vec</span></span>(<span>value: float, dim: phi.math._shape.Shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a single-dimension tensor with all values equal to <code>value</code>.
<code>value</code> is not converted to the default backend, even when it is a Python primitive.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>Value for filling the vector.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Either single-dimension non-spatial Shape or <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> consisting of any number of spatial dimensions.
In the latter case, a new channel dimension named <code>'vector'</code> will be created from the spatial shape.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def const_vec(value: float or Tensor, dim: Shape or tuple or list or str):
    &#34;&#34;&#34;
    Creates a single-dimension tensor with all values equal to `value`.
    `value` is not converted to the default backend, even when it is a Python primitive.

    Args:
        value: Value for filling the vector.
        dim: Either single-dimension non-spatial Shape or `Shape` consisting of any number of spatial dimensions.
            In the latter case, a new channel dimension named `&#39;vector&#39;` will be created from the spatial shape.

    Returns:
        `Tensor`
    &#34;&#34;&#34;
    if isinstance(dim, Shape):
        if dim.spatial:
            assert not dim.non_spatial, f&#34;When creating a vector given spatial dimensions, the shape may only contain spatial dimensions but got {dim}&#34;
            shape = channel(vector=dim.names)
        else:
            assert dim.rank == 1, f&#34;Cannot create vector from {dim}&#34;
            shape = dim
    else:
        dims = parse_dim_order(dim)
        shape = channel(vector=dims)
    return wrap([value] * shape.size, shape)</code></pre>
</details>
</dd>
<dt id="phi.math.convert"><code class="name flex">
<span>def <span class="ident">convert</span></span>(<span>x, backend: phi.math.backend._backend.Backend = None, use_dlpack=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert the native representation of a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> to the native format of <code><a title="phi.math.backend" href="backend/index.html">phi.math.backend</a></code>.</p>
<p><em>Warning</em>: This operation breaks the automatic differentiation chain.</p>
<p>See Also:
<code><a title="phi.math.backend.convert" href="backend/index.html#phi.math.backend.convert">convert()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> to convert. If <code>x</code> is a <code>PhiTreeNode</code>, its variable attributes are converted.</dd>
<dt><strong><code>backend</code></strong></dt>
<dd>Target backend. If <code>None</code>, uses the current default backend, see <code><a title="phi.math.backend.default_backend" href="backend/index.html#phi.math.backend.default_backend">default_backend()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with native representation belonging to <code><a title="phi.math.backend" href="backend/index.html">phi.math.backend</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert(x, backend: Backend = None, use_dlpack=True):
    &#34;&#34;&#34;
    Convert the native representation of a `Tensor` or `PhiTreeNode` to the native format of `backend`.

    *Warning*: This operation breaks the automatic differentiation chain.

    See Also:
        `phi.math.backend.convert()`.

    Args:
        x: `Tensor` to convert. If `x` is a `PhiTreeNode`, its variable attributes are converted.
        backend: Target backend. If `None`, uses the current default backend, see `phi.math.backend.default_backend()`.

    Returns:
        `Tensor` with native representation belonging to `backend`.
    &#34;&#34;&#34;
    if isinstance(x, Tensor):
        return x._op1(lambda native: b_convert(native, backend, use_dlpack=use_dlpack))
    elif isinstance(x, PhiTreeNode):
        return copy_with(x, **{a: convert(getattr(x, a), backend, use_dlpack=use_dlpack) for a in variable_attributes(x)})
    else:
        return choose_backend(x).as_tensor(x)</code></pre>
</details>
</dd>
<dt id="phi.math.convolve"><code class="name flex">
<span>def <span class="ident">convolve</span></span>(<span>value: phi.math._tensors.Tensor, kernel: phi.math._tensors.Tensor, extrapolation: e_.Extrapolation = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the convolution of <code>value</code> and <code>kernel</code> along the spatial axes of <code>kernel</code>.</p>
<p>The channel dimensions of <code>value</code> are reduced against the equally named dimensions of <code>kernel</code>.
The result will have the non-reduced channel dimensions of <code>kernel</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> whose shape includes all spatial dimensions of <code>kernel</code>.</dd>
<dt><strong><code>kernel</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> used as convolutional filter.</dd>
<dt><strong><code>extrapolation</code></strong></dt>
<dd>If not None, pads <code>value</code> so that the result has the same shape as <code>value</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convolve(value: Tensor,
             kernel: Tensor,
             extrapolation: &#39;e_.Extrapolation&#39; = None) -&gt; Tensor:
    &#34;&#34;&#34;
    Computes the convolution of `value` and `kernel` along the spatial axes of `kernel`.

    The channel dimensions of `value` are reduced against the equally named dimensions of `kernel`.
    The result will have the non-reduced channel dimensions of `kernel`.

    Args:
        value: `Tensor` whose shape includes all spatial dimensions of `kernel`.
        kernel: `Tensor` used as convolutional filter.
        extrapolation: If not None, pads `value` so that the result has the same shape as `value`.

    Returns:
        `Tensor`
    &#34;&#34;&#34;
    assert all(dim in value.shape for dim in kernel.shape.spatial.names), f&#34;Value must have all spatial dimensions of kernel but got value {value} kernel {kernel}&#34;
    conv_shape = kernel.shape.spatial
    in_channels = value.shape.channel
    out_channels = kernel.shape.channel.without(in_channels)
    batch = value.shape.batch &amp; kernel.shape.batch
    if extrapolation is not None and extrapolation != e_.ZERO:
        value = pad(value, {dim: (kernel.shape.get_size(dim) // 2, (kernel.shape.get_size(dim) - 1) // 2)
                            for dim in conv_shape.name}, extrapolation)
    native_kernel = reshaped_native(kernel, (batch, out_channels, in_channels, *conv_shape.names), force_expand=in_channels)
    native_value = reshaped_native(value, (batch, in_channels, *conv_shape.names), force_expand=batch)
    backend = choose_backend(native_value, native_kernel)
    native_result = backend.conv(native_value, native_kernel, zero_padding=extrapolation == e_.ZERO)
    result = reshaped_tensor(native_result, (batch, out_channels, *conv_shape))
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>value: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Copies the data buffer and encapsulating <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> to be copied.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of <code>value</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy(value: Tensor):
    &#34;&#34;&#34;
    Copies the data buffer and encapsulating `Tensor` object.

    Args:
        value: `Tensor` to be copied.

    Returns:
        Copy of `value`.
    &#34;&#34;&#34;
    if value._is_tracer:
        warnings.warn(&#34;Tracing tensors cannot be copied.&#34;, RuntimeWarning)
        return value
    return value._op1(lambda native: choose_backend(native).copy(native))</code></pre>
</details>
</dd>
<dt id="phi.math.copy_with"><code class="name flex">
<span>def <span class="ident">copy_with</span></span>(<span>obj: ~PhiTreeNodeType, **updates) ‑> ~PhiTreeNodeType</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a copy of the given <code>PhiTreeNode</code> with updated values as specified in <code>updates</code>.</p>
<p>If <code>obj</code> overrides <code>__with_attrs__</code>, the copy will be created via that specific implementation.
Otherwise, the <code><a title="phi.math.copy" href="#phi.math.copy">copy()</a></code> module and <code>setattr</code> will be used.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code>PhiTreeNode</code></dd>
<dt><strong><code>**updates</code></strong></dt>
<dd>Values to be replaced.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of <code>obj</code> with updated values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy_with(obj: PhiTreeNodeType, **updates) -&gt; PhiTreeNodeType:
    &#34;&#34;&#34;
    Creates a copy of the given `PhiTreeNode` with updated values as specified in `updates`.

    If `obj` overrides `__with_attrs__`, the copy will be created via that specific implementation.
    Otherwise, the `copy` module and `setattr` will be used.

    Args:
        obj: `PhiTreeNode`
        **updates: Values to be replaced.

    Returns:
        Copy of `obj` with updated values.
    &#34;&#34;&#34;
    if hasattr(obj, &#39;__with_attrs__&#39;):
        return obj.__with_attrs__(**updates)
    elif isinstance(obj, (Number, bool)):
        return obj
    else:
        cpy = copy.copy(obj)
        for attr, value in updates.items():
            setattr(cpy, attr, value)
        return cpy</code></pre>
</details>
</dd>
<dt id="phi.math.cos"><code class="name flex">
<span>def <span class="ident">cos</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>cos(x)</em> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cos(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Computes *cos(x)* of the `Tensor` or `PhiTreeNode` `x`. &#34;&#34;&#34;
    return _backend_op1(x, Backend.cos)</code></pre>
</details>
</dd>
<dt id="phi.math.cross_product"><code class="name flex">
<span>def <span class="ident">cross_product</span></span>(<span>vec1: phi.math._tensors.Tensor, vec2: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the cross product of two vectors in 2D.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>vec1</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with a single channel dimension called <code>'vector'</code></dd>
<dt><strong><code>vec2</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with a single channel dimension called <code>'vector'</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cross_product(vec1: Tensor, vec2: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;
    Computes the cross product of two vectors in 2D.

    Args:
        vec1: `Tensor` with a single channel dimension called `&#39;vector&#39;`
        vec2: `Tensor` with a single channel dimension called `&#39;vector&#39;`

    Returns:
        `Tensor`
    &#34;&#34;&#34;
    vec1 = math.tensor(vec1)
    vec2 = math.tensor(vec2)
    spatial_rank = vec1.vector.size if &#39;vector&#39; in vec1.shape else vec2.vector.size
    if spatial_rank == 2:  # Curl in 2D
        assert vec2.vector.exists
        if vec1.vector.exists:
            v1_x, v1_y = vec1.vector
            v2_x, v2_y = vec2.vector
            return v1_x * v2_y - v1_y * v2_x
        else:
            v2_x, v2_y = vec2.vector
            return vec1 * math.stack_tensors([-v2_y, v2_x], channel(&#39;vector&#39;))
    elif spatial_rank == 3:  # Curl in 3D
        raise NotImplementedError(f&#39;spatial_rank={spatial_rank} not yet implemented&#39;)
    else:
        raise AssertionError(f&#39;dims = {spatial_rank}. Vector product not available in &gt; 3 dimensions&#39;)</code></pre>
</details>
</dd>
<dt id="phi.math.cumulative_sum"><code class="name flex">
<span>def <span class="ident">cumulative_sum</span></span>(<span>x: phi.math._tensors.Tensor, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable])</span>
</code></dt>
<dd>
<div class="desc"><p>Performs a cumulative sum of <code>x</code> along <code>dim</code>.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html"><code>cumsum</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.cumsum.html"><code>cumsum</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/math/cumsum"><code>cumsum</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.cumsum.html"><code>cumsum</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension along which to sum, as <code>str</code> or <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with the same shape as <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cumulative_sum(x: Tensor, dim: DimFilter):
    &#34;&#34;&#34;
    Performs a cumulative sum of `x` along `dim`.

    Implementations:

    * NumPy: [`cumsum`](https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html)
    * PyTorch: [`cumsum`](https://pytorch.org/docs/stable/generated/torch.cumsum.html)
    * TensorFlow: [`cumsum`](https://www.tensorflow.org/api_docs/python/tf/math/cumsum)
    * Jax: [`cumsum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.cumsum.html)

    Args:
        x: `Tensor`
        dim: Dimension along which to sum, as `str` or `Shape`.

    Returns:
        `Tensor` with the same shape as `x`.
    &#34;&#34;&#34;
    dim = x.shape.only(dim)
    assert len(dim) == 1, f&#34;dim must be a single dimension but got {dim}&#34;
    native_x = x.native(x.shape)
    native_result = choose_backend(native_x).cumsum(native_x, x.shape.index(dim))
    return NativeTensor(native_result, x.shape)</code></pre>
</details>
</dd>
<dt id="phi.math.custom_gradient"><code class="name flex">
<span>def <span class="ident">custom_gradient</span></span>(<span>f: Callable, gradient: Callable, auxiliary_args: str = '')</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a function based on <code>f</code> that uses a custom gradient for the backpropagation pass.</p>
<p><em>Warning</em> This method can lead to memory leaks if the gradient function is not called.
Make sure to pass tensors without gradients if the gradient is not required, see <code><a title="phi.math.stop_gradient" href="#phi.math.stop_gradient">stop_gradient()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Forward function mapping <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> arguments <code>x</code> to a single <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> output or sequence of tensors <code>y</code>.</dd>
<dt><strong><code>gradient</code></strong></dt>
<dd>Function to compute the vector-Jacobian product for backpropagation.
Will be called as <code>gradient(input_dict, *y, *dy) -&gt; output_dict</code> where <code>input_dict</code> contains all named arguments passed to the forward function
and <code>output_dict</code> contains only those parameters for which a gradient is defined.</dd>
<dt><strong><code>auxiliary_args</code></strong></dt>
<dd>Comma-separated parameter names of arguments that are not relevant to backpropagation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with similar signature and return values as <code>f</code>. However, the returned function does not support keyword arguments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def custom_gradient(f: Callable, gradient: Callable, auxiliary_args: str = &#39;&#39;):
    &#34;&#34;&#34;
    Creates a function based on `f` that uses a custom gradient for the backpropagation pass.

    *Warning* This method can lead to memory leaks if the gradient function is not called.
    Make sure to pass tensors without gradients if the gradient is not required, see `stop_gradient()`.

    Args:
        f: Forward function mapping `Tensor` arguments `x` to a single `Tensor` output or sequence of tensors `y`.
        gradient: Function to compute the vector-Jacobian product for backpropagation.
            Will be called as `gradient(input_dict, *y, *dy) -&gt; output_dict` where `input_dict` contains all named arguments passed to the forward function
            and `output_dict` contains only those parameters for which a gradient is defined.
        auxiliary_args: Comma-separated parameter names of arguments that are not relevant to backpropagation.

    Returns:
        Function with similar signature and return values as `f`. However, the returned function does not support keyword arguments.
    &#34;&#34;&#34;
    auxiliary_args = set(s.strip() for s in auxiliary_args.split(&#39;,&#39;) if s.strip())
    return CustomGradientFunction(f, gradient, auxiliary_args)</code></pre>
</details>
</dd>
<dt id="phi.math.degrees"><code class="name flex">
<span>def <span class="ident">degrees</span></span>(<span>deg)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert degrees to radians.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def degrees(deg):
    &#34;&#34;&#34; Convert degrees to radians. &#34;&#34;&#34;
    return deg * (3.1415 / 180.)</code></pre>
</details>
</dd>
<dt id="phi.math.dim_mask"><code class="name flex">
<span>def <span class="ident">dim_mask</span></span>(<span>all_dims: phi.math._shape.Shape, dims: Union[str, tuple, list, phi.math._shape.Shape, Callable], mask_dim=(vectorᶜ=None)) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a masked vector with 1 elements for <code>dims</code> and 0 for all other dimensions in <code>all_dims</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>all_dims</code></strong></dt>
<dd>All dimensions for which the vector should have an entry.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions marked as 1.</dd>
<dt><strong><code>mask_dim</code></strong></dt>
<dd>Dimension of the masked vector. Item names are assigned automatically.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dim_mask(all_dims: Shape or tuple or list, dims: DimFilter, mask_dim=channel(&#39;vector&#39;)) -&gt; Tensor:
    &#34;&#34;&#34;
    Creates a masked vector with 1 elements for `dims` and 0 for all other dimensions in `all_dims`.

    Args:
        all_dims: All dimensions for which the vector should have an entry.
        dims: Dimensions marked as 1.
        mask_dim: Dimension of the masked vector. Item names are assigned automatically.

    Returns:
        `Tensor`
    &#34;&#34;&#34;
    assert isinstance(all_dims, (Shape, tuple, list)), f&#34;all_dims must be a tuple or Shape but got {type(all_dims)}&#34;
    assert isinstance(mask_dim, Shape) and mask_dim.rank == 1, f&#34;mask_dim must be a single-dimension Shape but got {mask_dim}&#34;
    if isinstance(all_dims, (tuple, list)):
        all_dims = spatial(*all_dims)
    dims = all_dims.only(dims)
    mask = [1 if dim in dims else 0 for dim in all_dims]
    mask_dim = mask_dim.with_size(all_dims.names)
    return wrap(mask, mask_dim)</code></pre>
</details>
</dd>
<dt id="phi.math.divide_no_nan"><code class="name flex">
<span>def <span class="ident">divide_no_nan</span></span>(<span>x: float, y: float)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>x/y</em> with the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>s <code>x</code> and <code>y</code> but returns 0 where <em>y=0</em>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def divide_no_nan(x: float or Tensor, y: float or Tensor):
    &#34;&#34;&#34; Computes *x/y* with the `Tensor`s `x` and `y` but returns 0 where *y=0*. &#34;&#34;&#34;
    return custom_op2(x, y,
                      l_operator=divide_no_nan,
                      l_native_function=lambda x_, y_: choose_backend(x_, y_).divide_no_nan(x_, y_),
                      r_operator=lambda y_, x_: divide_no_nan(x_, y_),
                      r_native_function=lambda y_, x_: choose_backend(x_, y_).divide_no_nan(x_, y_),
                      op_name=&#39;divide_no_nan&#39;)</code></pre>
</details>
</dd>
<dt id="phi.math.dot"><code class="name flex">
<span>def <span class="ident">dot</span></span>(<span>x: phi.math._tensors.Tensor, x_dims: Union[str, tuple, list, phi.math._shape.Shape, Callable], y: phi.math._tensors.Tensor, y_dims: Union[str, tuple, list, phi.math._shape.Shape, Callable]) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the dot product along the specified dimensions.
Contracts <code>x_dims</code> with <code>y_dims</code> by first multiplying the elements and then summing them up.</p>
<p>For one dimension, this is equal to matrix-matrix or matrix-vector multiplication.</p>
<p>The function replaces the traditional <code><a title="phi.math.dot" href="#phi.math.dot">dot()</a></code> / <code>tensordot</code> / <code>matmul</code> / <code>einsum</code> functions.</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html"><code>numpy.tensordot</code></a>, <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html"><code>numpy.einsum</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.tensordot.html#torch.tensordot"><code>torch.tensordot</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.einsum.html"><code>torch.einsum</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/tensordot"><code>tf.tensordot</code></a>, <a href="https://www.tensorflow.org/api_docs/python/tf/einsum"><code>tf.einsum</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tensordot.html"><code>jax.numpy.tensordot</code></a>, <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.einsum.html"><code>jax.numpy.einsum</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>First <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>x_dims</code></strong></dt>
<dd>Dimensions of <code>x</code> to reduce against <code>y</code></dd>
<dt><strong><code>y</code></strong></dt>
<dd>Second <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>y_dims</code></strong></dt>
<dd>Dimensions of <code>y</code> to reduce against <code>x</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Dot product as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dot(x: Tensor,
        x_dims: DimFilter,
        y: Tensor,
        y_dims: DimFilter) -&gt; Tensor:
    &#34;&#34;&#34;
    Computes the dot product along the specified dimensions.
    Contracts `x_dims` with `y_dims` by first multiplying the elements and then summing them up.

    For one dimension, this is equal to matrix-matrix or matrix-vector multiplication.

    The function replaces the traditional `dot` / `tensordot` / `matmul` / `einsum` functions.

    * NumPy: [`numpy.tensordot`](https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html), [`numpy.einsum`](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html)
    * PyTorch: [`torch.tensordot`](https://pytorch.org/docs/stable/generated/torch.tensordot.html#torch.tensordot), [`torch.einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html)
    * TensorFlow: [`tf.tensordot`](https://www.tensorflow.org/api_docs/python/tf/tensordot), [`tf.einsum`](https://www.tensorflow.org/api_docs/python/tf/einsum)
    * Jax: [`jax.numpy.tensordot`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tensordot.html), [`jax.numpy.einsum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.einsum.html)

    Args:
        x: First `Tensor`
        x_dims: Dimensions of `x` to reduce against `y`
        y: Second `Tensor`
        y_dims: Dimensions of `y` to reduce against `x`.

    Returns:
        Dot product as `Tensor`.
    &#34;&#34;&#34;
    x_dims = x.shape.only(x_dims)
    y_dims = y.shape.only(y_dims)
    if not x_dims:
        assert y_dims.volume == 1, f&#34;Cannot compute dot product between dimensions {x_dims} on {x.shape} and {y_dims} on {y.shape}&#34;
        y = y[{d: 0 for d in y_dims.names}]
        return x * y
    if not y_dims:
        assert x_dims.volume == 1, f&#34;Cannot compute dot product between dimensions {x_dims} on {x.shape} and {y_dims} on {y.shape}&#34;
        x = x[{d: 0 for d in x_dims.names}]
        return x * y
    x_native = x.native(x.shape)
    y_native = y.native(y.shape)
    backend = choose_backend(x_native, y_native)
    remaining_shape_x = x.shape.without(x_dims)
    remaining_shape_y = y.shape.without(y_dims)
    assert x_dims.volume == y_dims.volume, f&#34;Failed to reduce {x_dims} against {y_dims} in dot product of {x.shape} and {y.shape}. Sizes do not match.&#34;
    if remaining_shape_y.only(remaining_shape_x).is_empty:  # no shared batch dimensions -&gt; tensordot
        result_native = backend.tensordot(x_native, x.shape.indices(x_dims), y_native, y.shape.indices(y_dims))
        result_shape = concat_shapes(remaining_shape_x, remaining_shape_y)
    else:  # shared batch dimensions -&gt; einsum
        result_shape = merge_shapes(x.shape.without(x_dims), y.shape.without(y_dims))
        REDUCE_LETTERS = list(&#39;ijklmn&#39;)
        KEEP_LETTERS = list(&#39;abcdefgh&#39;)
        x_letters = [(REDUCE_LETTERS if dim in x_dims else KEEP_LETTERS).pop(0) for dim in x.shape.names]
        letter_map = {dim: letter for dim, letter in zip(x.shape.names, x_letters)}
        REDUCE_LETTERS = list(&#39;ijklmn&#39;)
        y_letters = []
        for dim in y.shape.names:
            if dim in y_dims:
                y_letters.append(REDUCE_LETTERS.pop(0))
            else:
                if dim in x.shape and dim not in x_dims:
                    y_letters.append(letter_map[dim])
                else:
                    next_letter = KEEP_LETTERS.pop(0)
                    letter_map[dim] = next_letter
                    y_letters.append(next_letter)
        keep_letters = [letter_map[dim] for dim in result_shape.names]
        subscripts = f&#39;{&#34;&#34;.join(x_letters)},{&#34;&#34;.join(y_letters)}-&gt;{&#34;&#34;.join(keep_letters)}&#39;
        result_native = backend.einsum(subscripts, x_native, y_native)
    return NativeTensor(result_native, result_shape)</code></pre>
</details>
</dd>
<dt id="phi.math.downsample2x"><code class="name flex">
<span>def <span class="ident">downsample2x</span></span>(<span>grid: phi.math._tensors.Tensor, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function spatial&gt;) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Resamples a regular grid to half the number of spatial sample points per dimension.
The grid values at the new points are determined via mean (linear interpolation).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>full size grid</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>grid extrapolation. Used to insert an additional value for odd spatial dims</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>dims along which down-sampling is applied. If None, down-sample along all spatial dims.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>half-size grid</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def downsample2x(grid: Tensor,
                 padding: Extrapolation = extrapolation.BOUNDARY,
                 dims: DimFilter = spatial) -&gt; Tensor:
    &#34;&#34;&#34;
    Resamples a regular grid to half the number of spatial sample points per dimension.
    The grid values at the new points are determined via mean (linear interpolation).

    Args:
      grid: full size grid
      padding: grid extrapolation. Used to insert an additional value for odd spatial dims
      dims: dims along which down-sampling is applied. If None, down-sample along all spatial dims.
      grid: Tensor: 
      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
      dims: tuple or None:  (Default value = None)

    Returns:
      half-size grid

    &#34;&#34;&#34;
    dims = grid.shape.only(dims).names
    odd_dimensions = [dim for dim in dims if grid.shape.get_size(dim) % 2 != 0]
    grid = math.pad(grid, {dim: (0, 1) for dim in odd_dimensions}, padding)
    for dim in dims:
        grid = (grid[{dim: slice(1, None, 2)}] + grid[{dim: slice(0, None, 2)}]) / 2
    return grid</code></pre>
</details>
</dd>
<dt id="phi.math.dtype"><code class="name flex">
<span>def <span class="ident">dtype</span></span>(<span>x) ‑> phi.math.backend._dtype.DType</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the data type of <code>x</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or native tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.DType" href="#phi.math.DType">DType</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dtype(x) -&gt; DType:
    &#34;&#34;&#34;
    Returns the data type of `x`.

    Args:
        x: `Tensor` or native tensor.

    Returns:
        `DType`
    &#34;&#34;&#34;
    if isinstance(x, Tensor):
        return x.dtype
    else:
        return choose_backend(x).dtype(x)</code></pre>
</details>
</dd>
<dt id="phi.math.exp"><code class="name flex">
<span>def <span class="ident">exp</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>exp(x)</em> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def exp(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Computes *exp(x)* of the `Tensor` or `PhiTreeNode` `x`. &#34;&#34;&#34;
    return _backend_op1(x, Backend.exp)</code></pre>
</details>
</dd>
<dt id="phi.math.expand"><code class="name flex">
<span>def <span class="ident">expand</span></span>(<span>value, dims: phi.math._shape.Shape, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds dimensions to a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> by implicitly repeating the tensor values along the new dimensions.
If <code>value</code> already contains some of the new dimensions, a size and type check is performed instead.</p>
<p>This function replaces the usual <code>tile</code> / <code>repeat</code> functions of
<a href="https://numpy.org/doc/stable/reference/generated/numpy.tile.html">NumPy</a>,
<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.repeat">PyTorch</a>,
<a href="https://www.tensorflow.org/api_docs/python/tf/tile">TensorFlow</a> and
<a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tile.html">Jax</a>.</p>
<p>Additionally, it replaces the traditional <code>unsqueeze</code> / <code>expand_dims</code> functions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.magic.Shapable" href="magic.html#phi.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions to be added as <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as <code>value</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand(value, dims: Shape, **kwargs):
    &#34;&#34;&#34;
    Adds dimensions to a `Tensor` by implicitly repeating the tensor values along the new dimensions.
    If `value` already contains some of the new dimensions, a size and type check is performed instead.

    This function replaces the usual `tile` / `repeat` functions of
    [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.tile.html),
    [PyTorch](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.repeat),
    [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/tile) and
    [Jax](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.tile.html).

    Additionally, it replaces the traditional `unsqueeze` / `expand_dims` functions.

    Args:
        value: `phi.math.magic.Shapable`, such as `phi.math.Tensor`
        dims: Dimensions to be added as `Shape`
        **kwargs: Additional keyword arguments required by specific implementations.
            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
            Adding batch dimensions must always work without keyword arguments.

    Returns:
        Same type as `value`.
    &#34;&#34;&#34;
    merge_shapes(value, dims.only(shape(value)))  # check that existing sizes match
    if not dims.without(shape(value)):  # no new dims to add
        if set(dims) == set(shape(value).only(dims)):  # sizes and item names might differ, though
            return value
    # --- First try __stack__
    if hasattr(value, &#39;__expand__&#39;):
        result = value.__expand__(dims, **kwargs)
        if result is not NotImplemented:
            return result
    # --- Next try Tree Node ---
    if isinstance(value, PhiTreeNode):
        new_attributes = {a: expand(getattr(value, a), dims, **kwargs) for a in value_attributes(value)}
        return copy_with(value, **new_attributes)
    # --- Fallback: stack ---
    if hasattr(value, &#39;__stack__&#39;):
        if dims.volume &gt; 8:
            warnings.warn(f&#34;expand() default implementation is slow on large shapes {dims}. Please implement __expand__() for {type(value).__name__} as defined in phi.math.magic&#34;, RuntimeWarning, stacklevel=2)
        for dim in reversed(dims):
            value = stack((value,) * dim.size, dim, **kwargs)
            assert value is not NotImplemented, &#34;Value must implement either __expand__ or __stack__&#34;
        return value
    try:  # value may be a native scalar
        from ._ops import expand_tensor
        from ._tensors import wrap
        value = wrap(value)
    except ValueError:
        raise AssertionError(f&#34;Cannot expand non-shapable object {type(value)}&#34;)
    return expand_tensor(value, dims)</code></pre>
</details>
</dd>
<dt id="phi.math.fft"><code class="name flex">
<span>def <span class="ident">fft</span></span>(<span>x: phi.math._tensors.Tensor, dims: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function spatial&gt;) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Performs a fast Fourier transform (FFT) on all spatial dimensions of x.</p>
<p>The inverse operation is <code><a title="phi.math.ifft" href="#phi.math.ifft">ifft()</a></code>.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/generated/numpy.fft.fft.html"><code>np.fft.fft</code></a>,
<a href="https://numpy.org/doc/stable/reference/generated/numpy.fft.fft2.html"><code>numpy.fft.fft2</code></a>,
<a href="https://numpy.org/doc/stable/reference/generated/numpy.fft.fftn.html"><code>numpy.fft.fftn</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/fft.html"><code>torch.fft.fft</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/signal/fft"><code>tf.signal.fft</code></a>,
<a href="https://www.tensorflow.org/api_docs/python/tf/signal/fft2d"><code>tf.signal.fft2d</code></a>,
<a href="https://www.tensorflow.org/api_docs/python/tf/signal/fft3d"><code>tf.signal.fft3d</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fft.html"><code>jax.numpy.fft.fft</code></a>,
<a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fft2.html"><code>jax.numpy.fft.fft2</code></a>
<a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fftn.html"><code>jax.numpy.fft.fft</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Uniform complex or float <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with at least one spatial dimension.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions along which to perform the FFT.
If <code>None</code>, performs the FFT along all spatial dimensions of <code>x</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><em>Ƒ(x)</em> as complex <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fft(x: Tensor, dims: DimFilter = spatial) -&gt; Tensor:
    &#34;&#34;&#34;
    Performs a fast Fourier transform (FFT) on all spatial dimensions of x.
    
    The inverse operation is `ifft()`.

    Implementations:

    * NumPy: [`np.fft.fft`](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft.html),
      [`numpy.fft.fft2`](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft2.html),
      [`numpy.fft.fftn`](https://numpy.org/doc/stable/reference/generated/numpy.fft.fftn.html)
    * PyTorch: [`torch.fft.fft`](https://pytorch.org/docs/stable/fft.html)
    * TensorFlow: [`tf.signal.fft`](https://www.tensorflow.org/api_docs/python/tf/signal/fft),
      [`tf.signal.fft2d`](https://www.tensorflow.org/api_docs/python/tf/signal/fft2d),
      [`tf.signal.fft3d`](https://www.tensorflow.org/api_docs/python/tf/signal/fft3d)
    * Jax: [`jax.numpy.fft.fft`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fft.html),
      [`jax.numpy.fft.fft2`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fft2.html)
      [`jax.numpy.fft.fft`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fftn.html)

    Args:
        x: Uniform complex or float `Tensor` with at least one spatial dimension.
        dims: Dimensions along which to perform the FFT.
            If `None`, performs the FFT along all spatial dimensions of `x`.

    Returns:
        *Ƒ(x)* as complex `Tensor`
    &#34;&#34;&#34;
    dims = x.shape.only(dims)
    x_native = x.native(x.shape)
    result_native = choose_backend(x_native).fft(x_native, x.shape.indices(dims))
    return NativeTensor(result_native, x.shape)</code></pre>
</details>
</dd>
<dt id="phi.math.fftfreq"><code class="name flex">
<span>def <span class="ident">fftfreq</span></span>(<span>resolution: phi.math._shape.Shape, dx: phi.math._tensors.Tensor = 1, dtype: phi.math.backend._dtype.DType = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the discrete Fourier transform sample frequencies.
These are the frequencies corresponding to the components of the result of <code>math.fft</code> on a tensor of shape <code>resolution</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>resolution</code></strong></dt>
<dd>Grid resolution measured in cells</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Distance between sampling points in real space.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Data type of the returned tensor (Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> holding the frequencies of the corresponding values computed by math.fft</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fftfreq(resolution: Shape, dx: Tensor or float = 1, dtype: DType = None):
    &#34;&#34;&#34;
    Returns the discrete Fourier transform sample frequencies.
    These are the frequencies corresponding to the components of the result of `math.fft` on a tensor of shape `resolution`.

    Args:
        resolution: Grid resolution measured in cells
        dx: Distance between sampling points in real space.
        dtype: Data type of the returned tensor (Default value = None)

    Returns:
        `Tensor` holding the frequencies of the corresponding values computed by math.fft
    &#34;&#34;&#34;
    k = meshgrid(**{dim: np.fft.fftfreq(int(n)) for dim, n in resolution.spatial._named_sizes})
    k /= dx
    return to_float(k) if dtype is None else cast(k, dtype)</code></pre>
</details>
</dd>
<dt id="phi.math.finite_fill"><code class="name flex">
<span>def <span class="ident">finite_fill</span></span>(<span>values: phi.math._tensors.Tensor, dims: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function spatial&gt;, distance: int = 1, diagonal: bool = True, padding=boundary) ‑> Tuple[phi.math._tensors.Tensor, phi.math._tensors.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Fills non-finite (NaN, inf, -inf) values from nearby finite values.
Extrapolates the finite values of <code>values</code> for <code>distance</code> steps along <code>dims</code>.
Where multiple finite values could fill an invalid value, the average is computed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Floating-point <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>. All non-numeric values (<code>NaN</code>, <code>inf</code>, <code>-inf</code>) are interpreted as invalid.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions along which to fill invalid values from finite ones.</dd>
<dt><strong><code>distance</code></strong></dt>
<dd>Number of extrapolation steps, each extrapolating one cell out.</dd>
<dt><strong><code>diagonal</code></strong></dt>
<dd>Whether to extrapolate values to their diagonal neighbors per step.</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation of <code>values</code>. Determines whether to extrapolate from the edges as well.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of same shape as <code>values</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def finite_fill(values: Tensor, dims: DimFilter = spatial, distance: int = 1, diagonal: bool = True, padding=extrapolation.BOUNDARY) -&gt; Tuple[Tensor, Tensor]:
    &#34;&#34;&#34;
    Fills non-finite (NaN, inf, -inf) values from nearby finite values.
    Extrapolates the finite values of `values` for `distance` steps along `dims`.
    Where multiple finite values could fill an invalid value, the average is computed.

    Args:
        values: Floating-point `Tensor`. All non-numeric values (`NaN`, `inf`, `-inf`) are interpreted as invalid.
        dims: Dimensions along which to fill invalid values from finite ones.
        distance: Number of extrapolation steps, each extrapolating one cell out.
        diagonal: Whether to extrapolate values to their diagonal neighbors per step.
        padding: Extrapolation of `values`. Determines whether to extrapolate from the edges as well.

    Returns:
        `Tensor` of same shape as `values`.
    &#34;&#34;&#34;
    if diagonal:
        distance = min(distance, max(values.shape.sizes))
        dims = values.shape.only(dims)
        for _ in range(distance):
            valid = math.is_finite(values)
            valid_values = math.where(valid, values, 0)
            overlap = valid
            for dim in dims:
                values_l, values_r = shift(valid_values, (-1, 1), dims=dim, padding=padding)
                valid_values = math.sum_(values_l + values_r + valid_values, dim=&#39;shift&#39;)
                mask_l, mask_r = shift(overlap, (-1, 1), dims=dim, padding=padding)
                overlap = math.sum_(mask_l + mask_r + overlap, dim=&#39;shift&#39;)
            values = math.where(valid, values, valid_values / overlap)
    else:
        distance = min(distance, sum(values.shape.sizes))
        for _ in range(distance):
            neighbors = concat(shift(values, (-1, 1), dims, padding=padding, stack_dim=channel(&#39;neighbors&#39;)), &#39;neighbors&#39;)
            finite = math.is_finite(neighbors)
            avg_neighbors = math.sum_(math.where(finite, neighbors, 0), &#39;neighbors&#39;) / math.sum_(finite, &#39;neighbors&#39;)
            values = math.where(math.is_finite(values), values, avg_neighbors)
    return values</code></pre>
</details>
</dd>
<dt id="phi.math.finite_max"><code class="name flex">
<span>def <span class="ident">finite_max</span></span>(<span>value, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;, default: complex = nan)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the maximum along <code>dim</code> ignoring all non-finite values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
<dt><strong><code>default</code></strong></dt>
<dd>Value to use where no finite value was encountered.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def finite_max(value, dim: DimFilter = non_batch, default: complex or float = float(&#39;NaN&#39;)):
    &#34;&#34;&#34;
    Finds the maximum along `dim` ignoring all non-finite values.

    Args:
        value: `Tensor` or `list` / `tuple` of Tensors.
        dim: Dimension or dimensions to be reduced. One of

            * `None` to reduce all non-batch dimensions
            * `str` containing single dimension or comma-separated list of dimensions
            * `Tuple[str]` or `List[str]`
            * `Shape`
            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
            * `&#39;0&#39;` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors

        default: Value to use where no finite value was encountered.

    Returns:
        `Tensor` without the reduced dimensions.
    &#34;&#34;&#34;
    value_inf = where(is_finite(value), value, float(&#39;-inf&#39;))
    result_inf = max_(value_inf, dim)
    return where(is_finite(result_inf), result_inf, default)</code></pre>
</details>
</dd>
<dt id="phi.math.finite_mean"><code class="name flex">
<span>def <span class="ident">finite_mean</span></span>(<span>value, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;, default: complex = nan)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the mean value of all finite values in <code>value</code> along <code>dim</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
<dt><strong><code>default</code></strong></dt>
<dd>Value to use where no finite value was encountered.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def finite_mean(value, dim: DimFilter = non_batch, default: complex or float = float(&#39;NaN&#39;)):
    &#34;&#34;&#34;
    Computes the mean value of all finite values in `value` along `dim`.

    Args:
        value: `Tensor` or `list` / `tuple` of Tensors.
        dim: Dimension or dimensions to be reduced. One of

            * `None` to reduce all non-batch dimensions
            * `str` containing single dimension or comma-separated list of dimensions
            * `Tuple[str]` or `List[str]`
            * `Shape`
            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
            * `&#39;0&#39;` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors

        default: Value to use where no finite value was encountered.

    Returns:
        `Tensor` without the reduced dimensions.
    &#34;&#34;&#34;
    finite = is_finite(value)
    summed = sum_(where(finite, value, 0), dim)
    count = sum_(finite, dim)
    mean_nan = summed / count
    return where(is_finite(mean_nan), mean_nan, default)</code></pre>
</details>
</dd>
<dt id="phi.math.finite_min"><code class="name flex">
<span>def <span class="ident">finite_min</span></span>(<span>value, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;, default: complex = nan)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the minimum along <code>dim</code> ignoring all non-finite values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
<dt><strong><code>default</code></strong></dt>
<dd>Value to use where no finite value was encountered.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def finite_min(value, dim: DimFilter = non_batch, default: complex or float = float(&#39;NaN&#39;)):
    &#34;&#34;&#34;
    Finds the minimum along `dim` ignoring all non-finite values.

    Args:
        value: `Tensor` or `list` / `tuple` of Tensors.
        dim: Dimension or dimensions to be reduced. One of

            * `None` to reduce all non-batch dimensions
            * `str` containing single dimension or comma-separated list of dimensions
            * `Tuple[str]` or `List[str]`
            * `Shape`
            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
            * `&#39;0&#39;` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors

        default: Value to use where no finite value was encountered.

    Returns:
        `Tensor` without the reduced dimensions.
    &#34;&#34;&#34;
    value_inf = where(is_finite(value), value, float(&#39;inf&#39;))
    result_inf = min_(value_inf, dim)
    return where(is_finite(result_inf), result_inf, default)</code></pre>
</details>
</dd>
<dt id="phi.math.finite_sum"><code class="name flex">
<span>def <span class="ident">finite_sum</span></span>(<span>value, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;, default: complex = nan)</span>
</code></dt>
<dd>
<div class="desc"><p>Sums all finite values in <code>value</code> along <code>dim</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
<dt><strong><code>default</code></strong></dt>
<dd>Value to use where no finite value was encountered.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def finite_sum(value, dim: DimFilter = non_batch, default: complex or float = float(&#39;NaN&#39;)):
    &#34;&#34;&#34;
    Sums all finite values in `value` along `dim`.

    Args:
        value: `Tensor` or `list` / `tuple` of Tensors.
        dim: Dimension or dimensions to be reduced. One of

            * `None` to reduce all non-batch dimensions
            * `str` containing single dimension or comma-separated list of dimensions
            * `Tuple[str]` or `List[str]`
            * `Shape`
            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
            * `&#39;0&#39;` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors

        default: Value to use where no finite value was encountered.

    Returns:
        `Tensor` without the reduced dimensions.
    &#34;&#34;&#34;
    finite = is_finite(value)
    summed = sum_(where(finite, value, 0), dim)
    return where(any_(finite, dim), summed, default)</code></pre>
</details>
</dd>
<dt id="phi.math.flatten"><code class="name flex">
<span>def <span class="ident">flatten</span></span>(<span>value, flat_dim: phi.math._shape.Shape = (flatⁱ=None), flatten_batch=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with the same values as <code>value</code> but only a single dimension <code>flat_dim</code>.
The order of the values in memory is not changed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.magic.Shapable" href="magic.html#phi.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</dd>
<dt><strong><code>flat_dim</code></strong></dt>
<dd>Dimension name and type as <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object. The size is ignored.</dd>
<dt><strong><code>flatten_batch</code></strong></dt>
<dd>Whether to flatten batch dimensions as well.
If <code>False</code>, batch dimensions are kept, only onn-batch dimensions are flattened.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as <code>value</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">flatten(math.zeros(spatial(x=4, y=3)))
# Out: (flatⁱ=12) const 0.0
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatten(value, flat_dim: Shape = instance(&#39;flat&#39;), flatten_batch=False, **kwargs):
    &#34;&#34;&#34;
    Returns a `Tensor` with the same values as `value` but only a single dimension `flat_dim`.
    The order of the values in memory is not changed.

    Args:
        value: `phi.math.magic.Shapable`, such as `Tensor`.
        flat_dim: Dimension name and type as `Shape` object. The size is ignored.
        flatten_batch: Whether to flatten batch dimensions as well.
            If `False`, batch dimensions are kept, only onn-batch dimensions are flattened.
        **kwargs: Additional keyword arguments required by specific implementations.
            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
            Adding batch dimensions must always work without keyword arguments.

    Returns:
        Same type as `value`.

    Examples:
        ```python
        flatten(math.zeros(spatial(x=4, y=3)))
        # Out: (flatⁱ=12) const 0.0
        ```
    &#34;&#34;&#34;
    assert isinstance(flat_dim, Shape) and flat_dim.rank == 1, flat_dim
    assert isinstance(value, Shapable) and isinstance(value, Shaped), f&#34;value must be Shapable but got {type(value)}&#34;
    # --- First try __flatten__ ---
    if hasattr(value, &#39;__flatten__&#39;):
        result = value.__flatten__(flat_dim, flatten_batch, **kwargs)
        if result is not NotImplemented:
            return result
    # There is no tree node implementation for flatten because pack_dims is just as fast
    # --- Fallback: pack_dims ---
    return pack_dims(value, shape(value) if flatten_batch else non_batch(value), flat_dim, **kwargs)</code></pre>
</details>
</dd>
<dt id="phi.math.floor"><code class="name flex">
<span>def <span class="ident">floor</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>⌊x⌋</em> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def floor(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Computes *⌊x⌋* of the `Tensor` or `PhiTreeNode` `x`. &#34;&#34;&#34;
    return _backend_op1(x, Backend.floor)</code></pre>
</details>
</dd>
<dt id="phi.math.fourier_laplace"><code class="name flex">
<span>def <span class="ident">fourier_laplace</span></span>(<span>grid: phi.math._tensors.Tensor, dx: phi.math._tensors.Tensor, times: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies the spatial laplace operator to the given tensor with periodic boundary conditions.</p>
<p><em>Note:</em> The results of <code><a title="phi.math.fourier_laplace" href="#phi.math.fourier_laplace">fourier_laplace()</a></code> and <code><a title="phi.math.laplace" href="#phi.math.laplace">laplace()</a></code> are close but not identical.</p>
<p>This implementation computes the laplace operator in Fourier space.
The result for periodic fields is exact, i.e. no numerical instabilities can occur, even for higher-order derivatives.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>tensor, assumed to have periodic boundary conditions</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>distance between grid points, tensor-like, scalar or vector</dd>
<dt><strong><code>times</code></strong></dt>
<dd>number of times the laplace operator is applied. The computational cost is independent of this parameter.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Tensor or Shape or float or list or tuple: </dd>
<dt><strong><code>times</code></strong></dt>
<dd>int:
(Default value = 1)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of same shape as <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fourier_laplace(grid: Tensor,
                    dx: Tensor or Shape or float or list or tuple,
                    times: int = 1):
    &#34;&#34;&#34;
    Applies the spatial laplace operator to the given tensor with periodic boundary conditions.
    
    *Note:* The results of `fourier_laplace` and `laplace` are close but not identical.
    
    This implementation computes the laplace operator in Fourier space.
    The result for periodic fields is exact, i.e. no numerical instabilities can occur, even for higher-order derivatives.

    Args:
      grid: tensor, assumed to have periodic boundary conditions
      dx: distance between grid points, tensor-like, scalar or vector
      times: number of times the laplace operator is applied. The computational cost is independent of this parameter.
      grid: Tensor: 
      dx: Tensor or Shape or float or list or tuple: 
      times: int:  (Default value = 1)

    Returns:
      tensor of same shape as `tensor`

    &#34;&#34;&#34;
    frequencies = math.fft(math.to_complex(grid))
    k_squared = math.sum_(math.fftfreq(grid.shape) ** 2, &#39;vector&#39;)
    fft_laplace = -(2 * np.pi) ** 2 * k_squared
    result = math.real(math.ifft(frequencies * fft_laplace ** times))
    return math.cast(result / wrap(dx) ** 2, grid.dtype)</code></pre>
</details>
</dd>
<dt id="phi.math.fourier_poisson"><code class="name flex">
<span>def <span class="ident">fourier_poisson</span></span>(<span>grid: phi.math._tensors.Tensor, dx: phi.math._tensors.Tensor, times: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Inverse operation to <code><a title="phi.math.fourier_laplace" href="#phi.math.fourier_laplace">fourier_laplace()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Tensor or Shape or float or list or tuple: </dd>
<dt><strong><code>times</code></strong></dt>
<dd>int:
(Default value = 1)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fourier_poisson(grid: Tensor,
                    dx: Tensor or Shape or float or list or tuple,
                    times: int = 1):
    &#34;&#34;&#34;
    Inverse operation to `fourier_laplace`.

    Args:
      grid: Tensor: 
      dx: Tensor or Shape or float or list or tuple: 
      times: int:  (Default value = 1)

    Returns:

    &#34;&#34;&#34;
    frequencies = math.fft(math.to_complex(grid))
    k_squared = math.sum_(math.fftfreq(grid.shape) ** 2, &#39;vector&#39;)
    fft_laplace = -(2 * np.pi) ** 2 * k_squared
    # fft_laplace.tensor[(0,) * math.ndims(k_squared)] = math.inf  # assume NumPy array to edit
    result = math.real(math.ifft(math.divide_no_nan(frequencies, math.to_complex(fft_laplace ** times))))
    return math.cast(result * wrap(dx) ** 2, grid.dtype)</code></pre>
</details>
</dd>
<dt id="phi.math.frequency_loss"><code class="name flex">
<span>def <span class="ident">frequency_loss</span></span>(<span>x, frequency_falloff: float = 100, threshold=1e-05, ignore_mean=False, n=2) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Penalizes the squared <code>values</code> in frequency (Fourier) space.
Lower frequencies are weighted more strongly then higher frequencies, depending on <code>frequency_falloff</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> Values to penalize, typically <code>actual - target</code>.</dd>
<dt><strong><code>frequency_falloff</code></strong></dt>
<dd>Large values put more emphasis on lower frequencies, 1.0 weights all frequencies equally.
<em>Note</em>: The total loss is not normalized. Varying the value will result in losses of different magnitudes.</dd>
<dt><strong><code>threshold</code></strong></dt>
<dd>Frequency amplitudes below this value are ignored.
Setting this to zero may cause infinities or NaN values during backpropagation.</dd>
<dt><strong><code>ignore_mean</code></strong></dt>
<dd>If <code>True</code>, does not penalize the mean value (frequency=0 component).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Scalar loss value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def frequency_loss(x,
                   frequency_falloff: float = 100,
                   threshold=1e-5,
                   ignore_mean=False,
                   n=2) -&gt; Tensor:
    &#34;&#34;&#34;
    Penalizes the squared `values` in frequency (Fourier) space.
    Lower frequencies are weighted more strongly then higher frequencies, depending on `frequency_falloff`.

    Args:
        x: `Tensor` or `PhiTreeNode` Values to penalize, typically `actual - target`.
        frequency_falloff: Large values put more emphasis on lower frequencies, 1.0 weights all frequencies equally.
            *Note*: The total loss is not normalized. Varying the value will result in losses of different magnitudes.
        threshold: Frequency amplitudes below this value are ignored.
            Setting this to zero may cause infinities or NaN values during backpropagation.
        ignore_mean: If `True`, does not penalize the mean value (frequency=0 component).

    Returns:
      Scalar loss value
    &#34;&#34;&#34;
    assert n in (1, 2)
    if isinstance(x, Tensor):
        if ignore_mean:
            x -= math.mean(x, x.shape.non_batch)
        k_squared = vec_squared(math.fftfreq(x.shape.spatial))
        weights = math.exp(-0.5 * k_squared * frequency_falloff ** 2)

        diff_fft = abs_square(math.fft(x) * weights)
        diff_fft = math.sqrt(math.maximum(diff_fft, threshold))
        return l2_loss(diff_fft) if n == 2 else l1_loss(diff_fft)
    elif isinstance(x, PhiTreeNode):
        losses = [frequency_loss(getattr(x, a), frequency_falloff, threshold, ignore_mean, n) for a in variable_values(x)]
        return sum(losses)
    else:
        raise ValueError(x)</code></pre>
</details>
</dd>
<dt id="phi.math.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>dict_: dict, convert=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> from a serialized form.</p>
<p>See Also:
<code><a title="phi.math.to_dict" href="#phi.math.to_dict">to_dict()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dict_</code></strong></dt>
<dd>Serialized tensor properties.</dd>
<dt><strong><code>convert</code></strong></dt>
<dd>Whether to convert the data to the current backend format or keep it as a Numpy array.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def from_dict(dict_: dict, convert=False):
    &#34;&#34;&#34;
    Loads a `Tensor` or `Shape` from a serialized form.

    See Also:
        `to_dict()`.

    Args:
        dict_: Serialized tensor properties.
        convert: Whether to convert the data to the current backend format or keep it as a Numpy array.

    Returns:
        `Tensor` or `Shape`.
    &#34;&#34;&#34;
    shape = Shape._from_dict(dict_)
    if &#39;data&#39; in dict_:
        return tensor(dict_[&#39;data&#39;], shape, convert=convert)
    else:
        return shape</code></pre>
</details>
</dd>
<dt id="phi.math.functional_gradient"><code class="name flex">
<span>def <span class="ident">functional_gradient</span></span>(<span>f: Callable, wrt: str = None, get_output=True) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a function which computes the gradient of <code>f</code>.</p>
<p>Example:</p>
<pre><code class="language-python">def loss_function(x, y):
    prediction = f(x)
    loss = math.l2_loss(prediction - y)
    return loss, prediction

dx = functional_gradient(loss_function, 'x', get_output=False)(x, y)

(loss, prediction), (dx, dy) = functional_gradient(loss_function,
                                        'x,y', get_output=True)(x, y)
</code></pre>
<p>Functional gradients are implemented for the following backends:</p>
<ul>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad"><code>torch.autograd.grad</code></a> / <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward"><code>torch.autograd.backward</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code>tf.GradientTape</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/jax.html#jax.grad"><code>jax.grad</code></a></li>
</ul>
<p>When the gradient function is invoked, <code>f</code> is called with tensors that track the gradient.
For PyTorch, <code>arg.requires_grad = True</code> for all positional arguments of <code>f</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be differentiated.
<code>f</code> must return a floating point <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with rank zero.
It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if <code>return_values=True</code>.
All arguments for which the gradient is computed must be of dtype float or complex.</dd>
<dt><strong><code>get_output</code></strong></dt>
<dd>Whether the gradient function should also return the return values of <code>f</code>.</dd>
<dt><strong><code>wrt</code></strong></dt>
<dd>Comma-separated parameter names of <code>f</code> with respect to which the gradient should be computed.
If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with the same arguments as <code>f</code> that returns the value of <code>f</code>, auxiliary data and gradient of <code>f</code> if <code>get_output=True</code>, else just the gradient of <code>f</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def functional_gradient(f: Callable, wrt: str = None, get_output=True) -&gt; Callable:
    &#34;&#34;&#34;
    Creates a function which computes the gradient of `f`.

    Example:
    ```python
    def loss_function(x, y):
        prediction = f(x)
        loss = math.l2_loss(prediction - y)
        return loss, prediction

    dx = functional_gradient(loss_function, &#39;x&#39;, get_output=False)(x, y)

    (loss, prediction), (dx, dy) = functional_gradient(loss_function,
                                            &#39;x,y&#39;, get_output=True)(x, y)
    ```

    Functional gradients are implemented for the following backends:

    * PyTorch: [`torch.autograd.grad`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad) / [`torch.autograd.backward`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward)
    * TensorFlow: [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)
    * Jax: [`jax.grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)

    When the gradient function is invoked, `f` is called with tensors that track the gradient.
    For PyTorch, `arg.requires_grad = True` for all positional arguments of `f`.

    Args:
        f: Function to be differentiated.
            `f` must return a floating point `Tensor` with rank zero.
            It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if `return_values=True`.
            All arguments for which the gradient is computed must be of dtype float or complex.
        get_output: Whether the gradient function should also return the return values of `f`.
        wrt: Comma-separated parameter names of `f` with respect to which the gradient should be computed.
            If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).

    Returns:
        Function with the same arguments as `f` that returns the value of `f`, auxiliary data and gradient of `f` if `get_output=True`, else just the gradient of `f`.
    &#34;&#34;&#34;
    f_params, wrt = simplify_wrt(f, wrt)
    return GradientFunction(f, f_params, wrt, get_output, is_f_scalar=True)</code></pre>
</details>
</dd>
<dt id="phi.math.gather"><code class="name flex">
<span>def <span class="ident">gather</span></span>(<span>values: phi.math._tensors.Tensor, indices: phi.math._tensors.Tensor, dims: Union[str, tuple, list, phi.math._shape.Shape, Callable] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Gathers the entries of <code>values</code> at positions described by <code>indices</code>.</p>
<p>See Also:
<code><a title="phi.math.scatter" href="#phi.math.scatter">scatter()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> containing values to gather.</dd>
<dt><strong><code>indices</code></strong></dt>
<dd><code>int</code> <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>. Multi-dimensional position references in <code>values</code>.
Must contain a single channel dimension for the index vector matching the number of <code>dims</code>.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions indexed by <code>indices</code>.
If <code>None</code>, will default to all spatial dimensions or all instance dimensions, depending on which ones are present (but not both).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with combined batch dimensions, channel dimensions of <code>values</code> and spatial/instance dimensions of <code>indices</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gather(values: Tensor, indices: Tensor, dims: DimFilter or None = None):
    &#34;&#34;&#34;
    Gathers the entries of `values` at positions described by `indices`.

    See Also:
        `scatter()`.

    Args:
        values: `Tensor` containing values to gather.
        indices: `int` `Tensor`. Multi-dimensional position references in `values`.
            Must contain a single channel dimension for the index vector matching the number of `dims`.
        dims: Dimensions indexed by `indices`.
            If `None`, will default to all spatial dimensions or all instance dimensions, depending on which ones are present (but not both).

    Returns:
        `Tensor` with combined batch dimensions, channel dimensions of `values` and spatial/instance dimensions of `indices`.
    &#34;&#34;&#34;
    if dims is None:
        assert values.shape.instance.is_empty or values.shape.spatial.is_empty, f&#34;Specify gather dimensions for values with both instance and spatial dimensions. Got {values.shape}&#34;
        dims = values.shape.instance if values.shape.spatial.is_empty else values.shape.spatial
    if indices.dtype.kind == bool:
        indices = to_int32(indices)
    dims = parse_dim_order(dims)
    batch = (values.shape.batch &amp; indices.shape.batch).without(dims)
    channel = values.shape.without(dims).without(batch)
    native_values = reshaped_native(values, [batch, *dims, channel])
    native_indices = reshaped_native(indices, [batch, *indices.shape.non_batch.non_channel, indices.shape.channel])
    backend = choose_backend(native_values, native_indices)
    native_result = backend.batched_gather_nd(native_values, native_indices)
    result = reshaped_tensor(native_result, [batch, *indices.shape.non_channel.non_batch, channel])
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.get_precision"><code class="name flex">
<span>def <span class="ident">get_precision</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the current target floating point precision in bits.
The precision can be set globally using <code><a title="phi.math.set_global_precision" href="#phi.math.set_global_precision">set_global_precision()</a></code> or locally using <code>with precision(p):</code>.</p>
<p>Any Backend method may convert floating point values to this precision, even if the input had a different precision.</p>
<h2 id="returns">Returns</h2>
<p>16 for half, 32 for single, 64 for double</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_precision() -&gt; int:
    &#34;&#34;&#34;
    Gets the current target floating point precision in bits.
    The precision can be set globally using `set_global_precision()` or locally using `with precision(p):`.

    Any Backend method may convert floating point values to this precision, even if the input had a different precision.

    Returns:
        16 for half, 32 for single, 64 for double
    &#34;&#34;&#34;
    return _PRECISION[-1]</code></pre>
</details>
</dd>
<dt id="phi.math.gradient"><code class="name flex">
<span>def <span class="ident">gradient</span></span>(<span>f: Callable, wrt: str = None, get_output=True) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a function which computes the Jacobian matrix of <code>f</code>.
For scalar functions, consider using <code><a title="phi.math.functional_gradient" href="#phi.math.functional_gradient">functional_gradient()</a></code> instead.</p>
<p>Example:</p>
<pre><code class="language-python">def f(x, y):
    prediction = f(x)
    loss = math.l2_loss(prediction - y)
    return loss, prediction

dx = jacobian(loss_function, wrt='x', get_output=False)(x, y)

(loss, prediction), (dx, dy) = jacobian(loss_function,
                                    wrt='x,y', get_output=True)(x, y)
</code></pre>
<p>Functional gradients are implemented for the following backends:</p>
<ul>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad"><code>torch.autograd.grad</code></a> / <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward"><code>torch.autograd.backward</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code>tf.GradientTape</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/jax.html#jax.grad"><code>jax.grad</code></a></li>
</ul>
<p>When the gradient function is invoked, <code>f</code> is called with tensors that track the gradient.
For PyTorch, <code>arg.requires_grad = True</code> for all positional arguments of <code>f</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be differentiated.
<code>f</code> must return a floating point <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with rank zero.
It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if <code>return_values=True</code>.
All arguments for which the gradient is computed must be of dtype float or complex.</dd>
<dt><strong><code>get_output</code></strong></dt>
<dd>Whether the gradient function should also return the return values of <code>f</code>.</dd>
<dt><strong><code>wrt</code></strong></dt>
<dd>Comma-separated parameter names of <code>f</code> with respect to which the gradient should be computed.
If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with the same arguments as <code>f</code> that returns the value of <code>f</code>, auxiliary data and Jacobian of <code>f</code> if <code>get_output=True</code>, else just the Jacobian of <code>f</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def jacobian(f: Callable, wrt: str = None, get_output=True) -&gt; Callable:
    &#34;&#34;&#34;
    Creates a function which computes the Jacobian matrix of `f`.
    For scalar functions, consider using `functional_gradient()` instead.

    Example:
    ```python
    def f(x, y):
        prediction = f(x)
        loss = math.l2_loss(prediction - y)
        return loss, prediction

    dx = jacobian(loss_function, wrt=&#39;x&#39;, get_output=False)(x, y)

    (loss, prediction), (dx, dy) = jacobian(loss_function,
                                        wrt=&#39;x,y&#39;, get_output=True)(x, y)
    ```

    Functional gradients are implemented for the following backends:

    * PyTorch: [`torch.autograd.grad`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad) / [`torch.autograd.backward`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward)
    * TensorFlow: [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)
    * Jax: [`jax.grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)

    When the gradient function is invoked, `f` is called with tensors that track the gradient.
    For PyTorch, `arg.requires_grad = True` for all positional arguments of `f`.

    Args:
        f: Function to be differentiated.
            `f` must return a floating point `Tensor` with rank zero.
            It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if `return_values=True`.
            All arguments for which the gradient is computed must be of dtype float or complex.
        get_output: Whether the gradient function should also return the return values of `f`.
        wrt: Comma-separated parameter names of `f` with respect to which the gradient should be computed.
            If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).

    Returns:
        Function with the same arguments as `f` that returns the value of `f`, auxiliary data and Jacobian of `f` if `get_output=True`, else just the Jacobian of `f`.
    &#34;&#34;&#34;
    f_params, wrt = simplify_wrt(f, wrt)
    return GradientFunction(f, f_params, wrt, get_output, is_f_scalar=False)</code></pre>
</details>
</dd>
<dt id="phi.math.grid_sample"><code class="name flex">
<span>def <span class="ident">grid_sample</span></span>(<span>grid: phi.math._tensors.Tensor, coordinates: phi.math._tensors.Tensor, extrap: e_.Extrapolation, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Samples values of <code>grid</code> at the locations referenced by <code>coordinates</code>.
Values lying in between sample points are determined via linear interpolation.</p>
<p>For values outside the valid bounds of <code>grid</code> (<code>coord &lt; 0 or coord &gt; grid.shape - 1</code>), <code>extrap</code> is used to determine the neighboring grid values.
If the extrapolation does not support resampling, the grid is padded by one cell layer before resampling.
In that case, values lying further outside will not be sampled according to the extrapolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>Grid with at least one spatial dimension and no instance dimensions.</dd>
<dt><strong><code>coordinates</code></strong></dt>
<dd>Coordinates with a single channel dimension called <code>'vector'</code>.
The size of the <code>vector</code> dimension must match the number of spatial dimensions of <code>grid</code>.</dd>
<dt><strong><code>extrap</code></strong></dt>
<dd>Extrapolation used to determine the values of <code>grid</code> outside its valid bounds.</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>Additional information for the extrapolation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with channel dimensions of <code>grid</code>, spatial and instance dimensions of <code>coordinates</code> and combined batch dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def grid_sample(grid: Tensor, coordinates: Tensor, extrap: &#39;e_.Extrapolation&#39;, **kwargs):
    &#34;&#34;&#34;
    Samples values of `grid` at the locations referenced by `coordinates`.
    Values lying in between sample points are determined via linear interpolation.

    For values outside the valid bounds of `grid` (`coord &lt; 0 or coord &gt; grid.shape - 1`), `extrap` is used to determine the neighboring grid values.
    If the extrapolation does not support resampling, the grid is padded by one cell layer before resampling.
    In that case, values lying further outside will not be sampled according to the extrapolation.

    Args:
        grid: Grid with at least one spatial dimension and no instance dimensions.
        coordinates: Coordinates with a single channel dimension called `&#39;vector&#39;`.
            The size of the `vector` dimension must match the number of spatial dimensions of `grid`.
        extrap: Extrapolation used to determine the values of `grid` outside its valid bounds.
        kwargs: Additional information for the extrapolation.

    Returns:
        `Tensor` with channel dimensions of `grid`, spatial and instance dimensions of `coordinates` and combined batch dimensions.
    &#34;&#34;&#34;
    result = broadcast_op(functools.partial(_grid_sample, extrap=extrap, pad_kwargs=kwargs), [grid, coordinates])
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.hessian"><code class="name flex">
<span>def <span class="ident">hessian</span></span>(<span>f: Callable, wrt: str, get_output=True, get_gradient=True, dim_suffixes=('', '_')) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p><em>Experimental. This function currently only supports PyTorch and the Hessian can only be computed w.r.t. one argument.</em></p>
<p>Creates a function which computes the Hessian (second derivative) of <code>f</code>.</p>
<p>Example:</p>
<pre><code class="language-python">def loss_function(x, y):
    prediction = f(x)
    loss = math.l2_loss(prediction - y)
    return loss, prediction

hess, = hessian(loss_function, 'x', get_output=False, get_gradient=False)(x, y)

(loss, prediction), (dx, dy), ((dx_dx, dx_dy), (dy_dx, dy_dy)) = hessian(loss_function,
                                    wrt='x,y', get_output=True)(x, y)
</code></pre>
<p>When the gradient function is invoked, <code>f</code> is called with tensors that track the gradient.
For PyTorch, <code>arg.requires_grad = True</code> for all positional arguments of <code>f</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be differentiated.
<code>f</code> must return a floating point <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with rank zero.
It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if <code>return_values=True</code>.
All arguments for which the gradient is computed must be of dtype float or complex.</dd>
<dt><strong><code>wrt</code></strong></dt>
<dd>Comma-separated parameter names of <code>f</code> with respect to which the gradient should be computed.
If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).</dd>
<dt><strong><code>get_output</code></strong></dt>
<dd>Whether the Hessian function should also return the return values of <code>f</code>.</dd>
<dt><strong><code>get_gradient</code></strong></dt>
<dd>Whether the Hessian function should also return the gradient of <code>f</code>.</dd>
<dt><strong><code>dim_suffixes</code></strong></dt>
<dd><code>tuple</code> containing two strings.
All Non-batch dimensions of the parameters occur twice in the corresponding Hessian.
To avoid duplicate names, suffixes are added to non-batch dimensions.
The dimensions from the first derivative computation are appended with <code>dim_suffixes[0]</code> and the second ones with <code>dim_suffixes[1]</code>.
This argument has no effect on the dimension names of the gradient if <code>get_gradient=True</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with the same arguments as <code>f</code> that returns <code>(f(x), g(x), H(x))</code> or less depending on <code>get_output</code> and <code>get_gradient</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hessian(f: Callable, wrt: str, get_output=True, get_gradient=True, dim_suffixes=(&#39;&#39;, &#39;_&#39;)) -&gt; Callable:
    &#34;&#34;&#34;
    *Experimental. This function currently only supports PyTorch and the Hessian can only be computed w.r.t. one argument.*

    Creates a function which computes the Hessian (second derivative) of `f`.

    Example:
    ```python
    def loss_function(x, y):
        prediction = f(x)
        loss = math.l2_loss(prediction - y)
        return loss, prediction

    hess, = hessian(loss_function, &#39;x&#39;, get_output=False, get_gradient=False)(x, y)

    (loss, prediction), (dx, dy), ((dx_dx, dx_dy), (dy_dx, dy_dy)) = hessian(loss_function,
                                        wrt=&#39;x,y&#39;, get_output=True)(x, y)
    ```

    When the gradient function is invoked, `f` is called with tensors that track the gradient.
    For PyTorch, `arg.requires_grad = True` for all positional arguments of `f`.

    Args:
        f: Function to be differentiated.
            `f` must return a floating point `Tensor` with rank zero.
            It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if `return_values=True`.
            All arguments for which the gradient is computed must be of dtype float or complex.
        wrt: Comma-separated parameter names of `f` with respect to which the gradient should be computed.
            If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).
        get_output: Whether the Hessian function should also return the return values of `f`.
        get_gradient: Whether the Hessian function should also return the gradient of `f`.
        dim_suffixes: `tuple` containing two strings.
            All Non-batch dimensions of the parameters occur twice in the corresponding Hessian.
            To avoid duplicate names, suffixes are added to non-batch dimensions.
            The dimensions from the first derivative computation are appended with `dim_suffixes[0]` and the second ones with `dim_suffixes[1]`.
            This argument has no effect on the dimension names of the gradient if `get_gradient=True`.

    Returns:
        Function with the same arguments as `f` that returns `(f(x), g(x), H(x))` or less depending on `get_output` and `get_gradient`.
    &#34;&#34;&#34;
    f_params, wrt = simplify_wrt(f, wrt)
    return HessianFunction(f, f_params, wrt, get_output, get_gradient, dim_suffixes)</code></pre>
</details>
</dd>
<dt id="phi.math.ifft"><code class="name flex">
<span>def <span class="ident">ifft</span></span>(<span>k: phi.math._tensors.Tensor, dims: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function spatial&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Inverse of <code><a title="phi.math.fft" href="#phi.math.fft">fft()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>k</code></strong></dt>
<dd>Complex or float <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with at least one spatial dimension.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions along which to perform the inverse FFT.
If <code>None</code>, performs the inverse FFT along all spatial dimensions of <code>k</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><em>Ƒ<sup>-1</sup>(k)</em> as complex <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ifft(k: Tensor, dims: DimFilter = spatial):
    &#34;&#34;&#34;
    Inverse of `fft()`.

    Args:
        k: Complex or float `Tensor` with at least one spatial dimension.
        dims: Dimensions along which to perform the inverse FFT.
            If `None`, performs the inverse FFT along all spatial dimensions of `k`.

    Returns:
        *Ƒ&lt;sup&gt;-1&lt;/sup&gt;(k)* as complex `Tensor`
    &#34;&#34;&#34;
    dims = k.shape.only(dims)
    k_native = k.native(k.shape)
    result_native = choose_backend(k_native).ifft(k_native, k.shape.indices(dims))
    return NativeTensor(result_native, k.shape)</code></pre>
</details>
</dd>
<dt id="phi.math.imag"><code class="name flex">
<span>def <span class="ident">imag</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the imaginary part of <code>x</code>.
If <code>x</code> does not store complex numbers, returns a zero tensor with the same shape and dtype as this tensor.</p>
<p>See Also:
<code><a title="phi.math.real" href="#phi.math.real">real()</a></code>, <code><a title="phi.math.conjugate" href="#phi.math.conjugate">conjugate()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> or native tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Imaginary component of <code>x</code> if <code>x</code> is complex, zeros otherwise.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def imag(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34;
    Returns the imaginary part of `x`.
    If `x` does not store complex numbers, returns a zero tensor with the same shape and dtype as this tensor.

    See Also:
        `real()`, `conjugate()`.

    Args:
        x: `Tensor` or `PhiTreeNode` or native tensor.

    Returns:
        Imaginary component of `x` if `x` is complex, zeros otherwise.
    &#34;&#34;&#34;
    return _backend_op1(x, Backend.imag)</code></pre>
</details>
</dd>
<dt id="phi.math.instance"><code class="name flex">
<span>def <span class="ident">instance</span></span>(<span>*args, **dims: int) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the instance dimensions of an existing <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or creates a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> with only instance dimensions.</p>
<p>Usage for filtering instance dimensions:</p>
<pre><code class="language-python">instance_dims = instance(shape)
instance_dims = instance(tensor)
</code></pre>
<p>Usage for creating a <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> with only instance dimensions:</p>
<pre><code class="language-python">instance_shape = instance('undef', points=2)
# Out: (points=2, undef=None)
</code></pre>
<p>Here, the dimension <code>undef</code> is created with an undefined size of <code>None</code>.
Undefined sizes are automatically filled in by <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code>, <code><a title="phi.math.wrap" href="#phi.math.wrap">wrap()</a></code>, <code><a title="phi.math.stack" href="#phi.math.stack">stack()</a></code> and <code><a title="phi.math.concat" href="#phi.math.concat">concat()</a></code>.</p>
<p>To create a shape with multiple types, use <code><a title="phi.math.merge_shapes" href="#phi.math.merge_shapes">merge_shapes()</a></code>, <code><a title="phi.math.concat_shapes" href="#phi.math.concat_shapes">concat_shapes()</a></code> or the syntax <code>shape1 &amp; shape2</code>.</p>
<p>See Also:
<code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code>, <code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>
<p>Either</p>
<ul>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> to filter or</li>
<li>Names of dimensions with undefined sizes as <code>str</code>.</li>
</ul>
</dd>
<dt><strong><code>**dims</code></strong></dt>
<dd>Dimension sizes and names. Must be empty when used as a filter operation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> containing only dimensions of type instance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def instance(*args, **dims: int or str or tuple or list or Shape) -&gt; Shape:
    &#34;&#34;&#34;
    Returns the instance dimensions of an existing `Shape` or creates a new `Shape` with only instance dimensions.

    Usage for filtering instance dimensions:
    ```python
    instance_dims = instance(shape)
    instance_dims = instance(tensor)
    ```

    Usage for creating a `Shape` with only instance dimensions:
    ```python
    instance_shape = instance(&#39;undef&#39;, points=2)
    # Out: (points=2, undef=None)
    ```
    Here, the dimension `undef` is created with an undefined size of `None`.
    Undefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.

    To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 &amp; shape2`.

    See Also:
        `channel`, `batch`, `spatial`

    Args:
        *args: Either

            * `Shape` or `Tensor` to filter or
            * Names of dimensions with undefined sizes as `str`.

        **dims: Dimension sizes and names. Must be empty when used as a filter operation.

    Returns:
        `Shape` containing only dimensions of type instance.
    &#34;&#34;&#34;
    from .magic import Shaped
    if all(isinstance(arg, str) for arg in args) or dims:
        return _construct_shape(INSTANCE_DIM, *args, **dims)
    elif len(args) == 1 and isinstance(args[0], Shape):
        return args[0].instance
    elif len(args) == 1 and isinstance(args[0], Shaped):
        return shape(args[0]).instance
    else:
        raise AssertionError(f&#34;instance() must be called either as a selector instance(Shape) or instance(Tensor) or as a constructor instance(*names, **dims). Got *args={args}, **dims={dims}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.is_finite"><code class="name flex">
<span>def <span class="ident">is_finite</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> matching <code>x</code> with values <code>True</code> where <code>x</code> has a finite value and <code>False</code> otherwise.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_finite(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Returns a `Tensor` or `PhiTreeNode` matching `x` with values `True` where `x` has a finite value and `False` otherwise. &#34;&#34;&#34;
    return _backend_op1(x, Backend.isfinite)</code></pre>
</details>
</dd>
<dt id="phi.math.is_scalar"><code class="name flex">
<span>def <span class="ident">is_scalar</span></span>(<span>value) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Checks whether <code>value</code> has no dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or Python primitive or native tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>bool</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_scalar(value) -&gt; bool:
    &#34;&#34;&#34;
    Checks whether `value` has no dimensions.

    Args:
        value: `Tensor` or Python primitive or native tensor.

    Returns:
        `bool`
    &#34;&#34;&#34;
    if isinstance(value, Tensor):
        return value.shape.rank == 0
    elif isinstance(value, numbers.Number):
        return True
    else:
        shape = choose_backend(value).staticshape(value)
        return len(shape) == 0</code></pre>
</details>
</dd>
<dt id="phi.math.isfinite"><code class="name flex">
<span>def <span class="ident">isfinite</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> matching <code>x</code> with values <code>True</code> where <code>x</code> has a finite value and <code>False</code> otherwise.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_finite(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Returns a `Tensor` or `PhiTreeNode` matching `x` with values `True` where `x` has a finite value and `False` otherwise. &#34;&#34;&#34;
    return _backend_op1(x, Backend.isfinite)</code></pre>
</details>
</dd>
<dt id="phi.math.iterate"><code class="name flex">
<span>def <span class="ident">iterate</span></span>(<span>f: Callable, iterations: int, *x0, f_kwargs: dict = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Repeatedly call <code>function</code>, passing the previous output as the next input.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to call. Must be callable as <code>f(x0, **f_kwargs)</code> and <code>f(f(x0, **f_kwargs), **f_kwargs)</code>.</dd>
<dt><strong><code>iterations</code></strong></dt>
<dd>Number of iterations as <code>int</code> or single-dimension <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.
If <code>int</code>, returns the final output of <code>f</code>.
If <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>, returns the trajectory (<code>x0</code> and all outputs of <code>f</code>), stacking the values along this dimension.</dd>
<dt><strong><code>x0</code></strong></dt>
<dd>Initial positional arguments for <code>f</code>.</dd>
<dt><strong><code>f_kwargs</code></strong></dt>
<dd>Additional keyword arguments to be passed to <code>f</code>.
These arguments can be of any type.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Trajectory of final output of <code>f</code>, depending on <code>iterations</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def iterate(f: Callable, iterations: int or Shape, *x0, f_kwargs: dict = None):
    &#34;&#34;&#34;
    Repeatedly call `function`, passing the previous output as the next input.

    Args:
        f: Function to call. Must be callable as `f(x0, **f_kwargs)` and `f(f(x0, **f_kwargs), **f_kwargs)`.
        iterations: Number of iterations as `int` or single-dimension `Shape`.
            If `int`, returns the final output of `f`.
            If `Shape`, returns the trajectory (`x0` and all outputs of `f`), stacking the values along this dimension.
        x0: Initial positional arguments for `f`.
        f_kwargs: Additional keyword arguments to be passed to `f`.
            These arguments can be of any type.

    Returns:
        Trajectory of final output of `f`, depending on `iterations`.
    &#34;&#34;&#34;
    if f_kwargs is None:
        f_kwargs = {}
    x = x0
    if isinstance(iterations, int):
        for i in range(iterations):
            x = f(*x, **f_kwargs)
            if not isinstance(x, tuple):
                x = (x,)
            assert len(x) == len(x0), f&#34;Function to iterate must return {len(x0)} outputs to match input but got {x}&#34;
        return x[0] if len(x0) == 1 else x
    elif isinstance(iterations, Shape):
        xs = [x0]
        for i in range(iterations.size):
            x = f(*x, **f_kwargs)
            if not isinstance(x, tuple):
                x = (x,)
            assert len(x) == len(x0), f&#34;Function to iterate must return {len(x0)} outputs to match input but got {x}&#34;
            xs.append(x)
        xs = [stack(item, iterations.with_size(None)) for item in zip(*xs)]
        return xs[0] if len(x0) == 1 else xs</code></pre>
</details>
</dd>
<dt id="phi.math.jacobian"><code class="name flex">
<span>def <span class="ident">jacobian</span></span>(<span>f: Callable, wrt: str = None, get_output=True) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a function which computes the Jacobian matrix of <code>f</code>.
For scalar functions, consider using <code><a title="phi.math.functional_gradient" href="#phi.math.functional_gradient">functional_gradient()</a></code> instead.</p>
<p>Example:</p>
<pre><code class="language-python">def f(x, y):
    prediction = f(x)
    loss = math.l2_loss(prediction - y)
    return loss, prediction

dx = jacobian(loss_function, wrt='x', get_output=False)(x, y)

(loss, prediction), (dx, dy) = jacobian(loss_function,
                                    wrt='x,y', get_output=True)(x, y)
</code></pre>
<p>Functional gradients are implemented for the following backends:</p>
<ul>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad"><code>torch.autograd.grad</code></a> / <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward"><code>torch.autograd.backward</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/GradientTape"><code>tf.GradientTape</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/jax.html#jax.grad"><code>jax.grad</code></a></li>
</ul>
<p>When the gradient function is invoked, <code>f</code> is called with tensors that track the gradient.
For PyTorch, <code>arg.requires_grad = True</code> for all positional arguments of <code>f</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be differentiated.
<code>f</code> must return a floating point <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with rank zero.
It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if <code>return_values=True</code>.
All arguments for which the gradient is computed must be of dtype float or complex.</dd>
<dt><strong><code>get_output</code></strong></dt>
<dd>Whether the gradient function should also return the return values of <code>f</code>.</dd>
<dt><strong><code>wrt</code></strong></dt>
<dd>Comma-separated parameter names of <code>f</code> with respect to which the gradient should be computed.
If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with the same arguments as <code>f</code> that returns the value of <code>f</code>, auxiliary data and Jacobian of <code>f</code> if <code>get_output=True</code>, else just the Jacobian of <code>f</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def jacobian(f: Callable, wrt: str = None, get_output=True) -&gt; Callable:
    &#34;&#34;&#34;
    Creates a function which computes the Jacobian matrix of `f`.
    For scalar functions, consider using `functional_gradient()` instead.

    Example:
    ```python
    def f(x, y):
        prediction = f(x)
        loss = math.l2_loss(prediction - y)
        return loss, prediction

    dx = jacobian(loss_function, wrt=&#39;x&#39;, get_output=False)(x, y)

    (loss, prediction), (dx, dy) = jacobian(loss_function,
                                        wrt=&#39;x,y&#39;, get_output=True)(x, y)
    ```

    Functional gradients are implemented for the following backends:

    * PyTorch: [`torch.autograd.grad`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad) / [`torch.autograd.backward`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward)
    * TensorFlow: [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)
    * Jax: [`jax.grad`](https://jax.readthedocs.io/en/latest/jax.html#jax.grad)

    When the gradient function is invoked, `f` is called with tensors that track the gradient.
    For PyTorch, `arg.requires_grad = True` for all positional arguments of `f`.

    Args:
        f: Function to be differentiated.
            `f` must return a floating point `Tensor` with rank zero.
            It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if `return_values=True`.
            All arguments for which the gradient is computed must be of dtype float or complex.
        get_output: Whether the gradient function should also return the return values of `f`.
        wrt: Comma-separated parameter names of `f` with respect to which the gradient should be computed.
            If not specified, the gradient will be computed w.r.t. the first positional argument (highly discouraged).

    Returns:
        Function with the same arguments as `f` that returns the value of `f`, auxiliary data and Jacobian of `f` if `get_output=True`, else just the Jacobian of `f`.
    &#34;&#34;&#34;
    f_params, wrt = simplify_wrt(f, wrt)
    return GradientFunction(f, f_params, wrt, get_output, is_f_scalar=False)</code></pre>
</details>
</dd>
<dt id="phi.math.jit_compile"><code class="name flex">
<span>def <span class="ident">jit_compile</span></span>(<span>f: Callable, auxiliary_args: str = '') ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Compiles a graph based on the function <code>f</code>.
The graph compilation is performed just-in-time (jit), e.g. when the returned function is called for the first time.</p>
<p>The traced function will compute the same result as <code>f</code> but may run much faster.
Some checks may be disabled in the compiled function.</p>
<p>Can be used as a decorator:</p>
<pre><code class="language-python">@math.jit_compile
def my_function(x: math.Tensor) -&gt; math.Tensor:
</code></pre>
<p>Invoking the returned function may invoke re-tracing / re-compiling <code>f</code> after the first call if either</p>
<ul>
<li>it is called with a different number of arguments,</li>
<li>the tensor arguments have different dimension names or types (the dimension order also counts),</li>
<li>any <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> arguments require a different backend than previous invocations,</li>
<li><code>PhiTreeNode</code> positional arguments do not match in non-variable properties.</li>
</ul>
<p>Compilation is implemented for the following backends:</p>
<ul>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/jit.html"><code>torch.jit.trace</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/guide/function"><code>tf.function</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#using-jit-to-speed-up-functions"><code>jax.jit</code></a></li>
</ul>
<p>Jit-compilations cannot be nested, i.e. you cannot call <code><a title="phi.math.jit_compile" href="#phi.math.jit_compile">jit_compile()</a></code> while another function is being compiled.
An exception to this is <code><a title="phi.math.jit_compile_linear" href="#phi.math.jit_compile_linear">jit_compile_linear()</a></code> which can be called from within a jit-compiled function.</p>
<p>See Also:
<code><a title="phi.math.jit_compile_linear" href="#phi.math.jit_compile_linear">jit_compile_linear()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be traced.
All positional arguments must be of type <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> returning a single <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code>.</dd>
<dt><strong><code>auxiliary_args</code></strong></dt>
<dd>Comma-separated parameter names of arguments that are not relevant to backpropagation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with similar signature and return values as <code>f</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def jit_compile(f: Callable, auxiliary_args: str = &#39;&#39;) -&gt; Callable:
    &#34;&#34;&#34;
    Compiles a graph based on the function `f`.
    The graph compilation is performed just-in-time (jit), e.g. when the returned function is called for the first time.

    The traced function will compute the same result as `f` but may run much faster.
    Some checks may be disabled in the compiled function.

    Can be used as a decorator:
    ```python
    @math.jit_compile
    def my_function(x: math.Tensor) -&gt; math.Tensor:
    ```

    Invoking the returned function may invoke re-tracing / re-compiling `f` after the first call if either

    * it is called with a different number of arguments,
    * the tensor arguments have different dimension names or types (the dimension order also counts),
    * any `Tensor` arguments require a different backend than previous invocations,
    * `PhiTreeNode` positional arguments do not match in non-variable properties.

    Compilation is implemented for the following backends:

    * PyTorch: [`torch.jit.trace`](https://pytorch.org/docs/stable/jit.html)
    * TensorFlow: [`tf.function`](https://www.tensorflow.org/guide/function)
    * Jax: [`jax.jit`](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#using-jit-to-speed-up-functions)

    Jit-compilations cannot be nested, i.e. you cannot call `jit_compile()` while another function is being compiled.
    An exception to this is `jit_compile_linear()` which can be called from within a jit-compiled function.

    See Also:
        `jit_compile_linear()`

    Args:
        f: Function to be traced.
            All positional arguments must be of type `Tensor` or `PhiTreeNode` returning a single `Tensor` or `PhiTreeNode`.
        auxiliary_args: Comma-separated parameter names of arguments that are not relevant to backpropagation.

    Returns:
        Function with similar signature and return values as `f`.
    &#34;&#34;&#34;
    auxiliary_args = set(s.strip() for s in auxiliary_args.split(&#39;,&#39;) if s.strip())
    return f if isinstance(f, (JitFunction, LinearFunction)) and f.auxiliary_args == auxiliary_args else JitFunction(f, auxiliary_args)</code></pre>
</details>
</dd>
<dt id="phi.math.jit_compile_linear"><code class="name flex">
<span>def <span class="ident">jit_compile_linear</span></span>(<span>f: Callable[[~X], ~Y], auxiliary_args: str = None) ‑> phi.math._functional.LinearFunction[~X, ~Y]</span>
</code></dt>
<dd>
<div class="desc"><p>Compile an optimized representation of the linear function <code>f</code>.
For backends that support sparse tensors, a sparse matrix will be constructed for <code>f</code>.</p>
<p>Can be used as a decorator:</p>
<pre><code class="language-python">@math.jit_compile_linear
def my_linear_function(x: math.Tensor) -&gt; math.Tensor:
</code></pre>
<p>Unlike <code><a title="phi.math.jit_compile" href="#phi.math.jit_compile">jit_compile()</a></code>, <code><a title="phi.math.jit_compile_linear" href="#phi.math.jit_compile_linear">jit_compile_linear()</a></code> can be called during a regular jit compilation.</p>
<p>See Also:
<code><a title="phi.math.jit_compile" href="#phi.math.jit_compile">jit_compile()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function that is linear in its positional arguments.
All positional arguments must be of type <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> and <code>f</code> must return a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</dd>
<dt><strong><code>auxiliary_args</code></strong></dt>
<dd>Which parameters <code>f</code> is not linear in. These arguments are treated as conditioning arguments and will cause re-tracing on change.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.LinearFunction" href="#phi.math.LinearFunction">LinearFunction</a></code> with similar signature and return values as <code>f</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def jit_compile_linear(f: Callable[[X], Y], auxiliary_args: str = None) -&gt; &#39;LinearFunction[X, Y]&#39;:  # TODO add cache control method, e.g. max_traces
    &#34;&#34;&#34;
    Compile an optimized representation of the linear function `f`.
    For backends that support sparse tensors, a sparse matrix will be constructed for `f`.

    Can be used as a decorator:
    ```python
    @math.jit_compile_linear
    def my_linear_function(x: math.Tensor) -&gt; math.Tensor:
    ```

    Unlike `jit_compile()`, `jit_compile_linear()` can be called during a regular jit compilation.

    See Also:
        `jit_compile()`

    Args:
        f: Function that is linear in its positional arguments.
            All positional arguments must be of type `Tensor` and `f` must return a `Tensor`.
        auxiliary_args: Which parameters `f` is not linear in. These arguments are treated as conditioning arguments and will cause re-tracing on change.

    Returns:
        `LinearFunction` with similar signature and return values as `f`.
    &#34;&#34;&#34;
    if isinstance(f, JitFunction):
        f = f.f  # cannot trace linear function from jitted version
    if isinstance(auxiliary_args, str):
        auxiliary_args = set(s.strip() for s in auxiliary_args.split(&#39;,&#39;) if s.strip())
    else:
        assert auxiliary_args is None
        f_params = function_parameters(f)
        auxiliary_args = f_params[1:]
    return f if isinstance(f, LinearFunction) and f.auxiliary_args == auxiliary_args else LinearFunction(f, auxiliary_args)</code></pre>
</details>
</dd>
<dt id="phi.math.l1_loss"><code class="name flex">
<span>def <span class="ident">l1_loss</span></span>(<span>x, reduce: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>∑<sub>i</sub> ||x<sub>i</sub>||<sub>1</sub></em>, summing over all non-batch dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> or 0D or 1D native tensor.
For <code>PhiTreeNode</code> objects, only value the sum over all value attributes is computed.</dd>
<dt><strong><code>reduce</code></strong></dt>
<dd>Dimensions to reduce as <code>DimFilter</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>loss</code></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def l1_loss(x, reduce: DimFilter = math.non_batch) -&gt; Tensor:
    &#34;&#34;&#34;
    Computes *∑&lt;sub&gt;i&lt;/sub&gt; ||x&lt;sub&gt;i&lt;/sub&gt;||&lt;sub&gt;1&lt;/sub&gt;*, summing over all non-batch dimensions.

    Args:
        x: `Tensor` or `PhiTreeNode` or 0D or 1D native tensor.
            For `PhiTreeNode` objects, only value the sum over all value attributes is computed.
        reduce: Dimensions to reduce as `DimFilter`.

    Returns:
        loss: `Tensor`
    &#34;&#34;&#34;
    if isinstance(x, Tensor):
        return math.sum_(abs(x), reduce)
    elif isinstance(x, PhiTreeNode):
        return sum([l1_loss(getattr(x, a), reduce) for a in variable_values(x)])
    else:
        try:
            backend = math.choose_backend(x)
            shape = backend.staticshape(x)
            if len(shape) == 0:
                return abs(x)
            elif len(shape) == 1:
                return backend.sum(abs(x))
            else:
                raise ValueError(&#34;l2_loss is only defined for 0D and 1D native tensors. For higher-dimensional data, use Φ-Flow tensors.&#34;)
        except math.NoBackendFound:
            raise ValueError(x)</code></pre>
</details>
</dd>
<dt id="phi.math.l2_loss"><code class="name flex">
<span>def <span class="ident">l2_loss</span></span>(<span>x, reduce: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>∑<sub>i</sub> ||x<sub>i</sub>||<sub>2</sub><sup>2</sup> / 2</em>, summing over all non-batch dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> or 0D or 1D native tensor.
For <code>PhiTreeNode</code> objects, only value the sum over all value attributes is computed.</dd>
<dt><strong><code>reduce</code></strong></dt>
<dd>Dimensions to reduce as <code>DimFilter</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>loss</code></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def l2_loss(x, reduce: DimFilter = math.non_batch) -&gt; Tensor:
    &#34;&#34;&#34;
    Computes *∑&lt;sub&gt;i&lt;/sub&gt; ||x&lt;sub&gt;i&lt;/sub&gt;||&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; / 2*, summing over all non-batch dimensions.

    Args:
        x: `Tensor` or `PhiTreeNode` or 0D or 1D native tensor.
            For `PhiTreeNode` objects, only value the sum over all value attributes is computed.
        reduce: Dimensions to reduce as `DimFilter`.

    Returns:
        loss: `Tensor`
    &#34;&#34;&#34;
    if isinstance(x, Tensor):
        if x.dtype.kind == complex:
            x = abs(x)
        return math.sum_(x ** 2, reduce) * 0.5
    elif isinstance(x, PhiTreeNode):
        return sum([l2_loss(getattr(x, a), reduce) for a in variable_values(x)])
    else:
        try:
            backend = math.choose_backend(x)
            shape = backend.staticshape(x)
            if len(shape) == 0:
                return x ** 2 * 0.5
            elif len(shape) == 1:
                return backend.sum(x ** 2) * 0.5
            else:
                raise ValueError(&#34;l2_loss is only defined for 0D and 1D native tensors. For higher-dimensional data, use Φ-Flow tensors.&#34;)
        except math.NoBackendFound:
            raise ValueError(x)</code></pre>
</details>
</dd>
<dt id="phi.math.laplace"><code class="name flex">
<span>def <span class="ident">laplace</span></span>(<span>x: phi.math._tensors.Tensor, dx: phi.math._tensors.Tensor = 1, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function spatial&gt;, weights: phi.math._tensors.Tensor = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Spatial Laplace operator as defined for scalar fields.
If a vector field is passed, the laplace is computed component-wise.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>n-dimensional field of shape (batch, spacial dimensions&hellip;, components)</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>scalar or 1d tensor</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>extrapolation</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>The second derivative along these dimensions is summed over</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>(Optional) Multiply the axis terms by these factors before summation.
Must be a Tensor with a single channel dimension that lists all laplace dims by name.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of same shape as <code>x</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def laplace(x: Tensor,
            dx: Tensor or float = 1,
            padding: Extrapolation = extrapolation.BOUNDARY,
            dims: DimFilter = spatial,
            weights: Tensor = None):
    &#34;&#34;&#34;
    Spatial Laplace operator as defined for scalar fields.
    If a vector field is passed, the laplace is computed component-wise.

    Args:
        x: n-dimensional field of shape (batch, spacial dimensions..., components)
        dx: scalar or 1d tensor
        padding: extrapolation
        dims: The second derivative along these dimensions is summed over
        weights: (Optional) Multiply the axis terms by these factors before summation.
            Must be a Tensor with a single channel dimension that lists all laplace dims by name.

    Returns:
        `phi.math.Tensor` of same shape as `x`
    &#34;&#34;&#34;
    if isinstance(dx, (tuple, list)):
        dx = wrap(dx, batch(&#39;_laplace&#39;))
    elif isinstance(dx, Tensor) and dx.vector.exists:
        dx = rename_dims(dx, &#39;vector&#39;, batch(&#39;_laplace&#39;))
    if isinstance(x, Extrapolation):
        return x.spatial_gradient()
    left, center, right = shift(wrap(x), (-1, 0, 1), dims, padding, stack_dim=batch(&#39;_laplace&#39;))
    result = (left + right - 2 * center) / (dx ** 2)
    if weights is not None:
        dim_names = x.shape.only(dims).names
        assert channel(weights).rank == 1 and channel(weights).item_names is not None, f&#34;weights must have one channel dimension listing the laplace dims but got {shape(weights)}&#34;
        assert set(channel(weights).item_names[0]) &gt;= set(dim_names), f&#34;the channel dim of weights must contain all laplace dims {dim_names} but only has {channel(weights).item_names}&#34;
        result *= rename_dims(weights, channel, batch(&#39;_laplace&#39;))
    result = math.sum_(result, &#39;_laplace&#39;)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.layout"><code class="name flex">
<span>def <span class="ident">layout</span></span>(<span>objects, *shape: phi.math._shape.Shape) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Wraps a Python tree in a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, allowing elements to be accessed via dimensions.
A python tree is a structure of nested <code>tuple</code>, <code>list</code>, <code>dict</code> and <em>leaf</em> objects where leaves can be any Python object.</p>
<p>All keys of <code>dict</code> containers must be of type <code>str</code>.
The keys are automatically assigned as item names along that dimension unless conflicting with other elements.</p>
<p>Strings may also be used as containers.</p>
<p>Example:</p>
<pre><code class="language-python">t = layout({'a': 'text', 'b': [0, 1]}, channel('dict,inner'))
t.inner[1].dict['a'].native()  # returns 'e'
</code></pre>
<p>See Also:
<code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code>, <code><a title="phi.math.wrap" href="#phi.math.wrap">wrap()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>objects</code></strong></dt>
<dd>PyTree of <code>list</code> or <code>tuple</code>.</dd>
<dt><strong><code>*shape</code></strong></dt>
<dd>Tensor dimensions</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.
Calling <code><a title="phi.math.Tensor.native" href="#phi.math.Tensor.native">Tensor.native()</a></code> on the returned tensor will return <code>objects</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def layout(objects, *shape: Shape) -&gt; Tensor:
    &#34;&#34;&#34;
    Wraps a Python tree in a `Tensor`, allowing elements to be accessed via dimensions.
    A python tree is a structure of nested `tuple`, `list`, `dict` and *leaf* objects where leaves can be any Python object.

    All keys of `dict` containers must be of type `str`.
    The keys are automatically assigned as item names along that dimension unless conflicting with other elements.

    Strings may also be used as containers.

    Example:
    ```python
    t = layout({&#39;a&#39;: &#39;text&#39;, &#39;b&#39;: [0, 1]}, channel(&#39;dict,inner&#39;))
    t.inner[1].dict[&#39;a&#39;].native()  # returns &#39;e&#39;
    ```

    See Also:
        `tensor()`, `wrap()`.

    Args:
        objects: PyTree of `list` or `tuple`.
        *shape: Tensor dimensions

    Returns:
        `Tensor`.
        Calling `Tensor.native()` on the returned tensor will return `objects`.
    &#34;&#34;&#34;
    assert all(isinstance(s, Shape) for s in shape), f&#34;shape needs to be one or multiple Shape instances but got {shape}&#34;
    shape = EMPTY_SHAPE if len(shape) == 0 else concat_shapes(*shape)
    if isinstance(objects, Layout):
        assert objects.shape == shape
        return objects

    if not shape.well_defined:

        def recursive_determine_shape(native, shape: Shape):
            if not shape:
                return shape
            if isinstance(native, dict):
                assert all([isinstance(k, str) for k in native.keys()]), f&#34;All dict keys in PyTrees must be str but got {tuple(native.keys())}&#34;
                shape = shape.replace(shape[0], shape[0].with_size(tuple(native.keys())))
            if shape.rank == 1:
                return shape.with_sizes((len(native),))
            inner_shape = shape[1:]
            if isinstance(native, (tuple, list)):
                inner_shapes = [recursive_determine_shape(n, inner_shape) for n in native]
            elif isinstance(native, dict):
                inner_shapes = [recursive_determine_shape(n, inner_shape) for n in native.values()]
            else:
                raise ValueError(native)
            return shape_stack(shape[0], *inner_shapes)

        shape = recursive_determine_shape(objects, shape)

    return Layout(objects, shape)
    # if shape.volume == 1:
    #     objects = np.asarray(objects, dtype=object)
    #
    # if isinstance(objects, (tuple, list)):
    #     objects = np.asarray(objects, dtype=object)
    # if isinstance(objects, np.ndarray) and objects.dtype == object:
    #     return Layout(objects, shape)
    # else:
    #     assert shape.volume == 1, f&#34;Cannot layout object of type {objects} along {shape}, a tuple, list or object array is required.&#34;</code></pre>
</details>
</dd>
<dt id="phi.math.linspace"><code class="name flex">
<span>def <span class="ident">linspace</span></span>(<span>start: int, stop, dim: phi.math._shape.Shape) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Returns <code>number</code> evenly spaced numbers between <code>start</code> and <code>stop</code>.</p>
<p>See Also:
<code>arange()</code>, <code><a title="phi.math.meshgrid" href="#phi.math.meshgrid">meshgrid()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>start</code></strong></dt>
<dd>First value, <code>int</code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</dd>
<dt><strong><code>stop</code></strong></dt>
<dd>Last value, <code>int</code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Linspace dimension of integer size.
The size determines how many values to linearly space between <code>start</code> and <code>stop</code>.
The values will be laid out along <code>dim</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">math.linspace(0, 1, spatial(x=5))
# Out: (0.000, 0.250, 0.500, 0.750, 1.000) along xˢ

math.linspace(0, (-1, 1), spatial(x=3))
# Out: (0.000, 0.000); (-0.500, 0.500); (-1.000, 1.000) (xˢ=3, vectorᶜ=2)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def linspace(start: int or Tensor, stop, dim: Shape) -&gt; Tensor:
    &#34;&#34;&#34;
    Returns `number` evenly spaced numbers between `start` and `stop`.

    See Also:
        `arange()`, `meshgrid()`.

    Args:
        start: First value, `int` or `Tensor`.
        stop: Last value, `int` or `Tensor`.
        dim: Linspace dimension of integer size.
            The size determines how many values to linearly space between `start` and `stop`.
            The values will be laid out along `dim`.

    Returns:
        `Tensor`

    Examples:
        ```python
        math.linspace(0, 1, spatial(x=5))
        # Out: (0.000, 0.250, 0.500, 0.750, 1.000) along xˢ

        math.linspace(0, (-1, 1), spatial(x=3))
        # Out: (0.000, 0.000); (-0.500, 0.500); (-1.000, 1.000) (xˢ=3, vectorᶜ=2)
        ```
    &#34;&#34;&#34;
    assert isinstance(dim, Shape) and dim.rank == 1, f&#34;dim must be a single-dimension Shape but got {dim}&#34;
    if is_scalar(start) and is_scalar(stop):
        if isinstance(start, Tensor):
            start = start.native()
        if isinstance(stop, Tensor):
            stop = stop.native()
        native_linspace = choose_backend(start, stop, prefer_default=True).linspace(start, stop, dim.size)
        return NativeTensor(native_linspace, dim)
    else:
        return map_(linspace, start, stop, dim=dim)</code></pre>
</details>
</dd>
<dt id="phi.math.log"><code class="name flex">
<span>def <span class="ident">log</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the natural logarithm of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Computes the natural logarithm of the `Tensor` or `PhiTreeNode` `x`. &#34;&#34;&#34;
    return _backend_op1(x, Backend.log)</code></pre>
</details>
</dd>
<dt id="phi.math.log10"><code class="name flex">
<span>def <span class="ident">log10</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>log(x)</em> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code> with base 10.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log10(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Computes *log(x)* of the `Tensor` or `PhiTreeNode` `x` with base 10. &#34;&#34;&#34;
    return _backend_op1(x, Backend.log10)</code></pre>
</details>
</dd>
<dt id="phi.math.log2"><code class="name flex">
<span>def <span class="ident">log2</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>log(x)</em> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code> with base 2.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log2(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Computes *log(x)* of the `Tensor` or `PhiTreeNode` `x` with base 2. &#34;&#34;&#34;
    return _backend_op1(x, Backend.log2)</code></pre>
</details>
</dd>
<dt id="phi.math.map"><code class="name flex">
<span>def <span class="ident">map</span></span>(<span>function, *values, **kwargs) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Calls <code>function</code> on all elements of <code>value</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>function</code></strong></dt>
<dd>Function to be called on single elements contained in <code>value</code>. Must return a value that can be stored in tensors.</dd>
<dt><strong><code>values</code></strong></dt>
<dd>Tensors to iterate over. Number of tensors must match <code>function</code> signature.</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>Keyword arguments for <code>function</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of same shape as <code>value</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def map_(function, *values, **kwargs) -&gt; Tensor or None:
    &#34;&#34;&#34;
    Calls `function` on all elements of `value`.

    Args:
        function: Function to be called on single elements contained in `value`. Must return a value that can be stored in tensors.
        values: Tensors to iterate over. Number of tensors must match `function` signature.
        kwargs: Keyword arguments for `function`.

    Returns:
        `Tensor` of same shape as `value`.
    &#34;&#34;&#34;
    values = [wrap(v) for v in values]
    shape = merge_shapes(*[v.shape for v in values])
    values_reshaped = [expand(v, shape) for v in values]
    flat = [flatten(v, flatten_batch=True) for v in values_reshaped]
    result = []
    for items in zip(*flat):
        result.append(function(*items, **kwargs))
    if None in result:
        assert all(r is None for r in result), f&#34;map function returned None for some elements, {result}&#34;
        return None
    return unpack_dim(wrap(result, channel(&#39;_c&#39;)), &#39;_c&#39;, shape)</code></pre>
</details>
</dd>
<dt id="phi.math.map_i2b"><code class="name flex">
<span>def <span class="ident">map_i2b</span></span>(<span>f: Callable) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Map instance dimensions to batch dimensions. Short for <code><a title="phi.math.map_types" href="#phi.math.map_types">map_types()</a>(f, <a title="phi.math.instance" href="#phi.math.instance">instance()</a>, <a title="phi.math.batch" href="#phi.math.batch">batch()</a>)</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def map_i2b(f: Callable) -&gt; Callable:
    &#34;&#34;&#34; Map instance dimensions to batch dimensions. Short for `map_types(f, instance, batch)`. &#34;&#34;&#34;
    return map_types(f, instance, batch)</code></pre>
</details>
</dd>
<dt id="phi.math.map_s2b"><code class="name flex">
<span>def <span class="ident">map_s2b</span></span>(<span>f: Callable) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Map spatial dimensions to batch dimensions. Short for <code><a title="phi.math.map_types" href="#phi.math.map_types">map_types()</a>(f, <a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a>, <a title="phi.math.batch" href="#phi.math.batch">batch()</a>)</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def map_s2b(f: Callable) -&gt; Callable:
    &#34;&#34;&#34; Map spatial dimensions to batch dimensions. Short for `map_types(f, spatial, batch)`. &#34;&#34;&#34;
    return map_types(f, spatial, batch)</code></pre>
</details>
</dd>
<dt id="phi.math.map_types"><code class="name flex">
<span>def <span class="ident">map_types</span></span>(<span>f: Callable, dims: phi.math._shape.Shape, dim_type: Callable) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Wraps a function to change the dimension types of its <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> and <code>PhiTreeNode</code> arguments.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to wrap.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Concrete dimensions or dimension type, such as <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code> or <code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>.
These dimensions will be mapped to <code>dim_type</code> for all positional function arguments.</dd>
<dt><strong><code>dim_type</code></strong></dt>
<dd>Dimension type, such as <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code> or <code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>.
<code>f</code> will be called with dimensions remapped to this type.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with signature matching <code>f</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def map_types(f: Callable, dims: Shape or tuple or list or str or Callable, dim_type: Callable or str) -&gt; Callable:
    &#34;&#34;&#34;
    Wraps a function to change the dimension types of its `Tensor` and `PhiTreeNode` arguments.

    Args:
        f: Function to wrap.
        dims: Concrete dimensions or dimension type, such as `spatial` or `batch`.
            These dimensions will be mapped to `dim_type` for all positional function arguments.
        dim_type: Dimension type, such as `spatial` or `batch`.
            `f` will be called with dimensions remapped to this type.

    Returns:
        Function with signature matching `f`.
    &#34;&#34;&#34;

    def forward_retype(obj, input_types: Shape):
        tree, tensors = disassemble_tree(obj)
        retyped = []
        for t in tensors:
            for dim in t.shape.only(dims):
                t = t.dimension(dim).as_type(dim_type)
                input_types = math.merge_shapes(input_types, dim.with_size(None))
            retyped.append(t)
        return assemble_tree(tree, retyped), input_types

    def reverse_retype(obj, input_types: Shape):
        tree, tensors = disassemble_tree(obj)
        retyped = []
        for t in tensors:
            for dim in t.shape.only(input_types.names):
                t = t.dimension(dim).as_type(input_types.get_type(dim))
            retyped.append(t)
        return assemble_tree(tree, retyped)

    @wraps(f)
    def retyped_f(*args, **kwargs):
        input_types = EMPTY_SHAPE
        retyped_args = []
        for arg in args:
            retyped_arg, input_types = forward_retype(arg, input_types)
            retyped_args.append(retyped_arg)
        output = f(*retyped_args, **kwargs)
        restored_output = reverse_retype(output, input_types)
        return restored_output

    return retyped_f</code></pre>
</details>
</dd>
<dt id="phi.math.masked_fill"><code class="name flex">
<span>def <span class="ident">masked_fill</span></span>(<span>values: phi.math._tensors.Tensor, valid: phi.math._tensors.Tensor, distance: int = 1) ‑> Tuple[phi.math._tensors.Tensor, phi.math._tensors.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Extrapolates the values of <code>values</code> which are marked by the nonzero values of <code>valid</code> for <code>distance</code> steps in all spatial directions.
Overlapping extrapolated values get averaged. Extrapolation also includes diagonals.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensor which holds the values for extrapolation</dd>
<dt><strong><code>valid</code></strong></dt>
<dd>Tensor with same size as <code>x</code> marking the values for extrapolation with nonzero values</dd>
<dt><strong><code>distance</code></strong></dt>
<dd>Number of extrapolation steps</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>values</code></dt>
<dd>Extrapolation result</dd>
<dt><code>valid</code></dt>
<dd>mask marking all valid values after extrapolation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def masked_fill(values: Tensor, valid: Tensor, distance: int = 1) -&gt; Tuple[Tensor, Tensor]:
    &#34;&#34;&#34;
    Extrapolates the values of `values` which are marked by the nonzero values of `valid` for `distance` steps in all spatial directions.
    Overlapping extrapolated values get averaged. Extrapolation also includes diagonals.

    Args:
        values: Tensor which holds the values for extrapolation
        valid: Tensor with same size as `x` marking the values for extrapolation with nonzero values
        distance: Number of extrapolation steps

    Returns:
        values: Extrapolation result
        valid: mask marking all valid values after extrapolation
    &#34;&#34;&#34;
    def binarize(x):
        return math.divide_no_nan(x, x)
    distance = min(distance, max(values.shape.sizes))
    for _ in range(distance):
        valid = binarize(valid)
        valid_values = valid * values
        overlap = valid  # count how many values we are adding
        for dim in values.shape.spatial.names:
            values_l, values_r = shift(valid_values, (-1, 1), dims=dim, padding=extrapolation.ZERO)
            valid_values = math.sum_(values_l + values_r + valid_values, dim=&#39;shift&#39;)
            mask_l, mask_r = shift(overlap, (-1, 1), dims=dim, padding=extrapolation.ZERO)
            overlap = math.sum_(mask_l + mask_r + overlap, dim=&#39;shift&#39;)
        extp = math.divide_no_nan(valid_values, overlap)  # take mean where extrapolated values overlap
        values = math.where(valid, values, math.where(binarize(overlap), extp, values))
        valid = overlap
    return values, binarize(valid)</code></pre>
</details>
</dd>
<dt id="phi.math.max"><code class="name flex">
<span>def <span class="ident">max</span></span>(<span>value: phi.math._tensors.Tensor, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Determines the maximum value of <code>values</code> along the specified dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max_(value: Tensor or list or tuple, dim: DimFilter = non_batch) -&gt; Tensor:
    &#34;&#34;&#34;
    Determines the maximum value of `values` along the specified dimensions.

    Args:
        value: `Tensor` or `list` / `tuple` of Tensors.
        dim: Dimension or dimensions to be reduced. One of

            * `None` to reduce all non-batch dimensions
            * `str` containing single dimension or comma-separated list of dimensions
            * `Tuple[str]` or `List[str]`
            * `Shape`
            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
            * `&#39;0&#39;` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors

    Returns:
        `Tensor` without the reduced dimensions.
    &#34;&#34;&#34;
    return _reduce(value, dim, None, native_function=lambda backend, native, dim: backend.max(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.maximum"><code class="name flex">
<span>def <span class="ident">maximum</span></span>(<span>x: phi.math._tensors.Tensor, y: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the element-wise maximum of <code>x</code> and <code>y</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def maximum(x: Tensor or float, y: Tensor or float):
    &#34;&#34;&#34; Computes the element-wise maximum of `x` and `y`. &#34;&#34;&#34;
    return custom_op2(x, y, maximum, lambda x_, y_: choose_backend(x_, y_).maximum(x_, y_), op_name=&#39;maximum&#39;)</code></pre>
</details>
</dd>
<dt id="phi.math.mean"><code class="name flex">
<span>def <span class="ident">mean</span></span>(<span>value: phi.math._tensors.Tensor, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the mean over <code>values</code> along the specified dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean(value: Tensor or list or tuple, dim: DimFilter = non_batch) -&gt; Tensor:
    &#34;&#34;&#34;
    Computes the mean over `values` along the specified dimensions.

    Args:
        value: `Tensor` or `list` / `tuple` of Tensors.
        dim: Dimension or dimensions to be reduced. One of

            * `None` to reduce all non-batch dimensions
            * `str` containing single dimension or comma-separated list of dimensions
            * `Tuple[str]` or `List[str]`
            * `Shape`
            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
            * `&#39;0&#39;` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors

    Returns:
        `Tensor` without the reduced dimensions.
    &#34;&#34;&#34;
    return _reduce(value, dim, float, native_function=lambda backend, native, dim: backend.mean(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.median"><code class="name flex">
<span>def <span class="ident">median</span></span>(<span>value, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Reduces <code>dim</code> of <code>value</code> by picking the median value.
For odd dimension sizes (ambigous choice), the linear average of the two median values is computed.</p>
<p>Currently implemented via <code><a title="phi.math.quantile" href="#phi.math.quantile">quantile()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def median(value, dim: DimFilter = non_batch):
    &#34;&#34;&#34;
    Reduces `dim` of `value` by picking the median value.
    For odd dimension sizes (ambigous choice), the linear average of the two median values is computed.

    Currently implemented via `quantile()`.

    Args:
        value: `Tensor`
        dim: Dimension or dimensions to be reduced. One of

            * `None` to reduce all non-batch dimensions
            * `str` containing single dimension or comma-separated list of dimensions
            * `Tuple[str]` or `List[str]`
            * `Shape`
            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
            * `&#39;0&#39;` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors

    Returns:
        `Tensor`
    &#34;&#34;&#34;
    return quantile(value, 0.5, dim)</code></pre>
</details>
</dd>
<dt id="phi.math.merge_shapes"><code class="name flex">
<span>def <span class="ident">merge_shapes</span></span>(<span>*objs: phi.math._shape.Shape, order=(&lt;function batch&gt;, &lt;function instance&gt;, &lt;function spatial&gt;, &lt;function channel&gt;))</span>
</code></dt>
<dd>
<div class="desc"><p>Combines <code>shapes</code> into a single <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>, grouping dimensions by type.
If dimensions with equal names are present in multiple shapes, their types and sizes must match.</p>
<p>The shorthand <code>shape1 &amp; shape2</code> merges shapes with <code>check_exact=[spatial]</code>.</p>
<p>See Also:
<code><a title="phi.math.concat_shapes" href="#phi.math.concat_shapes">concat_shapes()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*objs</code></strong></dt>
<dd><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or <code>Shaped</code> objects to combine.</dd>
<dt><strong><code>order</code></strong></dt>
<dd>Dimension type order as <code>tuple</code> of type filters (<code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code>, <code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code> or <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>). Dimensions are grouped by type while merging.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Merged <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></p>
<h2 id="raises">Raises</h2>
<p>IncompatibleShapes if the shapes are not compatible</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_shapes(*objs: Shape or Any, order=(batch, instance, spatial, channel)):
    &#34;&#34;&#34;
    Combines `shapes` into a single `Shape`, grouping dimensions by type.
    If dimensions with equal names are present in multiple shapes, their types and sizes must match.

    The shorthand `shape1 &amp; shape2` merges shapes with `check_exact=[spatial]`.

    See Also:
        `concat_shapes()`.

    Args:
        *objs: `Shape` or `Shaped` objects to combine.
        order: Dimension type order as `tuple` of type filters (`channel`, `batch`, `spatial` or `instance`). Dimensions are grouped by type while merging.

    Returns:
        Merged `Shape`

    Raises:
        IncompatibleShapes if the shapes are not compatible
    &#34;&#34;&#34;
    if not objs:
        return EMPTY_SHAPE
    shapes = [obj if isinstance(obj, Shape) else shape(obj) for obj in objs]
    merged = []
    for dim_type in order:
        type_group = dim_type(shapes[0])
        for sh in shapes[1:]:
            sh = dim_type(sh)
            for dim in sh:
                if dim not in type_group:
                    type_group = type_group._expand(dim, pos=-1)
                else:  # check size match
                    if not _size_equal(dim.size, type_group.get_size(dim.name)):
                        raise IncompatibleShapes(f&#34;Cannot merge shapes {shapes} because dimension &#39;{dim.name}&#39; exists with different sizes.&#34;, *shapes)
                    names1 = type_group.get_item_names(dim)
                    names2 = sh.get_item_names(dim)
                    if names1 is not None and names2 is not None and len(names1) &gt; 1:
                        if names1 != names2:
                            if set(names1) == set(names2):
                                raise IncompatibleShapes(f&#34;Inconsistent component order: &#39;{&#39;,&#39;.join(names1)}&#39; vs &#39;{&#39;,&#39;.join(names2)}&#39; in dimension &#39;{dim.name}&#39;. Failed to merge shapes {shapes}&#34;, *shapes)
                            else:
                                raise IncompatibleShapes(f&#34;Cannot merge shapes {shapes} because dimension &#39;{dim.name}&#39; exists with different item names.&#34;, *shapes)
                    elif names1 is None and names2 is not None:
                        type_group = type_group._with_item_name(dim, tuple(names2))
        merged.append(type_group)
    return concat_shapes(*merged)</code></pre>
</details>
</dd>
<dt id="phi.math.meshgrid"><code class="name flex">
<span>def <span class="ident">meshgrid</span></span>(<span>dim_type=&lt;function spatial&gt;, stack_dim=(vectorᶜ=None), assign_item_names=True, **dimensions: int) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a mesh-grid <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> from keyword dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>**dimensions</code></strong></dt>
<dd>Mesh-grid dimensions, mapping names to values.
Values may be <code>int</code>, 1D <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or 1D native tensor.</dd>
<dt><strong><code>dim_type</code></strong></dt>
<dd>Dimension type of mesh-grid dimensions, one of <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code>, <code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>.</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>Vector dimension along which grids are stacked.</dd>
<dt><strong><code>assign_item_names</code></strong></dt>
<dd>Whether to use the dimension names from <code>**dimensions</code> as item names for <code>stack_dim</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Mesh-grid <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def meshgrid(dim_type=spatial, stack_dim=channel(&#39;vector&#39;), assign_item_names=True, **dimensions: int or Tensor) -&gt; Tensor:
    &#34;&#34;&#34;
    Generate a mesh-grid `Tensor` from keyword dimensions.

    Args:
        **dimensions: Mesh-grid dimensions, mapping names to values.
            Values may be `int`, 1D `Tensor` or 1D native tensor.
        dim_type: Dimension type of mesh-grid dimensions, one of `spatial`, `channel`, `batch`, `instance`.
        stack_dim: Vector dimension along which grids are stacked.
        assign_item_names: Whether to use the dimension names from `**dimensions` as item names for `stack_dim`.

    Returns:
        Mesh-grid `Tensor`
    &#34;&#34;&#34;
    assert &#39;vector&#39; not in dimensions
    dim_values = []
    dim_sizes = []
    for dim, spec in dimensions.items():
        if isinstance(spec, int):
            dim_values.append(tuple(range(spec)))
            dim_sizes.append(spec)
        elif isinstance(spec, Tensor):
            assert spec.rank == 1, f&#34;Only 1D sequences allowed, got {spec} for dimension &#39;{dim}&#39;.&#34;
            dim_values.append(spec.native())
            dim_sizes.append(spec.shape.volume)
        else:
            backend = choose_backend(spec)
            shape = backend.staticshape(spec)
            assert len(shape) == 1, &#34;Only 1D sequences allowed, got {spec} for dimension &#39;{dim}&#39;.&#34;
            dim_values.append(spec)
            dim_sizes.append(shape[0])
    backend = choose_backend(*dim_values, prefer_default=True)
    indices_list = backend.meshgrid(*dim_values)
    grid_shape = dim_type(**{dim: size for dim, size in zip(dimensions.keys(), dim_sizes)})
    channels = [NativeTensor(t, grid_shape) for t in indices_list]
    if assign_item_names:
        return stack_tensors(channels, stack_dim.with_size(tuple(dimensions.keys())))
    else:
        return stack_tensors(channels, stack_dim)</code></pre>
</details>
</dd>
<dt id="phi.math.min"><code class="name flex">
<span>def <span class="ident">min</span></span>(<span>value: phi.math._tensors.Tensor, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Determines the minimum value of <code>values</code> along the specified dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def min_(value: Tensor or list or tuple, dim: DimFilter = non_batch) -&gt; Tensor:
    &#34;&#34;&#34;
    Determines the minimum value of `values` along the specified dimensions.

    Args:
        value: `Tensor` or `list` / `tuple` of Tensors.
        dim: Dimension or dimensions to be reduced. One of

            * `None` to reduce all non-batch dimensions
            * `str` containing single dimension or comma-separated list of dimensions
            * `Tuple[str]` or `List[str]`
            * `Shape`
            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
            * `&#39;0&#39;` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors

    Returns:
        `Tensor` without the reduced dimensions.
    &#34;&#34;&#34;
    return _reduce(value, dim, None, native_function=lambda backend, native, dim: backend.min(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.minimize"><code class="name flex">
<span>def <span class="ident">minimize</span></span>(<span>f: Callable[[~X], ~Y], solve: phi.math._functional.Solve[~X, ~Y]) ‑> ~X</span>
</code></dt>
<dd>
<div class="desc"><p>Finds a minimum of the scalar function <em>f(x)</em>.
The <code>method</code> argument of <code>solve</code> determines which optimizer is used.
All optimizers supported by <code>scipy.optimize.minimize</code> are supported,
see <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html</a> .
Additionally a gradient descent solver with adaptive step size can be used with <code>method='GD'</code>.</p>
<p><code>math.minimize()</code> is limited to backends that support <code><a title="phi.math.jacobian" href="#phi.math.jacobian">jacobian()</a></code>, i.e. PyTorch, TensorFlow and Jax.</p>
<p>To obtain additional information about the performed solve, use a <code><a title="phi.math.SolveTape" href="#phi.math.SolveTape">SolveTape</a></code>.</p>
<p>See Also:
<code><a title="phi.math.solve_nonlinear" href="#phi.math.solve_nonlinear">solve_nonlinear()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function whose output is subject to minimization.
All positional arguments of <code>f</code> are optimized and must be <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code>.
If <code>solve.x0</code> is a <code>tuple</code> or <code>list</code>, it will be passed to <em>f</em> as varargs, <code>f(*x0)</code>.
To minimize a subset of the positional arguments, define a new (lambda) function depending only on those.
The first return value of <code>f</code> must be a scalar float <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code>.</dd>
<dt><strong><code>solve</code></strong></dt>
<dd><code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code> object to specify method type, parameters and initial guess for <code>x</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>x</code></dt>
<dd>solution, the minimum point <code>x</code>.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code><a title="phi.math.NotConverged" href="#phi.math.NotConverged">NotConverged</a></code></dt>
<dd>If the desired accuracy was not be reached within the maximum number of iterations.</dd>
<dt><code><a title="phi.math.Diverged" href="#phi.math.Diverged">Diverged</a></code></dt>
<dd>If the optimization failed prematurely.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def minimize(f: Callable[[X], Y], solve: Solve[X, Y]) -&gt; X:
    &#34;&#34;&#34;
    Finds a minimum of the scalar function *f(x)*.
    The `method` argument of `solve` determines which optimizer is used.
    All optimizers supported by `scipy.optimize.minimize` are supported,
    see https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html .
    Additionally a gradient descent solver with adaptive step size can be used with `method=&#39;GD&#39;`.

    `math.minimize()` is limited to backends that support `jacobian()`, i.e. PyTorch, TensorFlow and Jax.

    To obtain additional information about the performed solve, use a `SolveTape`.

    See Also:
        `solve_nonlinear()`.

    Args:
        f: Function whose output is subject to minimization.
            All positional arguments of `f` are optimized and must be `Tensor` or `PhiTreeNode`.
            If `solve.x0` is a `tuple` or `list`, it will be passed to *f* as varargs, `f(*x0)`.
            To minimize a subset of the positional arguments, define a new (lambda) function depending only on those.
            The first return value of `f` must be a scalar float `Tensor` or `PhiTreeNode`.
        solve: `Solve` object to specify method type, parameters and initial guess for `x`.

    Returns:
        x: solution, the minimum point `x`.

    Raises:
        NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.
        Diverged: If the optimization failed prematurely.
    &#34;&#34;&#34;
    assert (solve.relative_tolerance == 0).all, f&#34;relative_tolerance must be zero for minimize() but got {solve.relative_tolerance}&#34;
    assert solve.preprocess_y is None, &#34;minimize() does not allow preprocess_y&#34;
    x0_nest, x0_tensors = disassemble_tree(solve.x0)
    x0_tensors = [to_float(t) for t in x0_tensors]
    backend = choose_backend_t(*x0_tensors, prefer_default=True)
    batch_dims = merge_shapes(*[t.shape for t in x0_tensors]).batch
    x0_natives = []
    for t in x0_tensors:
        t._expand()
        assert t.shape.is_uniform
        x0_natives.append(reshaped_native(t, [batch_dims, t.shape.non_batch], force_expand=True))
    x0_flat = backend.concat(x0_natives, -1)

    def unflatten_assemble(x_flat, additional_dims: Shape = EMPTY_SHAPE, convert=True):
        i = 0
        x_tensors = []
        for x0_native, x0_tensor in zip(x0_natives, x0_tensors):
            vol = backend.shape(x0_native)[-1]
            flat_native = x_flat[..., i:i + vol]
            x_tensors.append(reshaped_tensor(flat_native, [*additional_dims, batch_dims, x0_tensor.shape.non_batch], convert=convert))
            i += vol
        x = assemble_tree(x0_nest, x_tensors)
        return x

    def native_function(x_flat):
        x = unflatten_assemble(x_flat)
        if isinstance(x, (tuple, list)):
            y = f(*x)
        else:
            y = f(x)
        _, y_tensors = disassemble_tree(y)
        assert not non_batch(y_tensors[0]), f&#34;Failed to minimize &#39;{f.__name__}&#39; because it returned a non-scalar output {shape(y_tensors[0])}. Reduce all non-batch dimensions, e.g. using math.l2_loss()&#34;
        try:
            loss_native = reshaped_native(y_tensors[0], [batch_dims])
        except AssertionError:
            raise AssertionError(f&#34;Failed to minimize &#39;{f.__name__}&#39; because its output loss {shape(y_tensors[0])} has more batch dimensions than the initial guess {batch_dims}.&#34;)
        return y_tensors[0].sum, (loss_native,)

    atol = backend.to_float(reshaped_native(solve.absolute_tolerance, [batch_dims], force_expand=True))
    maxi = backend.to_int32(reshaped_native(solve.max_iterations, [batch_dims], force_expand=True))
    trj = _SOLVE_TAPES and any(t.record_trajectories for t in _SOLVE_TAPES)
    t = time.perf_counter()
    ret = backend.minimize(solve.method, native_function, x0_flat, atol, maxi, trj)
    t = time.perf_counter() - t
    if not trj:
        assert isinstance(ret, SolveResult)
        converged = reshaped_tensor(ret.converged, [batch_dims])
        diverged = reshaped_tensor(ret.diverged, [batch_dims])
        x = unflatten_assemble(ret.x)
        iterations = reshaped_tensor(ret.iterations, [batch_dims])
        function_evaluations = reshaped_tensor(ret.function_evaluations, [batch_dims])
        residual = reshaped_tensor(ret.residual, [batch_dims])
        result = SolveInfo(solve, x, residual, iterations, function_evaluations, converged, diverged, ret.method, ret.message, t)
    else:  # trajectory
        assert isinstance(ret, (tuple, list)) and all(isinstance(r, SolveResult) for r in ret)
        converged = reshaped_tensor(ret[-1].converged, [batch_dims])
        diverged = reshaped_tensor(ret[-1].diverged, [batch_dims])
        x = unflatten_assemble(ret[-1].x)
        x_ = unflatten_assemble(numpy.stack([r.x for r in ret]), additional_dims=batch(&#39;trajectory&#39;), convert=False)
        residual = stack([reshaped_tensor(r.residual, [batch_dims]) for r in ret], batch(&#39;trajectory&#39;))
        iterations = reshaped_tensor(ret[-1].iterations, [batch_dims])
        function_evaluations = stack([reshaped_tensor(r.function_evaluations, [batch_dims]) for r in ret], batch(&#39;trajectory&#39;))
        result = SolveInfo(solve, x_, residual, iterations, function_evaluations, converged, diverged, ret[-1].method, ret[-1].message, t)
    for tape in _SOLVE_TAPES:
        tape._add(solve, trj, result)
    result.convergence_check(False)  # raises ConvergenceException
    return x</code></pre>
</details>
</dd>
<dt id="phi.math.minimum"><code class="name flex">
<span>def <span class="ident">minimum</span></span>(<span>x: phi.math._tensors.Tensor, y: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the element-wise minimum of <code>x</code> and <code>y</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def minimum(x: Tensor or float, y: Tensor or float):
    &#34;&#34;&#34; Computes the element-wise minimum of `x` and `y`. &#34;&#34;&#34;
    return custom_op2(x, y, minimum, lambda x_, y_: choose_backend(x_, y_).minimum(x_, y_), op_name=&#39;minimum&#39;)</code></pre>
</details>
</dd>
<dt id="phi.math.native"><code class="name flex">
<span>def <span class="ident">native</span></span>(<span>value: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the native tensor representation of <code>value</code>.
If <code>value</code> is a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, this is equal to calling <code><a title="phi.math.Tensor.native" href="#phi.math.Tensor.native">Tensor.native()</a></code>.
Otherwise, checks that <code>value</code> is a valid tensor object and returns it.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or native tensor or tensor-like.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Native tensor representation</p>
<h2 id="raises">Raises</h2>
<p>ValueError if the tensor cannot be transposed to match target_shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def native(value: Tensor or Number or tuple or list or Any):
    &#34;&#34;&#34;
    Returns the native tensor representation of `value`.
    If `value` is a `phi.math.Tensor`, this is equal to calling `phi.math.Tensor.native()`.
    Otherwise, checks that `value` is a valid tensor object and returns it.

    Args:
        value: `Tensor` or native tensor or tensor-like.

    Returns:
        Native tensor representation

    Raises:
        ValueError if the tensor cannot be transposed to match target_shape
    &#34;&#34;&#34;
    if isinstance(value, Tensor):
        return value.native()
    else:
        choose_backend(value)  # check that value is a native tensor
        return value</code></pre>
</details>
</dd>
<dt id="phi.math.native_call"><code class="name flex">
<span>def <span class="ident">native_call</span></span>(<span>f: Callable, *inputs: phi.math._tensors.Tensor, channels_last=None, channel_dim='vector', spatial_dim=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calls <code>f</code> with the native representations of the <code>inputs</code> tensors in standard layout and returns the result as a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p>
<p>All inputs are converted to native tensors (including precision cast) depending on <code>channels_last</code>:</p>
<ul>
<li><code>channels_last=True</code>: Dimension layout <code>(total_batch_size, spatial_dims&hellip;, total_channel_size)</code></li>
<li><code>channels_last=False</code>: Dimension layout <code>(total_batch_size, total_channel_size, spatial_dims&hellip;)</code></li>
</ul>
<p>All batch dimensions are compressed into a single dimension with <code>total_batch_size = input.shape.batch.volume</code>.
The same is done for all channel dimensions.</p>
<p>Additionally, missing batch and spatial dimensions are added so that all <code>inputs</code> have the same batch and spatial shape.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be called on native tensors of <code>inputs</code>.
The function output must have the same dimension layout as the inputs, unless overridden by <code>spatial_dim</code>,
and the batch size must be identical.</dd>
<dt><strong><code>*inputs</code></strong></dt>
<dd>Uniform <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> arguments</dd>
<dt><strong><code>channels_last</code></strong></dt>
<dd>(Optional) Whether to put channels as the last dimension of the native representation.
If <code>None</code>, the channels are put in the default position associated with the current backend,
see <code><a title="phi.math.backend.Backend.prefers_channels_last" href="backend/index.html#phi.math.backend.Backend.prefers_channels_last">Backend.prefers_channels_last()</a></code>.</dd>
<dt><strong><code>channel_dim</code></strong></dt>
<dd>Name of the channel dimension of the result.</dd>
<dt><strong><code>spatial_dim</code></strong></dt>
<dd>Name of the spatial dimension of the result.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with batch and spatial dimensions of <code>inputs</code>, unless overridden by <code>spatial_dim</code>,
and single channel dimension <code>channel_dim</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def native_call(f: Callable, *inputs: Tensor, channels_last=None, channel_dim=&#39;vector&#39;, spatial_dim=None):
    &#34;&#34;&#34;
    Calls `f` with the native representations of the `inputs` tensors in standard layout and returns the result as a `Tensor`.

    All inputs are converted to native tensors (including precision cast) depending on `channels_last`:

    * `channels_last=True`: Dimension layout `(total_batch_size, spatial_dims..., total_channel_size)`
    * `channels_last=False`: Dimension layout `(total_batch_size, total_channel_size, spatial_dims...)`

    All batch dimensions are compressed into a single dimension with `total_batch_size = input.shape.batch.volume`.
    The same is done for all channel dimensions.

    Additionally, missing batch and spatial dimensions are added so that all `inputs` have the same batch and spatial shape.

    Args:
        f: Function to be called on native tensors of `inputs`.
            The function output must have the same dimension layout as the inputs, unless overridden by `spatial_dim`,
            and the batch size must be identical.
        *inputs: Uniform `Tensor` arguments
        channels_last: (Optional) Whether to put channels as the last dimension of the native representation.
            If `None`, the channels are put in the default position associated with the current backend,
            see `phi.math.backend.Backend.prefers_channels_last()`.
        channel_dim: Name of the channel dimension of the result.
        spatial_dim: Name of the spatial dimension of the result.

    Returns:
        `Tensor` with batch and spatial dimensions of `inputs`, unless overridden by `spatial_dim`,
        and single channel dimension `channel_dim`.
    &#34;&#34;&#34;
    if channels_last is None:
        try:
            backend = choose_backend(f)
        except NoBackendFound:
            backend = choose_backend_t(*inputs, prefer_default=True)
        channels_last = backend.prefers_channels_last()
    batch = merge_shapes(*[i.shape.batch for i in inputs])
    spatial = merge_shapes(*[i.shape.spatial for i in inputs])
    natives = []
    for i in inputs:
        groups = (batch, *i.shape.spatial.names, i.shape.channel) if channels_last else (batch, i.shape.channel, *i.shape.spatial.names)
        natives.append(reshaped_native(i, groups))
    output = f(*natives)
    if isinstance(channel_dim, str):
        channel_dim = channel(channel_dim)
    assert isinstance(channel_dim, Shape), &#34;channel_dim must be a Shape or str&#34;
    if isinstance(output, (tuple, list)):
        raise NotImplementedError()
    else:
        if spatial_dim is None:
            groups = (batch, *spatial, channel_dim) if channels_last else (batch, channel_dim, *spatial)
        else:
            if isinstance(spatial_dim, str):
                spatial_dim = spatial(spatial_dim)
            assert isinstance(spatial_dim, Shape), &#34;spatial_dim must be a Shape or str&#34;
            groups = (batch, *spatial_dim, channel_dim) if channels_last else (batch, channel_dim, *spatial_dim)
        result = reshaped_tensor(output, groups, convert=False)
        if result.shape.get_size(channel_dim.name) == 1:
            result = result.dimension(channel_dim.name)[0]  # remove vector dim if not required
        return result</code></pre>
</details>
</dd>
<dt id="phi.math.non_batch"><code class="name flex">
<span>def <span class="ident">non_batch</span></span>(<span>obj) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the non-batch dimensions of an object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or object with a valid <code><a title="phi.math.shape" href="#phi.math.shape">shape()</a></code> property.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def non_batch(obj) -&gt; Shape:
    &#34;&#34;&#34;
    Returns the non-batch dimensions of an object.

    Args:
        obj: `Shape` or object with a valid `shape` property.

    Returns:
        `Shape`
    &#34;&#34;&#34;
    from .magic import Shaped
    if isinstance(obj, Shape):
        return obj.non_batch
    elif isinstance(obj, Shaped):
        return shape(obj).non_batch
    else:
        raise AssertionError(f&#34;non_batch() must be called either on a Shape or an object with a &#39;shape&#39; property but got {obj}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.non_channel"><code class="name flex">
<span>def <span class="ident">non_channel</span></span>(<span>obj) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the non-channel dimensions of an object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or object with a valid <code><a title="phi.math.shape" href="#phi.math.shape">shape()</a></code> property.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def non_channel(obj) -&gt; Shape:
    &#34;&#34;&#34;
    Returns the non-channel dimensions of an object.

    Args:
        obj: `Shape` or object with a valid `shape` property.

    Returns:
        `Shape`
    &#34;&#34;&#34;
    from .magic import Shaped
    if isinstance(obj, Shape):
        return obj.non_channel
    elif isinstance(obj, Shaped):
        return shape(obj).non_channel
    else:
        raise AssertionError(f&#34;non_channel() must be called either on a Shape or an object with a &#39;shape&#39; property but got {obj}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.non_instance"><code class="name flex">
<span>def <span class="ident">non_instance</span></span>(<span>obj) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the non-instance dimensions of an object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or object with a valid <code><a title="phi.math.shape" href="#phi.math.shape">shape()</a></code> property.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def non_instance(obj) -&gt; Shape:
    &#34;&#34;&#34;
    Returns the non-instance dimensions of an object.

    Args:
        obj: `Shape` or object with a valid `shape` property.

    Returns:
        `Shape`
    &#34;&#34;&#34;
    from .magic import Shaped
    if isinstance(obj, Shape):
        return obj.non_instance
    elif isinstance(obj, Shaped):
        return shape(obj).non_instance
    else:
        raise AssertionError(f&#34;non_instance() must be called either on a Shape or an object with a &#39;shape&#39; property but got {obj}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.non_spatial"><code class="name flex">
<span>def <span class="ident">non_spatial</span></span>(<span>obj) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the non-spatial dimensions of an object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or object with a valid <code><a title="phi.math.shape" href="#phi.math.shape">shape()</a></code> property.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def non_spatial(obj) -&gt; Shape:
    &#34;&#34;&#34;
    Returns the non-spatial dimensions of an object.

    Args:
        obj: `Shape` or object with a valid `shape` property.

    Returns:
        `Shape`
    &#34;&#34;&#34;
    from .magic import Shaped
    if isinstance(obj, Shape):
        return obj.non_spatial
    elif isinstance(obj, Shaped):
        return shape(obj).non_spatial
    else:
        raise AssertionError(f&#34;non_spatial() must be called either on a Shape or an object with a &#39;shape&#39; property but got {obj}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.nonzero"><code class="name flex">
<span>def <span class="ident">nonzero</span></span>(<span>value: phi.math._tensors.Tensor, list_dim: phi.math._shape.Shape = (nonzeroⁱ=None), index_dim: phi.math._shape.Shape = (vectorᶜ=None))</span>
</code></dt>
<dd>
<div class="desc"><p>Get spatial indices of non-zero / True values.</p>
<p>Batch dimensions are preserved by this operation.
If channel dimensions are present, this method returns the indices where any component is nonzero.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html"><code>numpy.argwhere</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.nonzero.html"><code>torch.nonzero</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/where"><code>tf.where(tf.not_equal(values, 0))</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.nonzero.html"><code>jax.numpy.nonzero</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>spatial tensor to find non-zero / True values in.</dd>
<dt><strong><code>list_dim</code></strong></dt>
<dd>Dimension listing non-zero values.</dd>
<dt><strong><code>index_dim</code></strong></dt>
<dd>Index dimension.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of shape (batch dims&hellip;, <code>list_dim</code>=#non-zero, <code>index_dim</code>=value.shape.spatial_rank)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nonzero(value: Tensor, list_dim: Shape or str = instance(&#39;nonzero&#39;), index_dim: Shape = channel(&#39;vector&#39;)):
    &#34;&#34;&#34;
    Get spatial indices of non-zero / True values.
    
    Batch dimensions are preserved by this operation.
    If channel dimensions are present, this method returns the indices where any component is nonzero.

    Implementations:

    * NumPy: [`numpy.argwhere`](https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html)
    * PyTorch: [`torch.nonzero`](https://pytorch.org/docs/stable/generated/torch.nonzero.html)
    * TensorFlow: [`tf.where(tf.not_equal(values, 0))`](https://www.tensorflow.org/api_docs/python/tf/where)
    * Jax: [`jax.numpy.nonzero`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.nonzero.html)

    Args:
        value: spatial tensor to find non-zero / True values in.
        list_dim: Dimension listing non-zero values.
        index_dim: Index dimension.

    Returns:
        `Tensor` of shape (batch dims..., `list_dim`=#non-zero, `index_dim`=value.shape.spatial_rank)

    &#34;&#34;&#34;
    if value.shape.channel_rank &gt; 0:
        value = sum_(abs(value), value.shape.channel)

    if isinstance(list_dim, str):
        list_dim = instance(list_dim)

    def unbatched_nonzero(value: Tensor):
        native = reshaped_native(value, [*value.shape.spatial])
        backend = choose_backend(native)
        indices = backend.nonzero(native)
        indices_shape = Shape(backend.staticshape(indices), (list_dim.name, index_dim.name), (list_dim.type, index_dim.type), (None, value.shape.spatial.names))
        return NativeTensor(indices, indices_shape)

    return broadcast_op(unbatched_nonzero, [value], iter_dims=value.shape.batch.names)</code></pre>
</details>
</dd>
<dt id="phi.math.normalize_to"><code class="name flex">
<span>def <span class="ident">normalize_to</span></span>(<span>target: phi.math._tensors.Tensor, source: float, epsilon=1e-05)</span>
</code></dt>
<dd>
<div class="desc"><p>Multiplies the target so that its sum matches the source.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>target</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>source</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or constant</dd>
<dt><strong><code>epsilon</code></strong></dt>
<dd>Small number to prevent division by zero.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Normalized tensor of the same shape as target</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize_to(target: Tensor, source: float or Tensor, epsilon=1e-5):
    &#34;&#34;&#34;
    Multiplies the target so that its sum matches the source.

    Args:
        target: `Tensor`
        source: `Tensor` or constant
        epsilon: Small number to prevent division by zero.

    Returns:
        Normalized tensor of the same shape as target
    &#34;&#34;&#34;
    target_total = math.sum_(target)
    denominator = math.maximum(target_total, epsilon) if epsilon is not None else target_total
    source_total = math.sum_(source)
    return target * (source_total / denominator)</code></pre>
</details>
</dd>
<dt id="phi.math.numpy"><code class="name flex">
<span>def <span class="ident">numpy</span></span>(<span>value: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Converts <code>value</code> to a <code>numpy.ndarray</code> where value must be a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, backend tensor or tensor-like.
If <code>value</code> is a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, this is equal to calling <code><a title="phi.math.Tensor.numpy" href="#phi.math.Tensor.numpy">Tensor.numpy()</a></code>.</p>
<p><em>Note</em>: Using this function breaks the autograd chain. The returned tensor is not differentiable.
To get a differentiable tensor, use <code><a title="phi.math.Tensor.native" href="#phi.math.Tensor.native">Tensor.native()</a></code> instead.</p>
<p>Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
If a dimension of the tensor is not listed in <code>order</code>, a <code>ValueError</code> is raised.</p>
<p>If <code>value</code> is a NumPy array, it may be returned directly.</p>
<h2 id="returns">Returns</h2>
<p>NumPy representation of <code>value</code></p>
<h2 id="raises">Raises</h2>
<p>ValueError if the tensor cannot be transposed to match target_shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def numpy(value: Tensor or Number or tuple or list or Any):
    &#34;&#34;&#34;
    Converts `value` to a `numpy.ndarray` where value must be a `Tensor`, backend tensor or tensor-like.
    If `value` is a `phi.math.Tensor`, this is equal to calling `phi.math.Tensor.numpy()`.

    *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
    To get a differentiable tensor, use `Tensor.native()` instead.

    Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
    If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

    If `value` is a NumPy array, it may be returned directly.

    Returns:
        NumPy representation of `value`

    Raises:
        ValueError if the tensor cannot be transposed to match target_shape
    &#34;&#34;&#34;
    if isinstance(value, Tensor):
        return value.numpy()
    else:
        backend = choose_backend(value)
        return backend.numpy(value)</code></pre>
</details>
</dd>
<dt id="phi.math.ones"><code class="name flex">
<span>def <span class="ident">ones</span></span>(<span>*shape: phi.math._shape.Shape, dtype=None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Define a tensor with specified shape with value <code>1.0</code>/ <code>1</code> / <code>True</code> everywhere.</p>
<p>This method may not immediately allocate the memory to store the values.</p>
<p>See Also:
<code><a title="phi.math.ones_like" href="#phi.math.ones_like">ones_like()</a></code>, <code><a title="phi.math.zeros" href="#phi.math.zeros">zeros()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*shape</code></strong></dt>
<dd>This (possibly empty) sequence of <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>s is concatenated, preserving the order.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Data type as <code><a title="phi.math.DType" href="#phi.math.DType">DType</a></code> object. Defaults to <code>float</code> matching the current precision setting.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ones(*shape: Shape, dtype=None) -&gt; Tensor:
    &#34;&#34;&#34;
    Define a tensor with specified shape with value `1.0`/ `1` / `True` everywhere.
    
    This method may not immediately allocate the memory to store the values.

    See Also:
        `ones_like()`, `zeros()`.

    Args:
        *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.
        dtype: Data type as `DType` object. Defaults to `float` matching the current precision setting.

    Returns:
        `Tensor`
    &#34;&#34;&#34;
    return _initialize(lambda shape: CollapsedTensor(NativeTensor(default_backend().ones((), dtype=DType.as_dtype(dtype)), EMPTY_SHAPE), shape), shape)</code></pre>
</details>
</dd>
<dt id="phi.math.ones_like"><code class="name flex">
<span>def <span class="ident">ones_like</span></span>(<span>value: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Create a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> containing only <code>1.0</code> / <code>1</code> / <code>True</code> with the same shape and dtype as <code>obj</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ones_like(value: Tensor) -&gt; Tensor:
    &#34;&#34;&#34; Create a `Tensor` containing only `1.0` / `1` / `True` with the same shape and dtype as `obj`. &#34;&#34;&#34;
    return zeros_like(value) + 1</code></pre>
</details>
</dd>
<dt id="phi.math.pack_dims"><code class="name flex">
<span>def <span class="ident">pack_dims</span></span>(<span>value, dims: Union[str, tuple, list, phi.math._shape.Shape, Callable], packed_dim: phi.math._shape.Shape, pos: int = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Compresses multiple dimensions into a single dimension by concatenating the elements.
Elements along the new dimensions are laid out according to the order of <code>dims</code>.
If the order of <code>dims</code> differs from the current dimension order, the tensor is transposed accordingly.
This function replaces the traditional <code>reshape</code> for these cases.</p>
<p>The type of the new dimension will be equal to the types of <code>dims</code>.
If <code>dims</code> have varying types, the new dimension will be a batch dimension.</p>
<p>If none of <code>dims</code> exist on <code>value</code>, <code>packed_dim</code> will be added only if it is given with a definite size.</p>
<p>See Also:
<code><a title="phi.math.unpack_dim" href="#phi.math.unpack_dim">unpack_dim()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.magic.Shapable" href="magic.html#phi.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions to be compressed in the specified order.</dd>
<dt><strong><code>packed_dim</code></strong></dt>
<dd>Single-dimension <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
<dt><strong><code>pos</code></strong></dt>
<dd>Index of new dimension. <code>None</code> for automatic, <code>-1</code> for last, <code>0</code> for first.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as <code>value</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">pack_dims(math.zeros(spatial(x=4, y=3)), spatial, instance('points'))
# Out: (pointsⁱ=12) const 0.0
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pack_dims(value, dims: DimFilter, packed_dim: Shape, pos: int or None = None, **kwargs):
    &#34;&#34;&#34;
    Compresses multiple dimensions into a single dimension by concatenating the elements.
    Elements along the new dimensions are laid out according to the order of `dims`.
    If the order of `dims` differs from the current dimension order, the tensor is transposed accordingly.
    This function replaces the traditional `reshape` for these cases.

    The type of the new dimension will be equal to the types of `dims`.
    If `dims` have varying types, the new dimension will be a batch dimension.

    If none of `dims` exist on `value`, `packed_dim` will be added only if it is given with a definite size.

    See Also:
        `unpack_dim()`

    Args:
        value: `phi.math.magic.Shapable`, such as `phi.math.Tensor`.
        dims: Dimensions to be compressed in the specified order.
        packed_dim: Single-dimension `Shape`.
        pos: Index of new dimension. `None` for automatic, `-1` for last, `0` for first.
        **kwargs: Additional keyword arguments required by specific implementations.
            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
            Adding batch dimensions must always work without keyword arguments.

    Returns:
        Same type as `value`.

    Examples:
        ```python
        pack_dims(math.zeros(spatial(x=4, y=3)), spatial, instance(&#39;points&#39;))
        # Out: (pointsⁱ=12) const 0.0
        ```
    &#34;&#34;&#34;
    assert isinstance(value, Shapable) and isinstance(value, Sliceable) and isinstance(value, Shaped), f&#34;value must be Shapable but got {type(value)}&#34;
    dims = shape(value).only(dims)
    if packed_dim in shape(value):
        assert packed_dim in dims, f&#34;Cannot pack dims into new dimension {packed_dim} because it already exists on value {value} and is not packed.&#34;
    if len(dims) == 0 or all(dim not in shape(value) for dim in dims):
        return value if packed_dim.size is None else expand(value, packed_dim, **kwargs)  # Inserting size=1 can cause shape errors
    elif len(dims) == 1:
        return rename_dims(value, dims, packed_dim, **kwargs)
    # --- First try __pack_dims__ ---
    if hasattr(value, &#39;__pack_dims__&#39;):
        result = value.__pack_dims__(dims.names, packed_dim, pos, **kwargs)
        if result is not NotImplemented:
            return result
    # --- Next try Tree Node ---
    if isinstance(value, PhiTreeNode):
        new_attributes = {a: pack_dims(getattr(value, a), dims, packed_dim, pos=pos, **kwargs) for a in value_attributes(value)}
        return copy_with(value, **new_attributes)
    # --- Fallback: unstack and stack ---
    if shape(value).only(dims).volume &gt; 8:
        warnings.warn(f&#34;pack_dims() default implementation is slow on large dimensions ({shape(value).only(dims)}). Please implement __pack_dims__() for {type(value).__name__} as defined in phi.math.magic&#34;, RuntimeWarning, stacklevel=2)
    return stack(unstack(value, dims), packed_dim, **kwargs)</code></pre>
</details>
</dd>
<dt id="phi.math.pad"><code class="name flex">
<span>def <span class="ident">pad</span></span>(<span>value: phi.math._tensors.Tensor, widths: dict, mode: e_.Extrapolation, **kwargs) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Pads a tensor along the specified dimensions, determining the added values using the given extrapolation.
Unlike <code>Extrapolation.pad()</code>, this function can handle negative widths which slice off outer values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> to be padded</dd>
<dt><strong><code>widths</code></strong></dt>
<dd><code>dict</code> mapping dimension name (<code>str</code>) to <code>(lower, upper)</code>
where <code>lower</code> and <code>upper</code> are <code>int</code> that can be positive (pad), negative (slice) or zero (pass).</dd>
<dt><strong><code>mode</code></strong></dt>
<dd><code>Extrapolation</code> used to determine values added from positive <code>widths</code>.
Assumes constant extrapolation if given a number or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> instead.</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>Additional padding arguments.
These are ignored by the standard extrapolations defined in <code><a title="phi.math.extrapolation" href="extrapolation.html">phi.math.extrapolation</a></code> but can be used to pass additional contextual information to custom extrapolations.
Grid classes from <code><a title="phi.field" href="../field/index.html">phi.field</a></code> will pass the argument <code>bounds: Box</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Padded <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">math.pad(math.ones(spatial(x=10, y=10)), {'x': (1, 1), 'y': (2, 1)}, 0)
# Out: (xˢ=12, yˢ=13) 0.641 ± 0.480 (0e+00...1e+00)

math.pad(math.ones(spatial(x=10, y=10)), {'x': (1, -1)}, 0)
# Out: (xˢ=10, yˢ=10) 0.900 ± 0.300 (0e+00...1e+00)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad(value: Tensor, widths: dict, mode: &#39;e_.Extrapolation&#39; or Tensor or Number, **kwargs) -&gt; Tensor:
    &#34;&#34;&#34;
    Pads a tensor along the specified dimensions, determining the added values using the given extrapolation.
    Unlike `Extrapolation.pad()`, this function can handle negative widths which slice off outer values.

    Args:
        value: `Tensor` to be padded
        widths: `dict` mapping dimension name (`str`) to `(lower, upper)`
            where `lower` and `upper` are `int` that can be positive (pad), negative (slice) or zero (pass).
        mode: `Extrapolation` used to determine values added from positive `widths`.
            Assumes constant extrapolation if given a number or `Tensor` instead.
        kwargs: Additional padding arguments.
            These are ignored by the standard extrapolations defined in `phi.math.extrapolation` but can be used to pass additional contextual information to custom extrapolations.
            Grid classes from `phi.field` will pass the argument `bounds: Box`.

    Returns:
        Padded `Tensor`

    Examples:
        ```python
        math.pad(math.ones(spatial(x=10, y=10)), {&#39;x&#39;: (1, 1), &#39;y&#39;: (2, 1)}, 0)
        # Out: (xˢ=12, yˢ=13) 0.641 ± 0.480 (0e+00...1e+00)

        math.pad(math.ones(spatial(x=10, y=10)), {&#39;x&#39;: (1, -1)}, 0)
        # Out: (xˢ=10, yˢ=10) 0.900 ± 0.300 (0e+00...1e+00)
        ```
    &#34;&#34;&#34;
    mode = mode if isinstance(mode, e_.Extrapolation) else e_.ConstantExtrapolation(mode)
    has_negative_widths = any(w0 &lt; 0 or w1 &lt; 0 for w0, w1 in widths.values())
    has_positive_widths = any(w0 &gt; 0 or w1 &gt; 0 for w0, w1 in widths.values())
    slices = None
    if has_negative_widths:
        slices = {dim: slice(max(0, -w[0]), min(0, w[1]) or None) for dim, w in widths.items()}
        widths = {dim: (max(0, w[0]), max(0, w[1])) for dim, w in widths.items()}
    result_padded = mode.pad(value, widths, **kwargs) if has_positive_widths else value
    result_sliced = result_padded[slices] if has_negative_widths else result_padded
    return result_sliced</code></pre>
</details>
</dd>
<dt id="phi.math.precision"><code class="name flex">
<span>def <span class="ident">precision</span></span>(<span>floating_point_bits: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the floating point precision for the local context.</p>
<p>Usage: <code>with precision(p):</code></p>
<p>This overrides the global setting, see <code><a title="phi.math.set_global_precision" href="#phi.math.set_global_precision">set_global_precision()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>floating_point_bits</code></strong></dt>
<dd>16 for half, 32 for single, 64 for double</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def precision(floating_point_bits: int):
    &#34;&#34;&#34;
    Sets the floating point precision for the local context.

    Usage: `with precision(p):`

    This overrides the global setting, see `set_global_precision()`.

    Args:
        floating_point_bits: 16 for half, 32 for single, 64 for double
    &#34;&#34;&#34;
    _PRECISION.append(floating_point_bits)
    try:
        yield None
    finally:
        _PRECISION.pop(-1)</code></pre>
</details>
</dd>
<dt id="phi.math.print"><code class="name flex">
<span>def <span class="ident">print</span></span>(<span>obj: phi.math._tensors.Tensor = None, name: str = '')</span>
</code></dt>
<dd>
<div class="desc"><p>Print a tensor with no more than two spatial dimensions, slicing it along all batch and channel dimensions.</p>
<p>Unlike NumPy's array printing, the dimensions are sorted.
Elements along the alphabetically first dimension is printed to the right, the second dimension upward.
Typically, this means x right, y up.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd>tensor-like</dd>
<dt><strong><code>name</code></strong></dt>
<dd>name of the tensor</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_(obj: Tensor or PhiTreeNode or Number or tuple or list or None = None, name: str = &#34;&#34;):
    &#34;&#34;&#34;
    Print a tensor with no more than two spatial dimensions, slicing it along all batch and channel dimensions.
    
    Unlike NumPy&#39;s array printing, the dimensions are sorted.
    Elements along the alphabetically first dimension is printed to the right, the second dimension upward.
    Typically, this means x right, y up.

    Args:
        obj: tensor-like
        name: name of the tensor

    Returns:

    &#34;&#34;&#34;
    def variables(obj) -&gt; dict:
        if hasattr(obj, &#39;__variable_attrs__&#39;) or hasattr(obj, &#39;__value_attrs__&#39;):
            return {f&#34;.{a}&#34;: getattr(obj, a) for a in variable_attributes(obj)}
        elif isinstance(obj, (tuple, list)):
            return {f&#34;[{i}]&#34;: item for i, item in enumerate(obj)}
        elif isinstance(obj, dict):
            return obj
        else:
            raise ValueError(f&#34;Not PhiTreeNode: {type(obj)}&#34;)

    if name:
        print(&#34; &#34; * 12 + name)
    if obj is None:
        print(&#34;None&#34;)
    elif isinstance(obj, Tensor):
        print(f&#34;{obj:full}&#34;)
    elif isinstance(obj, PhiTreeNode):
        for n, val in variables(obj).items():
            print_(val, name + n)
    else:
        print(f&#34;{wrap(obj):full}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.print_gradient"><code class="name flex">
<span>def <span class="ident">print_gradient</span></span>(<span>value: phi.math._tensors.Tensor, name='', detailed=False) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Prints the gradient vector of <code>value</code> when computed.
The gradient at <code>value</code> is the vector-Jacobian product of all operations between the output of this function and the loss value.</p>
<p>The gradient is not printed in jit mode, see <code><a title="phi.math.jit_compile" href="#phi.math.jit_compile">jit_compile()</a></code>.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python">def f(x):
    x = math.print_gradient(x, 'dx')
    return math.l1_loss(x)

math.jacobian(f)(math.ones(x=6))
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> for which the gradient may be computed later.</dd>
<dt><strong><code>name</code></strong></dt>
<dd>(Optional) Name to print along with the gradient values</dd>
<dt><strong><code>detailed</code></strong></dt>
<dd>If <code>False</code>, prints a short summary of the gradient tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>identity(value)</code> which when differentiated, prints the gradient vector.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_gradient(value: Tensor, name=&#34;&#34;, detailed=False) -&gt; Tensor:
    &#34;&#34;&#34;
    Prints the gradient vector of `value` when computed.
    The gradient at `value` is the vector-Jacobian product of all operations between the output of this function and the loss value.

    The gradient is not printed in jit mode, see `jit_compile()`.

    Example:
        ```python
        def f(x):
            x = math.print_gradient(x, &#39;dx&#39;)
            return math.l1_loss(x)

        math.jacobian(f)(math.ones(x=6))
        ```

    Args:
        value: `Tensor` for which the gradient may be computed later.
        name: (Optional) Name to print along with the gradient values
        detailed: If `False`, prints a short summary of the gradient tensor.

    Returns:
        `identity(value)` which when differentiated, prints the gradient vector.
    &#34;&#34;&#34;

    def print_grad(params: dict, _y, dx):
        param_name, x = next(iter(params.items()))
        if all_available(x, dx):
            if detailed:
                print_(dx, name=name)
            else:
                print(f&#34;{name}:  \t{dx}&#34;)
        else:
            print(f&#34;Cannot print gradient for {param_name}, data not available.&#34;)
        return {param_name: dx}

    identity = custom_gradient(lambda x: x, print_grad)
    return identity(value)</code></pre>
</details>
</dd>
<dt id="phi.math.prod"><code class="name flex">
<span>def <span class="ident">prod</span></span>(<span>value: phi.math._tensors.Tensor, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Multiplies <code>values</code> along the specified dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prod(value: Tensor or list or tuple, dim: DimFilter = non_batch) -&gt; Tensor:
    &#34;&#34;&#34;
    Multiplies `values` along the specified dimensions.

    Args:
        value: `Tensor` or `list` / `tuple` of Tensors.
        dim: Dimension or dimensions to be reduced. One of

            * `None` to reduce all non-batch dimensions
            * `str` containing single dimension or comma-separated list of dimensions
            * `Tuple[str]` or `List[str]`
            * `Shape`
            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
            * `&#39;0&#39;` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors

    Returns:
        `Tensor` without the reduced dimensions.
    &#34;&#34;&#34;
    return _reduce(value, dim, None,
                   native_function=lambda backend, native, dim: backend.prod(native, dim),
                   collapsed_function=lambda inner, red_shape: inner ** red_shape.volume)</code></pre>
</details>
</dd>
<dt id="phi.math.quantile"><code class="name flex">
<span>def <span class="ident">quantile</span></span>(<span>value: phi.math._tensors.Tensor, quantiles: float, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the q-th quantile of <code>value</code> along <code>dim</code> for each q in <code>quantiles</code>.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/generated/numpy.quantile.html"><code>quantile</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"><code>quantile</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/probability/api_docs/python/tfp/stats/percentile"><code>tfp.stats.percentile</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.quantile.html"><code>quantile</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>quantiles</code></strong></dt>
<dd>Single quantile or tensor of quantiles to compute.
Must be of type <code>float</code>, <code>tuple</code>, <code>list</code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to reduce the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with dimensions of <code>quantiles</code> and non-reduced dimensions of <code>value</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def quantile(value: Tensor,
             quantiles: float or tuple or list or Tensor,
             dim: DimFilter = non_batch):
    &#34;&#34;&#34;
    Compute the q-th quantile of `value` along `dim` for each q in `quantiles`.

    Implementations:

    * NumPy: [`quantile`](https://numpy.org/doc/stable/reference/generated/numpy.quantile.html)
    * PyTorch: [`quantile`](https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile)
    * TensorFlow: [`tfp.stats.percentile`](https://www.tensorflow.org/probability/api_docs/python/tfp/stats/percentile)
    * Jax: [`quantile`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.quantile.html)

    Args:
        value: `Tensor`
        quantiles: Single quantile or tensor of quantiles to compute.
            Must be of type `float`, `tuple`, `list` or `Tensor`.
        dim: Dimension or dimensions to be reduced. One of

            * `None` to reduce all non-batch dimensions
            * `str` containing single dimension or comma-separated list of dimensions
            * `Tuple[str]` or `List[str]`
            * `Shape`
            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
            * `&#39;0&#39;` when `isinstance(value, (tuple, list))` to reduce the sequence of Tensors

    Returns:
        `Tensor` with dimensions of `quantiles` and non-reduced dimensions of `value`.
    &#34;&#34;&#34;
    dims = value.shape.only(dim)
    native_values = reshaped_native(value, [*value.shape.without(dims), value.shape.only(dims)])
    backend = choose_backend(native_values)
    q = tensor(quantiles, default_list_dim=instance(&#39;quantiles&#39;))
    native_quantiles = reshaped_native(q, [q.shape])
    native_result = backend.quantile(native_values, native_quantiles)
    return reshaped_tensor(native_result, [q.shape, *value.shape.without(dims)])</code></pre>
</details>
</dd>
<dt id="phi.math.random_normal"><code class="name flex">
<span>def <span class="ident">random_normal</span></span>(<span>*shape: phi.math._shape.Shape, dtype=None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with the specified shape, filled with random values sampled from a normal / Gaussian distribution.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.standard_normal.html"><code>numpy.random.standard_normal</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.randn.html"><code>torch.randn</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/random/normal"><code>tf.random.normal</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.random.normal.html"><code>jax.random.normal</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*shape</code></strong></dt>
<dd>This (possibly empty) sequence of <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>s is concatenated, preserving the order.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>(optional) floating point <code><a title="phi.math.DType" href="#phi.math.DType">DType</a></code>. If <code>None</code>, a float tensor with the current default precision is created, see <code><a title="phi.math.get_precision" href="#phi.math.get_precision">get_precision()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_normal(*shape: Shape, dtype=None) -&gt; Tensor:
    &#34;&#34;&#34;
    Creates a `Tensor` with the specified shape, filled with random values sampled from a normal / Gaussian distribution.

    Implementations:

    * NumPy: [`numpy.random.standard_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.standard_normal.html)
    * PyTorch: [`torch.randn`](https://pytorch.org/docs/stable/generated/torch.randn.html)
    * TensorFlow: [`tf.random.normal`](https://www.tensorflow.org/api_docs/python/tf/random/normal)
    * Jax: [`jax.random.normal`](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.normal.html)

    Args:
        *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.
        dtype: (optional) floating point `DType`. If `None`, a float tensor with the current default precision is created, see `get_precision()`.

    Returns:
        `Tensor`
    &#34;&#34;&#34;

    def uniform_random_normal(shape):
        native = choose_backend(*shape.sizes, prefer_default=True).random_normal(shape.sizes, DType.as_dtype(dtype))
        return NativeTensor(native, shape)

    return _initialize(uniform_random_normal, shape)</code></pre>
</details>
</dd>
<dt id="phi.math.random_uniform"><code class="name flex">
<span>def <span class="ident">random_uniform</span></span>(<span>*shape: phi.math._shape.Shape, low: phi.math._tensors.Tensor = 0, high: phi.math._tensors.Tensor = 1, dtype: phi.math.backend._dtype.DType = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with the specified shape, filled with random values sampled from a uniform distribution.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*shape</code></strong></dt>
<dd>This (possibly empty) sequence of <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>s is concatenated, preserving the order.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>(optional) <code><a title="phi.math.DType" href="#phi.math.DType">DType</a></code> or <code>(kind, bits)</code>.
The dtype kind must be one of <code>float</code>, <code>int</code>, <code>complex</code>.
If not specified, a <code>float</code> tensor with the current default precision is created, see <code><a title="phi.math.get_precision" href="#phi.math.get_precision">get_precision()</a></code>.</dd>
<dt><strong><code>low</code></strong></dt>
<dd>Minimum value, included.</dd>
<dt><strong><code>high</code></strong></dt>
<dd>Maximum value, excluded.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_uniform(*shape: Shape,
                   low: Tensor or float = 0,
                   high: Tensor or float = 1,
                   dtype: DType or tuple = None) -&gt; Tensor:
    &#34;&#34;&#34;
    Creates a `Tensor` with the specified shape, filled with random values sampled from a uniform distribution.

    Args:
        *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.
        dtype: (optional) `DType` or `(kind, bits)`.
            The dtype kind must be one of `float`, `int`, `complex`.
            If not specified, a `float` tensor with the current default precision is created, see `get_precision()`.
        low: Minimum value, included.
        high: Maximum value, excluded.
    Returns:
        `Tensor`
    &#34;&#34;&#34;
    def uniform_random_uniform(shape):
        native = choose_backend(low, high, *shape.sizes, prefer_default=True).random_uniform(shape.sizes, low, high, DType.as_dtype(dtype))
        return NativeTensor(native, shape)

    return _initialize(uniform_random_uniform, shape)</code></pre>
</details>
</dd>
<dt id="phi.math.range"><code class="name flex">
<span>def <span class="ident">range</span></span>(<span>dim: phi.math._shape.Shape, start_or_stop: int = None, stop: int = None, step=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns evenly spaced values between <code>start</code> and <code>stop</code>.
If only one limit is given, <code>0</code> is used for the start.</p>
<p>See Also:
<code><a title="phi.math.range_tensor" href="#phi.math.range_tensor">range_tensor()</a></code>, <code><a title="phi.math.linspace" href="#phi.math.linspace">linspace()</a></code>, <code><a title="phi.math.meshgrid" href="#phi.math.meshgrid">meshgrid()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension name and type as <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.
The <code>size</code> of <code>dim</code> is interpreted as <code>stop</code> unless <code>start_or_stop</code> is specified.</dd>
<dt><strong><code>start_or_stop</code></strong></dt>
<dd>(Optional) <code>int</code>. Interpreted as <code>start</code> if <code>stop</code> is specified as well. Otherwise this is <code>stop</code>.</dd>
<dt><strong><code>stop</code></strong></dt>
<dd>(Optional) <code>int</code>. <code>stop</code> value.</dd>
<dt><strong><code>step</code></strong></dt>
<dd>Distance between values.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def arange(dim: Shape, start_or_stop: int or None = None, stop: int or None = None, step=1):
    &#34;&#34;&#34;
    Returns evenly spaced values between `start` and `stop`.
    If only one limit is given, `0` is used for the start.

    See Also:
        `range_tensor()`, `linspace()`, `meshgrid()`.

    Args:
        dim: Dimension name and type as `Shape` object.
            The `size` of `dim` is interpreted as `stop` unless `start_or_stop` is specified.
        start_or_stop: (Optional) `int`. Interpreted as `start` if `stop` is specified as well. Otherwise this is `stop`.
        stop: (Optional) `int`. `stop` value.
        step: Distance between values.

    Returns:
        `Tensor`
    &#34;&#34;&#34;
    if start_or_stop is None:
        assert stop is None, &#34;start_or_stop must be specified when stop is given.&#34;
        assert isinstance(dim.size, int), &#34;When start_or_stop is not specified, dim.size must be an integer.&#34;
        start, stop = 0, dim.size
    elif stop is None:
        start, stop = 0, start_or_stop
    else:
        start = start_or_stop
    native = choose_backend(start, stop, prefer_default=True).range(start, stop, step, DType(int, 32))
    return NativeTensor(native, dim.with_sizes([stop - start]))</code></pre>
</details>
</dd>
<dt id="phi.math.range_tensor"><code class="name flex">
<span>def <span class="ident">range_tensor</span></span>(<span>shape: phi.math._shape.Shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with given <code><a title="phi.math.shape" href="#phi.math.shape">shape()</a></code> containing the linear indices of each element.
For 1D tensors, this equivalent to <code>arange()</code> with <code>step=1</code>.</p>
<p>See Also:
<code>arange()</code>, <code><a title="phi.math.meshgrid" href="#phi.math.meshgrid">meshgrid()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong></dt>
<dd>Tensor shape.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def range_tensor(shape: Shape):
    &#34;&#34;&#34;
    Returns a `Tensor` with given `shape` containing the linear indices of each element.
    For 1D tensors, this equivalent to `arange()` with `step=1`.

    See Also:
        `arange()`, `meshgrid()`.

    Args:
        shape: Tensor shape.

    Returns:
        `Tensor`
    &#34;&#34;&#34;
    data = arange(spatial(&#39;range&#39;), 0, shape.volume)
    return unpack_dim(data, &#39;range&#39;, shape)</code></pre>
</details>
</dd>
<dt id="phi.math.real"><code class="name flex">
<span>def <span class="ident">real</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>See Also:
<code><a title="phi.math.imag" href="#phi.math.imag">imag()</a></code>, <code><a title="phi.math.conjugate" href="#phi.math.conjugate">conjugate()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> or native tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Real component of <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def real(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34;
    See Also:
        `imag()`, `conjugate()`.

    Args:
        x: `Tensor` or `PhiTreeNode` or native tensor.

    Returns:
        Real component of `x`.
    &#34;&#34;&#34;
    return _backend_op1(x, Backend.real)</code></pre>
</details>
</dd>
<dt id="phi.math.rename_dims"><code class="name flex">
<span>def <span class="ident">rename_dims</span></span>(<span>value, dims: str, names: str, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Change the name and optionally the type of some dimensions of <code>value</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>Shapable</code>.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Existing dimensions of <code>value</code>.</dd>
<dt><strong><code>names</code></strong></dt>
<dd>
<p>Either</p>
<ul>
<li>Sequence of names matching <code>dims</code> as <code>tuple</code>, <code>list</code> or <code>str</code>. This replaces only the dimension names but leaves the types untouched.</li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> matching <code>dims</code> to replace names and types.</li>
</ul>
</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as <code>value</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rename_dims(value,
                dims: str or tuple or list or Shape,
                names: str or tuple or list or Shape,
                **kwargs):
    &#34;&#34;&#34;
    Change the name and optionally the type of some dimensions of `value`.

    Args:
        value: `Shape` or `Tensor` or `Shapable`.
        dims: Existing dimensions of `value`.
        names: Either

            * Sequence of names matching `dims` as `tuple`, `list` or `str`. This replaces only the dimension names but leaves the types untouched.
            * `Shape` matching `dims` to replace names and types.

        **kwargs: Additional keyword arguments required by specific implementations.
            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
            Adding batch dimensions must always work without keyword arguments.

    Returns:
        Same type as `value`.
    &#34;&#34;&#34;
    if isinstance(value, Shape):
        return value._replace_names_and_types(dims, names)
    assert isinstance(value, Shapable) and isinstance(value, Shaped), f&#34;value must be a Shape or Shapable but got {type(value).__name__}&#34;
    dims = shape(value).only(dims)
    names = dims._replace_names_and_types(dims, names)
    # --- First try __replace_dims__ ---
    if hasattr(value, &#39;__replace_dims__&#39;):
        result = value.__replace_dims__(dims.names, names, **kwargs)
        if result is not NotImplemented:
            return result
    # --- Next try Tree Node ---
    if isinstance(value, PhiTreeNode):
        new_attributes = {a: rename_dims(getattr(value, a), dims, names, **kwargs) for a in value_attributes(value)}
        return copy_with(value, **new_attributes)
    # --- Fallback: unstack and stack ---
    if shape(value).only(dims).volume &gt; 8:
        warnings.warn(f&#34;rename_dims() default implementation is slow on large dimensions ({shape(value).only(dims)}). Please implement __replace_dims__() for {type(value).__name__} as defined in phi.math.magic&#34;, RuntimeWarning, stacklevel=2)
    for old_name, new_dim in zip(dims.names, names):
        value = stack(unstack(value, old_name), new_dim, **kwargs)
    return value</code></pre>
</details>
</dd>
<dt id="phi.math.reshaped_native"><code class="name flex">
<span>def <span class="ident">reshaped_native</span></span>(<span>value: phi.math._tensors.Tensor, groups: tuple, force_expand: Any = False, to_numpy=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a native representation of <code>value</code> where dimensions are laid out according to <code>groups</code>.</p>
<p>See Also:
<code><a title="phi.math.native" href="#phi.math.native">native()</a></code>, <code><a title="phi.math.pack_dims" href="#phi.math.pack_dims">pack_dims()</a></code>, <code><a title="phi.math.reshaped_tensor" href="#phi.math.reshaped_tensor">reshaped_tensor()</a></code>, <code><a title="phi.math.reshaped_numpy" href="#phi.math.reshaped_numpy">reshaped_numpy()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>groups</code></strong></dt>
<dd>Sequence of dimension names as <code>str</code> or groups of dimensions to be packed_dim as <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
<dt><strong><code>force_expand</code></strong></dt>
<dd><code>bool</code> or sequence of dimensions.
If <code>True</code>, repeats the tensor along missing dimensions.
If <code>False</code>, puts singleton dimensions where possible.
If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.</dd>
<dt><strong><code>to_numpy</code></strong></dt>
<dd>If True, converts the native tensor to a <code>numpy.ndarray</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Native tensor with dimensions matching <code>groups</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reshaped_native(value: Tensor,
                    groups: tuple or list,
                    force_expand: Any = False,
                    to_numpy=False):
    &#34;&#34;&#34;
    Returns a native representation of `value` where dimensions are laid out according to `groups`.

    See Also:
        `native()`, `pack_dims()`, `reshaped_tensor()`, `reshaped_numpy()`.

    Args:
        value: `Tensor`
        groups: Sequence of dimension names as `str` or groups of dimensions to be packed_dim as `Shape`.
        force_expand: `bool` or sequence of dimensions.
            If `True`, repeats the tensor along missing dimensions.
            If `False`, puts singleton dimensions where possible.
            If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.
        to_numpy: If True, converts the native tensor to a `numpy.ndarray`.

    Returns:
        Native tensor with dimensions matching `groups`.
    &#34;&#34;&#34;
    assert isinstance(value, Tensor), f&#34;value must be a Tensor but got {type(value)}&#34;
    order = []
    for i, group in enumerate(groups):
        if isinstance(group, Shape):
            present = value.shape.only(group)
            if force_expand is True or present.volume &gt; 1 or (force_expand is not False and group.only(force_expand).volume &gt; 1):
                value = expand(value, group)
            value = pack_dims(value, group, batch(f&#34;group{i}&#34;))
            order.append(f&#34;group{i}&#34;)
        else:
            assert isinstance(group, str), f&#34;Groups must be either str or Shape but got {group}&#34;
            order.append(group)
    return value.numpy(order) if to_numpy else value.native(order)</code></pre>
</details>
</dd>
<dt id="phi.math.reshaped_numpy"><code class="name flex">
<span>def <span class="ident">reshaped_numpy</span></span>(<span>value: phi.math._tensors.Tensor, groups: tuple, force_expand: Any = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the NumPy representation of <code>value</code> where dimensions are laid out according to <code>groups</code>.</p>
<p>See Also:
<code><a title="phi.math.numpy" href="#phi.math.numpy">numpy()</a></code>, <code><a title="phi.math.reshaped_native" href="#phi.math.reshaped_native">reshaped_native()</a></code>, <code><a title="phi.math.pack_dims" href="#phi.math.pack_dims">pack_dims()</a></code>, <code><a title="phi.math.reshaped_tensor" href="#phi.math.reshaped_tensor">reshaped_tensor()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>groups</code></strong></dt>
<dd>Sequence of dimension names as <code>str</code> or groups of dimensions to be packed_dim as <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
<dt><strong><code>force_expand</code></strong></dt>
<dd><code>bool</code> or sequence of dimensions.
If <code>True</code>, repeats the tensor along missing dimensions.
If <code>False</code>, puts singleton dimensions where possible.
If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>NumPy <code>ndarray</code> with dimensions matching <code>groups</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reshaped_numpy(value: Tensor, groups: tuple or list, force_expand: Any = False):
    &#34;&#34;&#34;
    Returns the NumPy representation of `value` where dimensions are laid out according to `groups`.

    See Also:
        `numpy()`, `reshaped_native()`, `pack_dims()`, `reshaped_tensor()`.

    Args:
        value: `Tensor`
        groups: Sequence of dimension names as `str` or groups of dimensions to be packed_dim as `Shape`.
        force_expand: `bool` or sequence of dimensions.
            If `True`, repeats the tensor along missing dimensions.
            If `False`, puts singleton dimensions where possible.
            If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.

    Returns:
        NumPy `ndarray` with dimensions matching `groups`.
    &#34;&#34;&#34;
    return reshaped_native(value, groups, force_expand=force_expand, to_numpy=True)</code></pre>
</details>
</dd>
<dt id="phi.math.reshaped_tensor"><code class="name flex">
<span>def <span class="ident">reshaped_tensor</span></span>(<span>value: Any, groups: tuple, check_sizes=False, convert=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> from a native tensor or tensor-like whereby the dimensions of <code>value</code> are split according to <code>groups</code>.</p>
<p>See Also:
<code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code>, <code><a title="phi.math.reshaped_native" href="#phi.math.reshaped_native">reshaped_native()</a></code>, <code><a title="phi.math.unpack_dim" href="#phi.math.unpack_dim">unpack_dim()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>Native tensor or tensor-like.</dd>
<dt><strong><code>groups</code></strong></dt>
<dd>Sequence of dimension groups to be packed_dim as <code>tuple[<a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>]</code> or <code>list[<a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>]</code>.</dd>
<dt><strong><code>check_sizes</code></strong></dt>
<dd>If True, group sizes must match the sizes of <code>value</code> exactly. Otherwise, allows singleton dimensions.</dd>
<dt><strong><code>convert</code></strong></dt>
<dd>If True, converts the data to the native format of the current default backend.
If False, wraps the data in a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> but keeps the given data reference if possible.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with all dimensions from <code>groups</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reshaped_tensor(value: Any,
                    groups: tuple or list,
                    check_sizes=False,
                    convert=True):
    &#34;&#34;&#34;
    Creates a `Tensor` from a native tensor or tensor-like whereby the dimensions of `value` are split according to `groups`.

    See Also:
        `phi.math.tensor()`, `reshaped_native()`, `unpack_dim()`.

    Args:
        value: Native tensor or tensor-like.
        groups: Sequence of dimension groups to be packed_dim as `tuple[Shape]` or `list[Shape]`.
        check_sizes: If True, group sizes must match the sizes of `value` exactly. Otherwise, allows singleton dimensions.
        convert: If True, converts the data to the native format of the current default backend.
            If False, wraps the data in a `Tensor` but keeps the given data reference if possible.

    Returns:
        `Tensor` with all dimensions from `groups`
    &#34;&#34;&#34;
    assert all(isinstance(g, Shape) for g in groups), &#34;groups must be a sequence of Shapes&#34;
    dims = [batch(f&#39;group{i}&#39;) for i, group in enumerate(groups)]
    try:
        value = tensor(value, *dims, convert=convert)
    except IncompatibleShapes:
        raise IncompatibleShapes(f&#34;Cannot reshape native tensor with sizes {value.shape} given groups {groups}&#34;)
    for i, group in enumerate(groups):
        if value.shape.get_size(f&#39;group{i}&#39;) == group.volume:
            value = unpack_dim(value, f&#39;group{i}&#39;, group)
        elif check_sizes:
            raise AssertionError(f&#34;Group {group} does not match dimension {i} of value {value.shape}&#34;)
        else:
            value = unpack_dim(value, f&#39;group{i}&#39;, group)
    return value</code></pre>
</details>
</dd>
<dt id="phi.math.rotate_vector"><code class="name flex">
<span>def <span class="ident">rotate_vector</span></span>(<span>vector: phi.math._tensors.Tensor, angle: float) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Rotates <code>vector</code> around the origin.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>vector</code></strong></dt>
<dd>n-dimensional vector with a channel dimension called <code>'vector'</code></dd>
<dt><strong><code>angle</code></strong></dt>
<dd>Euler angle. The direction is the rotation axis and the length is the amount (in radians).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Rotated vector as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rotate_vector(vector: math.Tensor, angle: float or math.Tensor) -&gt; Tensor:
    &#34;&#34;&#34;
    Rotates `vector` around the origin.

    Args:
        vector: n-dimensional vector with a channel dimension called `&#39;vector&#39;`
        angle: Euler angle. The direction is the rotation axis and the length is the amount (in radians).

    Returns:
        Rotated vector as `Tensor`
    &#34;&#34;&#34;
    assert &#39;vector&#39; in vector.shape, &#34;vector must have &#39;vector&#39; dimension.&#34;
    if vector.vector.size == 2:
        sin = wrap(math.sin(angle))
        cos = wrap(math.cos(angle))
        x, y = vector.vector
        rot_x = cos * x - sin * y
        rot_y = sin * x + cos * y
        return math.stack_tensors([rot_x, rot_y], channel(vector=vector.vector.item_names))
    elif vector.vector.size == 1:
        raise AssertionError(f&#34;Cannot rotate a 1D vector. shape={vector.shape}&#34;)
    else:
        raise NotImplementedError(f&#34;Rotation in {vector.vector.size}D not yet implemented.&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.round"><code class="name flex">
<span>def <span class="ident">round</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Rounds the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code> to the closest integer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def round_(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Rounds the `Tensor` or `PhiTreeNode` `x` to the closest integer. &#34;&#34;&#34;
    return _backend_op1(x, Backend.round)</code></pre>
</details>
</dd>
<dt id="phi.math.sample_subgrid"><code class="name flex">
<span>def <span class="ident">sample_subgrid</span></span>(<span>grid: phi.math._tensors.Tensor, start: phi.math._tensors.Tensor, size: phi.math._shape.Shape) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Samples a sub-grid from <code>grid</code> with equal distance between sampling points.
The values at the new sample points are determined via linear interpolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> to be resampled. Values are assumed to be sampled at cell centers.</dd>
<dt><strong><code>start</code></strong></dt>
<dd>Origin point of sub-grid within <code>grid</code>, measured in number of cells.
Must have a single dimension called <code>vector</code>.
Example: <code>start=(1, 0.5)</code> would slice off the first grid point in dim 1 and take the mean of neighbouring points in dim 2.
The order of dims must be equal to <code>size</code> and <code>grid.shape.spatial</code>.</dd>
<dt><strong><code>size</code></strong></dt>
<dd>Resolution of the sub-grid. Must not be larger than the resolution of <code>grid</code>.
The order of dims must be equal to <code>start</code> and <code>grid.shape.spatial</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Sub-grid as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_subgrid(grid: Tensor, start: Tensor, size: Shape) -&gt; Tensor:
    &#34;&#34;&#34;
    Samples a sub-grid from `grid` with equal distance between sampling points.
    The values at the new sample points are determined via linear interpolation.

    Args:
        grid: `Tensor` to be resampled. Values are assumed to be sampled at cell centers.
        start: Origin point of sub-grid within `grid`, measured in number of cells.
            Must have a single dimension called `vector`.
            Example: `start=(1, 0.5)` would slice off the first grid point in dim 1 and take the mean of neighbouring points in dim 2.
            The order of dims must be equal to `size` and `grid.shape.spatial`.
        size: Resolution of the sub-grid. Must not be larger than the resolution of `grid`.
            The order of dims must be equal to `start` and `grid.shape.spatial`.

    Returns:
      Sub-grid as `Tensor`
    &#34;&#34;&#34;
    assert start.shape.names == (&#39;vector&#39;,)
    assert grid.shape.spatial.names == size.names
    assert math.all_available(start), &#34;Cannot perform sample_subgrid() during tracing, &#39;start&#39; must be known.&#34;
    crop = {}
    for dim, d_start, d_size in zip(grid.shape.spatial.names, start, size.sizes):
        crop[dim] = slice(int(d_start), int(d_start) + d_size + (0 if d_start % 1 in (0, 1) else 1))
    grid = grid[crop]
    upper_weight = start % 1
    lower_weight = 1 - upper_weight
    for i, dim in enumerate(grid.shape.spatial.names):
        if upper_weight[i].native() not in (0, 1):
            lower, upper = shift(grid, (0, 1), [dim], padding=None, stack_dim=None)
            grid = upper * upper_weight[i] + lower * lower_weight[i]
    return grid</code></pre>
</details>
</dd>
<dt id="phi.math.scatter"><code class="name flex">
<span>def <span class="ident">scatter</span></span>(<span>base_grid: phi.math._tensors.Tensor, indices: phi.math._tensors.Tensor, values: phi.math._tensors.Tensor, mode: str = 'update', outside_handling: str = 'discard', indices_gradient=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Scatters <code>values</code> into <code>base_grid</code> at <code>indices</code>.
instance dimensions of <code>indices</code> and/or <code>values</code> are reduced during scattering.
Depending on <code>mode</code>, this method has one of the following effects:</p>
<ul>
<li><code>mode='update'</code>: Replaces the values of <code>base_grid</code> at <code>indices</code> by <code>values</code>. The result is undefined if <code>indices</code> contains duplicates.</li>
<li><code>mode='add'</code>: Adds <code>values</code> to <code>base_grid</code> at <code>indices</code>. The values corresponding to duplicate indices are accumulated.</li>
<li><code>mode='mean'</code>: Replaces the values of <code>base_grid</code> at <code>indices</code> by the mean of all <code>values</code> with the same index.</li>
</ul>
<p>Implementations:</p>
<ul>
<li>NumPy: Slice assignment / <code>numpy.add.at</code></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.scatter.html"><code>torch.scatter</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.scatter_add.html"><code>torch.scatter_add</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add"><code>tf.tensor_scatter_nd_add</code></a>, <a href="https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update"><code>tf.tensor_scatter_nd_update</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter_add.html"><code>jax.lax.scatter_add</code></a>, <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter.html"><code>jax.lax.scatter</code></a></li>
</ul>
<p>See Also:
<code><a title="phi.math.gather" href="#phi.math.gather">gather()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>base_grid</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> into which <code>values</code> are scattered.</dd>
<dt><strong><code>indices</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of n-dimensional indices at which to place <code>values</code>.
Must have a single channel dimension with size matching the number of spatial dimensions of <code>base_grid</code>.
This dimension is optional if the spatial rank is 1.
Must also contain all <code>scatter_dims</code>.</dd>
<dt><strong><code>values</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of values to scatter at <code>indices</code>.</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>Scatter mode as <code>str</code>. One of ('add', 'mean', 'update')</dd>
<dt><strong><code>outside_handling</code></strong></dt>
<dd>
<p>Defines how indices lying outside the bounds of <code>base_grid</code> are handled.</p>
<ul>
<li><code>'discard'</code>: outside indices are ignored.</li>
<li><code>'clamp'</code>: outside indices are projected onto the closest point inside the grid.</li>
<li><code>'undefined'</code>: All points are expected to lie inside the grid. Otherwise an error may be thrown or an undefined tensor may be returned.</li>
</ul>
</dd>
<dt><strong><code>indices_gradient</code></strong></dt>
<dd>Whether to allow the gradient of this operation to be backpropagated through <code>indices</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of <code>base_grid</code> with updated values at <code>indices</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scatter(base_grid: Tensor or Shape,
            indices: Tensor,
            values: Tensor or float,
            mode: str = &#39;update&#39;,
            outside_handling: str = &#39;discard&#39;,
            indices_gradient=False):
    &#34;&#34;&#34;
    Scatters `values` into `base_grid` at `indices`.
    instance dimensions of `indices` and/or `values` are reduced during scattering.
    Depending on `mode`, this method has one of the following effects:

    * `mode=&#39;update&#39;`: Replaces the values of `base_grid` at `indices` by `values`. The result is undefined if `indices` contains duplicates.
    * `mode=&#39;add&#39;`: Adds `values` to `base_grid` at `indices`. The values corresponding to duplicate indices are accumulated.
    * `mode=&#39;mean&#39;`: Replaces the values of `base_grid` at `indices` by the mean of all `values` with the same index.

    Implementations:

    * NumPy: Slice assignment / `numpy.add.at`
    * PyTorch: [`torch.scatter`](https://pytorch.org/docs/stable/generated/torch.scatter.html), [`torch.scatter_add`](https://pytorch.org/docs/stable/generated/torch.scatter_add.html)
    * TensorFlow: [`tf.tensor_scatter_nd_add`](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add), [`tf.tensor_scatter_nd_update`](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update)
    * Jax: [`jax.lax.scatter_add`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter_add.html), [`jax.lax.scatter`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter.html)

    See Also:
        `gather()`.

    Args:
        base_grid: `Tensor` into which `values` are scattered.
        indices: `Tensor` of n-dimensional indices at which to place `values`.
            Must have a single channel dimension with size matching the number of spatial dimensions of `base_grid`.
            This dimension is optional if the spatial rank is 1.
            Must also contain all `scatter_dims`.
        values: `Tensor` of values to scatter at `indices`.
        mode: Scatter mode as `str`. One of (&#39;add&#39;, &#39;mean&#39;, &#39;update&#39;)
        outside_handling: Defines how indices lying outside the bounds of `base_grid` are handled.

            * `&#39;discard&#39;`: outside indices are ignored.
            * `&#39;clamp&#39;`: outside indices are projected onto the closest point inside the grid.
            * `&#39;undefined&#39;`: All points are expected to lie inside the grid. Otherwise an error may be thrown or an undefined tensor may be returned.
        indices_gradient: Whether to allow the gradient of this operation to be backpropagated through `indices`.

    Returns:
        Copy of `base_grid` with updated values at `indices`.
    &#34;&#34;&#34;
    assert mode in (&#39;update&#39;, &#39;add&#39;, &#39;mean&#39;)
    assert outside_handling in (&#39;discard&#39;, &#39;clamp&#39;, &#39;undefined&#39;)
    assert isinstance(indices_gradient, bool)
    grid_shape = base_grid if isinstance(base_grid, Shape) else base_grid.shape
    assert indices.shape.channel.names == (&#39;vector&#39;,) or (grid_shape.spatial_rank + grid_shape.instance_rank == 1 and indices.shape.channel_rank == 0)
    values = wrap(values)
    batches = values.shape.non_channel.non_instance &amp; indices.shape.non_channel.non_instance
    channels = grid_shape.channel &amp; values.shape.channel
    # --- Set up grid ---
    if isinstance(base_grid, Shape):
        with choose_backend_t(indices, values):
            base_grid = zeros(base_grid &amp; batches &amp; values.shape.channel)
        if mode != &#39;add&#39;:
            base_grid += math.nan
    # --- Handle outside indices ---
    if outside_handling == &#39;clamp&#39;:
        indices = clip(indices, 0, tensor(grid_shape.spatial, channel(&#39;vector&#39;)) - 1)
    elif outside_handling == &#39;discard&#39;:
        indices_linear = pack_dims(indices, instance, instance(_scatter_instance=1))
        indices_inside = min_((round_(indices_linear) &gt;= 0) &amp; (round_(indices_linear) &lt; tensor(grid_shape.spatial, channel(&#39;vector&#39;))), &#39;vector&#39;)
        indices_linear = boolean_mask(indices_linear, &#39;_scatter_instance&#39;, indices_inside)
        if instance(values).rank &gt; 0:
            values_linear = pack_dims(values, instance, instance(_scatter_instance=1))
            values_linear = boolean_mask(values_linear, &#39;_scatter_instance&#39;, indices_inside)
            values = unpack_dim(values_linear, &#39;_scatter_instance&#39;, instance(values))
        indices = unpack_dim(indices_linear, &#39;_scatter_instance&#39;, instance(indices))
        if indices.shape.is_non_uniform:
            raise NotImplementedError()
    lists = indices.shape.instance &amp; values.shape.instance

    def scatter_forward(base_grid, indices, values):
        indices = to_int32(round_(indices))
        native_grid = reshaped_native(base_grid, [batches, *base_grid.shape.instance, *base_grid.shape.spatial, channels], force_expand=True)
        native_values = reshaped_native(values, [batches, lists, channels], force_expand=True)
        native_indices = reshaped_native(indices, [batches, lists, &#39;vector&#39;], force_expand=True)
        backend = choose_backend(native_indices, native_values, native_grid)
        if mode in (&#39;add&#39;, &#39;update&#39;):
            native_result = backend.scatter(native_grid, native_indices, native_values, mode=mode)
        else:  # mean
            zero_grid = backend.zeros_like(native_grid)
            summed = backend.scatter(zero_grid, native_indices, native_values, mode=&#39;add&#39;)
            count = backend.scatter(zero_grid, native_indices, backend.ones_like(native_values), mode=&#39;add&#39;)
            native_result = summed / backend.maximum(count, 1)
            native_result = backend.where(count == 0, native_grid, native_result)
        return reshaped_tensor(native_result, [batches, *instance(base_grid), *spatial(base_grid), channels], check_sizes=True)

    def scatter_backward(shaped_base_grid_, shaped_indices_, shaped_values_, output, d_output):
        from ._nd import spatial_gradient
        values_grad = gather(d_output, shaped_indices_)
        spatial_gradient_indices = gather(spatial_gradient(d_output), shaped_indices_)
        indices_grad = mean(spatial_gradient_indices * shaped_values_, &#39;vector_&#39;)
        return None, indices_grad, values_grad

    scatter_function = scatter_forward
    if indices_gradient:
        from phi.math import custom_gradient
        scatter_function = custom_gradient(scatter_forward, scatter_backward)

    result = scatter_function(base_grid, indices, values)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.seed"><code class="name flex">
<span>def <span class="ident">seed</span></span>(<span>seed: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the current seed of all backends and the built-in <code>random</code> package.</p>
<p>Calling this function with a fixed value at the start of an application yields reproducible results
as long as the same backend is used.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>seed</code></strong></dt>
<dd>Seed to use.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def seed(seed: int):
    &#34;&#34;&#34;
    Sets the current seed of all backends and the built-in `random` package.

    Calling this function with a fixed value at the start of an application yields reproducible results
    as long as the same backend is used.

    Args:
        seed: Seed to use.
    &#34;&#34;&#34;
    for backend in BACKENDS:
        backend.seed(seed)
    import random
    random.seed(0)</code></pre>
</details>
</dd>
<dt id="phi.math.set_global_precision"><code class="name flex">
<span>def <span class="ident">set_global_precision</span></span>(<span>floating_point_bits: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the floating point precision of DYNAMIC_BACKEND which affects all registered backends.</p>
<p>If <code>floating_point_bits</code> is an integer, all floating point tensors created henceforth will be of the corresponding data type, float16, float32 or float64.
Operations may also convert floating point values to this precision, even if the input had a different precision.</p>
<p>If <code>floating_point_bits</code> is None, new tensors will default to float32 unless specified otherwise.
The output of math operations has the same precision as its inputs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>floating_point_bits</code></strong></dt>
<dd>one of (16, 32, 64, None)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_global_precision(floating_point_bits: int):
    &#34;&#34;&#34;
    Sets the floating point precision of DYNAMIC_BACKEND which affects all registered backends.

    If `floating_point_bits` is an integer, all floating point tensors created henceforth will be of the corresponding data type, float16, float32 or float64.
    Operations may also convert floating point values to this precision, even if the input had a different precision.

    If `floating_point_bits` is None, new tensors will default to float32 unless specified otherwise.
    The output of math operations has the same precision as its inputs.

    Args:
      floating_point_bits: one of (16, 32, 64, None)
    &#34;&#34;&#34;
    _PRECISION[0] = floating_point_bits</code></pre>
</details>
</dd>
<dt id="phi.math.shape"><code class="name flex">
<span>def <span class="ident">shape</span></span>(<span>obj) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>If <code>obj</code> is a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.magic.Shaped" href="magic.html#phi.math.magic.Shaped">Shaped</a></code>, returns its shape.
If <code>obj</code> is a <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>, returns <code>obj</code>.</p>
<p>This function can be passed as a <code>dim</code> argument to an operation to specify that it should act upon all dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or <code>Shaped</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shape(obj) -&gt; Shape:
    &#34;&#34;&#34;
    If `obj` is a `Tensor` or `phi.math.magic.Shaped`, returns its shape.
    If `obj` is a `Shape`, returns `obj`.

    This function can be passed as a `dim` argument to an operation to specify that it should act upon all dimensions.

    Args:
        obj: `Tensor` or `Shape` or `Shaped`

    Returns:
        `Shape`
    &#34;&#34;&#34;
    from phi.math.magic import PhiTreeNode
    if isinstance(obj, Shape):
        return obj
    elif hasattr(obj, &#39;__shape__&#39;):
        return obj.__shape__()
    elif hasattr(obj, &#39;shape&#39;) and isinstance(obj.shape, Shape):
        return obj.shape
    elif isinstance(obj, (int, float, complex, bool)):
        return EMPTY_SHAPE
    elif isinstance(obj, (tuple, list)):
        return channel(&#39;vector&#39;)
    elif isinstance(obj, (Number, bool)):
        return EMPTY_SHAPE
    elif isinstance(obj, PhiTreeNode):
        from phi.math._magic_ops import all_attributes
        return merge_shapes(*[getattr(obj, a) for a in all_attributes(obj)])
    else:
        from .backend import choose_backend, NoBackendFound
        try:
            backend = choose_backend(obj)
            shape_tuple = backend.staticshape(obj)
            if len(shape_tuple) == 0:
                return EMPTY_SHAPE
            elif len(shape_tuple) == 1:
                return channel(&#39;vector&#39;)
            else:
                raise ValueError(f&#34;Cannot auto-complete shape of {backend} tensor with shape {shape_tuple}. Only 0D and 1D tensors have a Φ-Flow shape by default.&#34;)
        except NoBackendFound:
            raise ValueError(f&#39;shape() requires Shaped or Shape argument but got {type(obj)}&#39;)</code></pre>
</details>
</dd>
<dt id="phi.math.shift"><code class="name flex">
<span>def <span class="ident">shift</span></span>(<span>x: phi.math._tensors.Tensor, offsets: tuple, dims: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function spatial&gt;, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, stack_dim: Optional[phi.math._shape.Shape] = (shiftᶜ=None), extend_bounds=0) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>shift Tensor by a fixed offset and abiding by extrapolation</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Input data</dd>
<dt><strong><code>offsets</code></strong></dt>
<dd>Shift size</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions along which to shift, defaults to None</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>padding to be performed at the boundary, defaults to extrapolation.BOUNDARY</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>dimensions to be stacked, defaults to 'shift'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>offset_tensor</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shift(x: Tensor,
          offsets: tuple,
          dims: DimFilter = math.spatial,
          padding: Extrapolation or None = extrapolation.BOUNDARY,
          stack_dim: Optional[Shape] = channel(&#39;shift&#39;),
          extend_bounds=0) -&gt; list:
    &#34;&#34;&#34;
    shift Tensor by a fixed offset and abiding by extrapolation

    Args:
        x: Input data
        offsets: Shift size
        dims: Dimensions along which to shift, defaults to None
        padding: padding to be performed at the boundary, defaults to extrapolation.BOUNDARY
        stack_dim: dimensions to be stacked, defaults to &#39;shift&#39;

    Returns:
        list: offset_tensor

    &#34;&#34;&#34;
    if dims is None:
        raise ValueError(&#34;dims=None is not supported anymore.&#34;)
    dims = x.shape.only(dims).names
    if stack_dim is None:
        assert len(dims) == 1
    x = wrap(x)
    pad_lower = max(0, -min(offsets))
    pad_upper = max(0, max(offsets))
    if padding:
        x = math.pad(x, {axis: (pad_lower + extend_bounds, pad_upper + extend_bounds) for axis in dims}, mode=padding)
    if extend_bounds:
        assert padding is not None
    offset_tensors = []
    for offset in offsets:
        components = []
        for dimension in dims:
            if padding:
                slices = {dim: slice(pad_lower + offset, (-pad_upper + offset) or None) if dim == dimension else slice(pad_lower, -pad_upper or None) for dim in dims}
            else:
                slices = {dim: slice(pad_lower + offset, (-pad_upper + offset) or None) if dim == dimension else slice(None, None) for dim in dims}
            components.append(x[slices])
        offset_tensors.append(stack(components, stack_dim) if stack_dim is not None else components[0])
    return offset_tensors</code></pre>
</details>
</dd>
<dt id="phi.math.sigmoid"><code class="name flex">
<span>def <span class="ident">sigmoid</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the sigmoid function of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sigmoid(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Computes the sigmoid function of the `Tensor` or `PhiTreeNode` `x`. &#34;&#34;&#34;
    return _backend_op1(x, Backend.sigmoid)</code></pre>
</details>
</dd>
<dt id="phi.math.sign"><code class="name flex">
<span>def <span class="ident">sign</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>The sign of positive numbers is 1 and -1 for negative numbers.
The sign of 0 is undefined.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> matching <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sign(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34;
    The sign of positive numbers is 1 and -1 for negative numbers.
    The sign of 0 is undefined.

    Args:
        x: `Tensor` or `PhiTreeNode`

    Returns:
        `Tensor` or `PhiTreeNode` matching `x`.
    &#34;&#34;&#34;
    return _backend_op1(x, Backend.sign)</code></pre>
</details>
</dd>
<dt id="phi.math.sin"><code class="name flex">
<span>def <span class="ident">sin</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>sin(x)</em> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sin(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Computes *sin(x)* of the `Tensor` or `PhiTreeNode` `x`. &#34;&#34;&#34;
    return _backend_op1(x, Backend.sin)</code></pre>
</details>
</dd>
<dt id="phi.math.solve_linear"><code class="name flex">
<span>def <span class="ident">solve_linear</span></span>(<span>f: Callable[[~X], ~Y], y: ~Y, solve: phi.math._functional.Solve[~X, ~Y], f_args: tuple = (), f_kwargs: dict = None) ‑> ~X</span>
</code></dt>
<dd>
<div class="desc"><p>Solves the system of linear equations <em>f(x) = y</em> and returns <em>x</em>.
For maximum performance, compile <code>f</code> using <code><a title="phi.math.jit_compile_linear" href="#phi.math.jit_compile_linear">jit_compile_linear()</a></code> beforehand.
Then, an optimized representation of <code>f</code> (such as a sparse matrix) will be used to solve the linear system.</p>
<p>To obtain additional information about the performed solve, use a <code><a title="phi.math.SolveTape" href="#phi.math.SolveTape">SolveTape</a></code>.</p>
<p>The gradient of this operation will perform another linear solve with the parameters specified by <code><a title="phi.math.Solve.gradient_solve" href="#phi.math.Solve.gradient_solve">Solve.gradient_solve</a></code>.</p>
<p>See Also:
<code><a title="phi.math.solve_nonlinear" href="#phi.math.solve_nonlinear">solve_nonlinear()</a></code>, <code><a title="phi.math.jit_compile_linear" href="#phi.math.jit_compile_linear">jit_compile_linear()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Linear function with <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> first parameter and return value.
<code>f</code> can have additional arguments.</dd>
<dt><strong><code>y</code></strong></dt>
<dd>Desired output of <code>f(x)</code> as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code>.</dd>
<dt><strong><code>solve</code></strong></dt>
<dd><code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code> object specifying optimization method, parameters and initial guess for <code>x</code>.</dd>
<dt><strong><code>f_args</code></strong></dt>
<dd>Additional <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> arguments to be passed to <code>f</code>.
<code>f</code> need not be linear in these arguments.
Use this instead of lambda function since a lambda will not be recognized as calling a jit-compiled function.</dd>
<dt><strong><code>f_kwargs</code></strong></dt>
<dd>Additional keyword arguments to be passed to <code>f</code>.
These arguments are treated as auxiliary arguments and can be of any type.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>x</code></dt>
<dd>solution of the linear system of equations <code>f(x) = y</code> as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code>.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code><a title="phi.math.NotConverged" href="#phi.math.NotConverged">NotConverged</a></code></dt>
<dd>If the desired accuracy was not be reached within the maximum number of iterations.</dd>
<dt><code><a title="phi.math.Diverged" href="#phi.math.Diverged">Diverged</a></code></dt>
<dd>If the solve failed prematurely.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def solve_linear(f: Callable[[X], Y],
                 y: Y, solve: Solve[X, Y],
                 f_args: tuple or list = (),
                 f_kwargs: dict = None) -&gt; X:
    &#34;&#34;&#34;
    Solves the system of linear equations *f(x) = y* and returns *x*.
    For maximum performance, compile `f` using `jit_compile_linear()` beforehand.
    Then, an optimized representation of `f` (such as a sparse matrix) will be used to solve the linear system.

    To obtain additional information about the performed solve, use a `SolveTape`.

    The gradient of this operation will perform another linear solve with the parameters specified by `Solve.gradient_solve`.

    See Also:
        `solve_nonlinear()`, `jit_compile_linear()`.

    Args:
        f: Linear function with `Tensor` or `PhiTreeNode` first parameter and return value.
            `f` can have additional arguments.
        y: Desired output of `f(x)` as `Tensor` or `PhiTreeNode`.
        solve: `Solve` object specifying optimization method, parameters and initial guess for `x`.
        f_args: Additional `Tensor` or `PhiTreeNode` arguments to be passed to `f`.
            `f` need not be linear in these arguments.
            Use this instead of lambda function since a lambda will not be recognized as calling a jit-compiled function.
        f_kwargs: Additional keyword arguments to be passed to `f`.
            These arguments are treated as auxiliary arguments and can be of any type.

    Returns:
        x: solution of the linear system of equations `f(x) = y` as `Tensor` or `PhiTreeNode`.

    Raises:
        NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.
        Diverged: If the solve failed prematurely.
    &#34;&#34;&#34;
    y_tree, y_tensors = disassemble_tree(y)
    x0_tree, x0_tensors = disassemble_tree(solve.x0)
    assert len(x0_tensors) == len(y_tensors) == 1, &#34;Only single-tensor linear solves are currently supported&#34;
    backend = choose_backend_t(*y_tensors, *x0_tensors)

    if isinstance(f, LinearFunction) and (backend.supports(Backend.sparse_coo_tensor) or backend.supports(Backend.csr_matrix)):  # Matrix solve
        matrix, bias = f.sparse_matrix_and_bias(solve.x0, *f_args, **(f_kwargs or {}))

        def _matrix_solve_forward(y, solve: Solve, matrix: SparseMatrixContainer, is_backprop=False):
            matrix_native = matrix.native()
            active_dims = matrix.src_shape
            result = _linear_solve_forward(y, solve, matrix_native, active_dims=active_dims, backend=backend, is_backprop=is_backprop)
            return result  # must return exactly `x` so gradient isn&#39;t computed w.r.t. other quantities

        _matrix_solve = attach_gradient_solve(_matrix_solve_forward, auxiliary_args=&#39;is_backprop&#39;)
        return _matrix_solve(y - bias, solve, matrix)
    else:  # Matrix-free solve
        f_args = cached(f_args)
        solve = cached(solve)

        def _function_solve_forward(y, solve: Solve, f_args: tuple, f_kwargs: dict = None, is_backprop=False):
            y_nest, (y_tensor,) = disassemble_tree(y)
            x0_nest, (x0_tensor,) = disassemble_tree(solve.x0)
            active_dims = (y_tensor.shape &amp; x0_tensor.shape).non_batch  # assumes batch dimensions are not active
            batches = (y_tensor.shape &amp; x0_tensor.shape).batch

            def native_lin_f(native_x, batch_index=None):
                if batch_index is not None and batches.volume &gt; 1:
                    native_x = backend.tile(backend.expand_dims(native_x), [batches.volume, 1])
                x = assemble_tree(x0_nest, [reshaped_tensor(native_x, [batches, active_dims] if backend.ndims(native_x) &gt;= 2 else [active_dims], convert=False)])
                y = f(x, *f_args, **f_kwargs)
                _, (y_tensor,) = disassemble_tree(y)
                y_native = reshaped_native(y_tensor, [batches, active_dims] if backend.ndims(native_x) &gt;= 2 else [active_dims])
                if batch_index is not None and batches.volume &gt; 1:
                    y_native = y_native[batch_index]
                return y_native

            result = _linear_solve_forward(y, solve, native_lin_f, active_dims=active_dims, backend=backend, is_backprop=is_backprop)
            return result  # must return exactly `x` so gradient isn&#39;t computed w.r.t. other quantities

        _function_solve = attach_gradient_solve(_function_solve_forward, auxiliary_args=&#39;is_backprop,f_kwargs&#39;)
        return _function_solve(y, solve, f_args, f_kwargs=f_kwargs or {})</code></pre>
</details>
</dd>
<dt id="phi.math.solve_nonlinear"><code class="name flex">
<span>def <span class="ident">solve_nonlinear</span></span>(<span>f: Callable, y, solve: phi.math._functional.Solve) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Solves the non-linear equation <em>f(x) = y</em> by minimizing the norm of the residual.</p>
<p>This method is limited to backends that support <code><a title="phi.math.jacobian" href="#phi.math.jacobian">jacobian()</a></code>, currently PyTorch, TensorFlow and Jax.</p>
<p>To obtain additional information about the performed solve, use a <code><a title="phi.math.SolveTape" href="#phi.math.SolveTape">SolveTape</a></code>.</p>
<p>See Also:
<code><a title="phi.math.minimize" href="#phi.math.minimize">minimize()</a></code>, <code><a title="phi.math.solve_linear" href="#phi.math.solve_linear">solve_linear()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function whose output is optimized to match <code>y</code>.
All positional arguments of <code>f</code> are optimized and must be <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code>.
The output of <code>f</code> must match <code>y</code>.</dd>
<dt><strong><code>y</code></strong></dt>
<dd>Desired output of <code>f(x)</code> as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code>.</dd>
<dt><strong><code>solve</code></strong></dt>
<dd><code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code> object specifying optimization method, parameters and initial guess for <code>x</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>x</code></dt>
<dd>Solution fulfilling <code>f(x) = y</code> within specified tolerance as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code>.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code><a title="phi.math.NotConverged" href="#phi.math.NotConverged">NotConverged</a></code></dt>
<dd>If the desired accuracy was not be reached within the maximum number of iterations.</dd>
<dt><code><a title="phi.math.Diverged" href="#phi.math.Diverged">Diverged</a></code></dt>
<dd>If the solve failed prematurely.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def solve_nonlinear(f: Callable, y, solve: Solve) -&gt; Tensor:
    &#34;&#34;&#34;
    Solves the non-linear equation *f(x) = y* by minimizing the norm of the residual.

    This method is limited to backends that support `jacobian()`, currently PyTorch, TensorFlow and Jax.

    To obtain additional information about the performed solve, use a `SolveTape`.

    See Also:
        `minimize()`, `solve_linear()`.

    Args:
        f: Function whose output is optimized to match `y`.
            All positional arguments of `f` are optimized and must be `Tensor` or `PhiTreeNode`.
            The output of `f` must match `y`.
        y: Desired output of `f(x)` as `Tensor` or `PhiTreeNode`.
        solve: `Solve` object specifying optimization method, parameters and initial guess for `x`.

    Returns:
        x: Solution fulfilling `f(x) = y` within specified tolerance as `Tensor` or `PhiTreeNode`.

    Raises:
        NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.
        Diverged: If the solve failed prematurely.
    &#34;&#34;&#34;
    from ._nd import l2_loss

    if solve.preprocess_y is not None:
        y = solve.preprocess_y(y)

    def min_func(x):
        diff = f(x) - y
        l2 = l2_loss(diff)
        return l2

    rel_tol_to_abs = solve.relative_tolerance * l2_loss(y)
    min_solve = copy_with(solve, absolute_tolerance=rel_tol_to_abs, relative_tolerance=0, preprocess_y=None)
    return minimize(min_func, min_solve)</code></pre>
</details>
</dd>
<dt id="phi.math.spatial"><code class="name flex">
<span>def <span class="ident">spatial</span></span>(<span>*args, **dims: int) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the spatial dimensions of an existing <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or creates a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> with only spatial dimensions.</p>
<p>Usage for filtering spatial dimensions:</p>
<pre><code class="language-python">spatial_dims = spatial(shape)
spatial_dims = spatial(tensor)
</code></pre>
<p>Usage for creating a <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> with only spatial dimensions:</p>
<pre><code class="language-python">spatial_shape = spatial('undef', x=2, y=3)
# Out: (x=2, y=3, undef=None)
</code></pre>
<p>Here, the dimension <code>undef</code> is created with an undefined size of <code>None</code>.
Undefined sizes are automatically filled in by <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code>, <code><a title="phi.math.wrap" href="#phi.math.wrap">wrap()</a></code>, <code><a title="phi.math.stack" href="#phi.math.stack">stack()</a></code> and <code><a title="phi.math.concat" href="#phi.math.concat">concat()</a></code>.</p>
<p>To create a shape with multiple types, use <code><a title="phi.math.merge_shapes" href="#phi.math.merge_shapes">merge_shapes()</a></code>, <code><a title="phi.math.concat_shapes" href="#phi.math.concat_shapes">concat_shapes()</a></code> or the syntax <code>shape1 &amp; shape2</code>.</p>
<p>See Also:
<code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code>, <code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>
<p>Either</p>
<ul>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> to filter or</li>
<li>Names of dimensions with undefined sizes as <code>str</code>.</li>
</ul>
</dd>
<dt><strong><code>**dims</code></strong></dt>
<dd>Dimension sizes and names. Must be empty when used as a filter operation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> containing only dimensions of type spatial.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial(*args, **dims: int or str or tuple or list or Shape) -&gt; Shape:
    &#34;&#34;&#34;
    Returns the spatial dimensions of an existing `Shape` or creates a new `Shape` with only spatial dimensions.

    Usage for filtering spatial dimensions:
    ```python
    spatial_dims = spatial(shape)
    spatial_dims = spatial(tensor)
    ```

    Usage for creating a `Shape` with only spatial dimensions:
    ```python
    spatial_shape = spatial(&#39;undef&#39;, x=2, y=3)
    # Out: (x=2, y=3, undef=None)
    ```
    Here, the dimension `undef` is created with an undefined size of `None`.
    Undefined sizes are automatically filled in by `tensor`, `wrap`, `stack` and `concat`.

    To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 &amp; shape2`.

    See Also:
        `channel`, `batch`, `instance`

    Args:
        *args: Either

            * `Shape` or `Tensor` to filter or
            * Names of dimensions with undefined sizes as `str`.

        **dims: Dimension sizes and names. Must be empty when used as a filter operation.

    Returns:
        `Shape` containing only dimensions of type spatial.
    &#34;&#34;&#34;
    from .magic import Shaped
    if all(isinstance(arg, str) for arg in args) or dims:
        return _construct_shape(SPATIAL_DIM, *args, **dims)
    elif len(args) == 1 and isinstance(args[0], Shape):
        return args[0].spatial
    elif len(args) == 1 and isinstance(args[0], Shaped):
        return shape(args[0]).spatial
    else:
        raise AssertionError(f&#34;spatial() must be called either as a selector spatial(Shape) or spatial(Tensor) or as a constructor spatial(*names, **dims). Got *args={args}, **dims={dims}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.spatial_gradient"><code class="name flex">
<span>def <span class="ident">spatial_gradient</span></span>(<span>grid: phi.math._tensors.Tensor, dx: float = 1, difference: str = 'central', padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function spatial&gt;, stack_dim: phi.math._shape.Shape = (gradientᶜ=None), pad=0) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the spatial_gradient of a scalar channel from finite differences.
The spatial_gradient vectors are in reverse order, lowest dimension first.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>grid values</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>(Optional) Dimensions along which the spatial derivative will be computed. sequence of dimension names</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Physical distance between grid points, <code>float</code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.
When passing a vector-valued <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, the dx values should be listed along <code>stack_dim</code>, matching <code>dims</code>.</dd>
<dt><strong><code>difference</code></strong></dt>
<dd>type of difference, one of ('forward', 'backward', 'central') (default 'forward')</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>tensor padding mode</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>name of the new vector dimension listing the spatial_gradient w.r.t. the various axes</dd>
<dt><strong><code>pad</code></strong></dt>
<dd>How many cells to extend the result compared to <code>grid</code>.
This value is added to the internal padding. For non-trivial extrapolations, this gives the correct result while manual padding before or after this operation would not respect the boundary locations.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial_gradient(grid: Tensor,
                     dx: float or Tensor = 1,
                     difference: str = &#39;central&#39;,
                     padding: Extrapolation or None = extrapolation.BOUNDARY,
                     dims: DimFilter = spatial,
                     stack_dim: Shape or None = channel(&#39;gradient&#39;),
                     pad=0) -&gt; Tensor:
    &#34;&#34;&#34;
    Calculates the spatial_gradient of a scalar channel from finite differences.
    The spatial_gradient vectors are in reverse order, lowest dimension first.

    Args:
        grid: grid values
        dims: (Optional) Dimensions along which the spatial derivative will be computed. sequence of dimension names
        dx: Physical distance between grid points, `float` or `Tensor`.
            When passing a vector-valued `Tensor`, the dx values should be listed along `stack_dim`, matching `dims`.
        difference: type of difference, one of (&#39;forward&#39;, &#39;backward&#39;, &#39;central&#39;) (default &#39;forward&#39;)
        padding: tensor padding mode
        stack_dim: name of the new vector dimension listing the spatial_gradient w.r.t. the various axes
        pad: How many cells to extend the result compared to `grid`.
            This value is added to the internal padding. For non-trivial extrapolations, this gives the correct result while manual padding before or after this operation would not respect the boundary locations.

    Returns:
        `Tensor`
    &#34;&#34;&#34;
    grid = wrap(grid)
    if stack_dim is not None and stack_dim in grid.shape:
        assert grid.shape.only(stack_dim).size == 1, f&#34;spatial_gradient() cannot list components along {stack_dim.name} because that dimension already exists on grid {grid}&#34;
        grid = grid[{stack_dim.name: 0}]
    dims = grid.shape.only(dims)
    dx = wrap(dx)
    if dx.vector.exists:
        dx = dx.vector[dims]
        if dx.vector.size in (None, 1):
            dx = dx.vector[0]
    if difference.lower() == &#39;central&#39;:
        left, right = shift(grid, (-1, 1), dims, padding, stack_dim=stack_dim, extend_bounds=pad)
        return (right - left) / (dx * 2)
    elif difference.lower() == &#39;forward&#39;:
        left, right = shift(grid, (0, 1), dims, padding, stack_dim=stack_dim, extend_bounds=pad)
        return (right - left) / dx
    elif difference.lower() == &#39;backward&#39;:
        left, right = shift(grid, (-1, 0), dims, padding, stack_dim=stack_dim, extend_bounds=pad)
        return (right - left) / dx
    else:
        raise ValueError(&#39;Invalid difference type: {}. Can be CENTRAL or FORWARD&#39;.format(difference))</code></pre>
</details>
</dd>
<dt id="phi.math.sqrt"><code class="name flex">
<span>def <span class="ident">sqrt</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>sqrt(x)</em> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sqrt(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Computes *sqrt(x)* of the `Tensor` or `PhiTreeNode` `x`. &#34;&#34;&#34;
    return _backend_op1(x, Backend.sqrt)</code></pre>
</details>
</dd>
<dt id="phi.math.stack"><code class="name flex">
<span>def <span class="ident">stack</span></span>(<span>values: tuple, dim: phi.math._shape.Shape, expand_values=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Stacks <code>values</code> along the new dimension <code>dim</code>.
All values must have the same spatial, instance and channel dimensions. If the dimension sizes vary, the resulting tensor will be non-uniform.
Batch dimensions will be added as needed.</p>
<p>Stacking tensors is performed lazily, i.e. the memory is allocated only when needed.
This makes repeated stacking and slicing along the same dimension very efficient, i.e. jit-compiled functions will not perform these operations.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Collection of <code><a title="phi.math.magic.Shapable" href="magic.html#phi.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>
If a <code>dict</code>, keys must be of type <code>str</code> and are used as item names along <code>dim</code>.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> with a least one dimension. None of these dimensions can be present with any of the <code>values</code>.
If <code>dim</code> is a single-dimension shape, its size is determined from <code>len(values)</code> and can be left undefined (<code>None</code>).
If <code>dim</code> is a multi-dimension shape, its volume must be equal to <code>len(values)</code>.</dd>
<dt><strong><code>expand_values</code></strong></dt>
<dd>If <code>True</code>, will first add missing dimensions to all values, not just batch dimensions.
This allows tensors with different dimensions to be stacked.
The resulting tensor will have all dimensions that are present in <code>values</code>.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> containing <code>values</code> stacked along <code>dim</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">stack({'x': 0, 'y': 1}, channel('vector'))
# Out: (x=0, y=1)

stack([math.zeros(batch(b=2)), math.ones(batch(b=2))], channel(c='x,y'))
# Out: (x=0.000, y=1.000); (x=0.000, y=1.000) (bᵇ=2, cᶜ=x,y)

stack([vec(x=1, y=0), vec(x=2, y=3.)], batch('b'))
# Out: (x=1.000, y=0.000); (x=2.000, y=3.000) (bᵇ=2, vectorᶜ=x,y)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stack(values: tuple or list or dict, dim: Shape, expand_values=False, **kwargs):
    &#34;&#34;&#34;
    Stacks `values` along the new dimension `dim`.
    All values must have the same spatial, instance and channel dimensions. If the dimension sizes vary, the resulting tensor will be non-uniform.
    Batch dimensions will be added as needed.

    Stacking tensors is performed lazily, i.e. the memory is allocated only when needed.
    This makes repeated stacking and slicing along the same dimension very efficient, i.e. jit-compiled functions will not perform these operations.

    Args:
        values: Collection of `phi.math.magic.Shapable`, such as `phi.math.Tensor`
            If a `dict`, keys must be of type `str` and are used as item names along `dim`.
        dim: `Shape` with a least one dimension. None of these dimensions can be present with any of the `values`.
            If `dim` is a single-dimension shape, its size is determined from `len(values)` and can be left undefined (`None`).
            If `dim` is a multi-dimension shape, its volume must be equal to `len(values)`.
        expand_values: If `True`, will first add missing dimensions to all values, not just batch dimensions.
            This allows tensors with different dimensions to be stacked.
            The resulting tensor will have all dimensions that are present in `values`.
        **kwargs: Additional keyword arguments required by specific implementations.
            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
            Adding batch dimensions must always work without keyword arguments.

    Returns:
        `Tensor` containing `values` stacked along `dim`.

    Examples:

        ```python
        stack({&#39;x&#39;: 0, &#39;y&#39;: 1}, channel(&#39;vector&#39;))
        # Out: (x=0, y=1)

        stack([math.zeros(batch(b=2)), math.ones(batch(b=2))], channel(c=&#39;x,y&#39;))
        # Out: (x=0.000, y=1.000); (x=0.000, y=1.000) (bᵇ=2, cᶜ=x,y)

        stack([vec(x=1, y=0), vec(x=2, y=3.)], batch(&#39;b&#39;))
        # Out: (x=1.000, y=0.000); (x=2.000, y=3.000) (bᵇ=2, vectorᶜ=x,y)
        ```
    &#34;&#34;&#34;
    assert len(values) &gt; 0, f&#34;stack() got empty sequence {values}&#34;
    assert isinstance(dim, Shape)
    values_ = tuple(values.values()) if isinstance(values, dict) else values
    if not expand_values:
        for v in values_[1:]:
            assert set(non_batch(v).names) == set(non_batch(values_[0]).names), f&#34;Stacked values must have the same non-batch dimensions but got {non_batch(values_[0])} and {non_batch(v)}&#34;
    # --- Add missing dimensions ---
    if expand_values:
        all_dims = merge_shapes(*values_)
        if isinstance(values, dict):
            values = {k: expand(v, all_dims.without(shape(v).non_batch)) for k, v in values.items()}
        else:
            values = [expand(v, all_dims.without(shape(v).non_batch)) for v in values]
    else:
        all_batch_dims = merge_shapes(*[batch(v) for v in values_])
        if isinstance(values, dict):
            values = {k: expand(v, all_batch_dims) for k, v in values.items()}
        else:
            values = [expand(v, all_batch_dims) for v in values]
    if dim.rank == 1:
        assert dim.size == len(values) or dim.size is None, f&#34;stack dim size must match len(values) or be undefined but got {dim} for {len(values)} values&#34;
        if dim.size is None:
            dim = dim.with_size(len(values))
        if isinstance(values, dict):
            dim_item_names = tuple(values.keys())
            values = tuple(values.values())
            dim = dim.with_size(dim_item_names)
        # --- First try __stack__ ---
        for v in values:
            if hasattr(v, &#39;__stack__&#39;):
                result = v.__stack__(values, dim, **kwargs)
                if result is not NotImplemented:
                    assert isinstance(result, Shapable), &#34;__stack__ must return a Shapable object&#34;
                    return result
        # --- Next: try stacking attributes for tree nodes ---
        if all(isinstance(v, PhiTreeNode) for v in values):
            attributes = all_attributes(values[0])
            if attributes and all(all_attributes(v) == attributes for v in values):
                new_attrs = {}
                for a in attributes:
                    assert all(shape(getattr(v, a)).only(dim).is_empty for v in values), f&#34;Cannot stack attribute {a} because one values contains the stack dimension {dim}.&#34;
                    a_values = [getattr(v, a) for v in values]
                    new_attrs[a] = stack(a_values, dim, expand_values=expand_values, **kwargs)
                return copy_with(values[0], **new_attrs)
            else:
                warnings.warn(f&#34;Failed to concat values using value attributes because attributes differ among values {values}&#34;)
        # --- Fallback: use expand and concat ---
        for v in values:
            if not hasattr(v, &#39;__stack__&#39;) and hasattr(v, &#39;__concat__&#39;) and hasattr(v, &#39;__expand__&#39;):
                expanded_values = tuple([expand(v, dim.with_size(1 if dim.item_names[0] is None else dim.item_names[0][i]), **kwargs) for i, v in enumerate(values)])
                if len(expanded_values) &gt; 8:
                    warnings.warn(f&#34;stack() default implementation is slow on large dimensions ({dim.name}={len(expanded_values)}). Please implement __stack__()&#34;, RuntimeWarning, stacklevel=2)
                result = v.__concat__(expanded_values, dim.name, **kwargs)
                if result is not NotImplemented:
                    assert isinstance(result, Shapable), &#34;__concat__ must return a Shapable object&#34;
                    return result
        # --- else maybe all values are native scalars ---
        from ._tensors import wrap
        try:
            values = tuple([wrap(v) for v in values])
        except ValueError:
            raise MagicNotImplemented(f&#34;At least one item in values must be Shapable but got types {[type(v) for v in values]}&#34;)
        return values[0].__stack__(values, dim, **kwargs)
    else:  # multi-dim stack
        assert dim.volume == len(values), f&#34;When passing multiple stack dims, their volume must equal len(values) but got {dim} for {len(values)} values&#34;
        if isinstance(values, dict):
            warnings.warn(f&#34;When stacking a dict along multiple dimensions, the key names are discarded. Got keys {tuple(values.keys())}&#34;, RuntimeWarning, stacklevel=2)
            values = tuple(values.values())
        # --- if any value implements Shapable, use stack and unpack_dim ---
        for v in values:
            if hasattr(v, &#39;__stack__&#39;) and hasattr(v, &#39;__unpack_dim__&#39;):
                stack_dim = batch(&#39;_stack&#39;)
                stacked = v.__stack__(values, stack_dim, **kwargs)
                if stacked is not NotImplemented:
                    assert isinstance(stacked, Shapable), &#34;__stack__ must return a Shapable object&#34;
                    assert hasattr(stacked, &#39;__unpack_dim__&#39;), &#34;If a value supports __unpack_dim__, the result of __stack__ must also support it.&#34;
                    reshaped = stacked.__unpack_dim__(stack_dim.name, dim, **kwargs)
                    if kwargs is NotImplemented:
                        warnings.warn(&#34;__unpack_dim__ is overridden but returned NotImplemented during multi-dimensional stack. This results in unnecessary stack operations.&#34;, RuntimeWarning, stacklevel=2)
                    else:
                        return reshaped
        # --- Fallback: multi-level stack ---
        for dim_ in reversed(dim):
            values = [stack(values[i:i + dim_.size], dim_, **kwargs) for i in range(0, len(values), dim_.size)]
        return values[0]</code></pre>
</details>
</dd>
<dt id="phi.math.std"><code class="name flex">
<span>def <span class="ident">std</span></span>(<span>value: phi.math._tensors.Tensor, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the standard deviation over <code>values</code> along the specified dimensions.</p>
<p><em>Warning</em>: The standard deviation of non-uniform tensors along the stack dimension is undefined.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def std(value: Tensor or list or tuple, dim: DimFilter = non_batch) -&gt; Tensor:
    &#34;&#34;&#34;
    Computes the standard deviation over `values` along the specified dimensions.

    *Warning*: The standard deviation of non-uniform tensors along the stack dimension is undefined.

    Args:
        value: `Tensor` or `list` / `tuple` of Tensors.
        dim: Dimension or dimensions to be reduced. One of

            * `None` to reduce all non-batch dimensions
            * `str` containing single dimension or comma-separated list of dimensions
            * `Tuple[str]` or `List[str]`
            * `Shape`
            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
            * `&#39;0&#39;` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors

    Returns:
        `Tensor` without the reduced dimensions.
    &#34;&#34;&#34;
    return _reduce(cached(value), dim, float,
                   native_function=lambda backend, native, dim: backend.std(native, dim),
                   collapsed_function=lambda inner, red_shape: inner,
                   unaffected_function=lambda value: value * 0)</code></pre>
</details>
</dd>
<dt id="phi.math.stop_gradient"><code class="name flex">
<span>def <span class="ident">stop_gradient</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>Disables gradients for the given tensor.
This may switch off the gradients for <code>x</code> itself or create a copy of <code>x</code> with disabled gradients.</p>
<p>Implementations:</p>
<ul>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach"><code>x.detach()</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/stop_gradient"><code>tf.stop_gradient</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.stop_gradient.html"><code>jax.lax.stop_gradient</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> for which gradients should be disabled.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stop_gradient(x):
    &#34;&#34;&#34;
    Disables gradients for the given tensor.
    This may switch off the gradients for `x` itself or create a copy of `x` with disabled gradients.

    Implementations:

    * PyTorch: [`x.detach()`](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach)
    * TensorFlow: [`tf.stop_gradient`](https://www.tensorflow.org/api_docs/python/tf/stop_gradient)
    * Jax: [`jax.lax.stop_gradient`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.stop_gradient.html)

    Args:
        x: `Tensor` or `PhiTreeNode` for which gradients should be disabled.

    Returns:
        Copy of `x`.
    &#34;&#34;&#34;
    if isinstance(x, Tensor):
        return x._op1(lambda native: choose_backend(native).stop_gradient(native))
    elif isinstance(x, PhiTreeNode):
        nest, values = disassemble_tree(x)
        new_values = [stop_gradient(v) for v in values]
        return assemble_tree(nest, new_values)
    else:
        return wrap(choose_backend(x).stop_gradient(x))</code></pre>
</details>
</dd>
<dt id="phi.math.sum"><code class="name flex">
<span>def <span class="ident">sum</span></span>(<span>value: phi.math._tensors.Tensor, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function non_batch&gt;) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Sums <code>values</code> along the specified dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>list</code> / <code>tuple</code> of Tensors.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>
<p>Dimension or dimensions to be reduced. One of</p>
<ul>
<li><code>None</code> to reduce all non-batch dimensions</li>
<li><code>str</code> containing single dimension or comma-separated list of dimensions</li>
<li><code>Tuple[str]</code> or <code>List[str]</code></li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> to select dimensions by type</li>
<li><code>'0'</code> when <code>isinstance(value, (tuple, list))</code> to add up the sequence of Tensors</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> without the reduced dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sum_(value: Tensor or list or tuple, dim: DimFilter = non_batch) -&gt; Tensor:
    &#34;&#34;&#34;
    Sums `values` along the specified dimensions.

    Args:
        value: `Tensor` or `list` / `tuple` of Tensors.
        dim: Dimension or dimensions to be reduced. One of

            * `None` to reduce all non-batch dimensions
            * `str` containing single dimension or comma-separated list of dimensions
            * `Tuple[str]` or `List[str]`
            * `Shape`
            * `batch`, `instance`, `spatial`, `channel` to select dimensions by type
            * `&#39;0&#39;` when `isinstance(value, (tuple, list))` to add up the sequence of Tensors

    Returns:
        `Tensor` without the reduced dimensions.
    &#34;&#34;&#34;
    return _reduce(value, dim, float,
                   native_function=lambda backend, native, dim: backend.sum(native, dim),
                   collapsed_function=lambda inner, red_shape: inner * red_shape.volume)</code></pre>
</details>
</dd>
<dt id="phi.math.tan"><code class="name flex">
<span>def <span class="ident">tan</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>tan(x)</em> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tan(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Computes *tan(x)* of the `Tensor` or `PhiTreeNode` `x`. &#34;&#34;&#34;
    return _backend_op1(x, Backend.tan)</code></pre>
</details>
</dd>
<dt id="phi.math.tensor"><code class="name flex">
<span>def <span class="ident">tensor</span></span>(<span>data: phi.math._tensors.Tensor, *shape: phi.math._shape.Shape, convert: bool = True, default_list_dim=(vectorᶜ=None)) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Create a Tensor from the specified <code>data</code>.
If <code>convert=True</code>, converts <code>data</code> to the preferred format of the default backend.</p>
<p><code>data</code> must be one of the following:</p>
<ul>
<li>Number: returns a dimensionless Tensor.</li>
<li>Native tensor such as NumPy array, TensorFlow tensor or PyTorch tensor.</li>
<li><code>tuple</code> or <code>list</code> of numbers: backs the Tensor with native tensor.</li>
<li><code>tuple</code> or <code>list</code> of non-numbers: creates tensors for the items and stacks them.</li>
<li>Tensor: renames dimensions and dimension types if <code>names</code> is specified. Converts all internal native values of the tensor if <code>convert=True</code>.</li>
<li>Shape: creates a 1D tensor listing the dimension sizes.</li>
</ul>
<p>While specifying <code>names</code> is optional in some cases, it is recommended to always specify them.</p>
<p>Dimension types are always inferred from the dimension names if specified.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/generated/numpy.array.html"><code>numpy.array</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/generated/torch.tensor.html"><code>torch.tensor</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.from_numpy.html"><code>torch.from_numpy</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor"><code>tf.convert_to_tensor</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html"><code>jax.numpy.array</code></a></li>
</ul>
<p>See Also:
<code><a title="phi.math.wrap" href="#phi.math.wrap">wrap()</a></code> which uses <code>convert=False</code>, <code><a title="phi.math.layout" href="#phi.math.layout">layout()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>native tensor, scalar, sequence, Shape or Tensor</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>Ordered dimensions and types. If sizes are defined, they will be checked against <code>data</code>.`</dd>
<dt><strong><code>convert</code></strong></dt>
<dd>If True, converts the data to the native format of the current default backend.
If False, wraps the data in a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> but keeps the given data reference if possible.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>AssertionError</code></dt>
<dd>if dimension names are not provided and cannot automatically be inferred</dd>
<dt><code>ValueError</code></dt>
<dd>if <code>data</code> is not tensor-like</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor containing same values as data</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">tensor([1, 2, 3], channel(vector='x,y,z'))
# Out: (x=1, y=2, z=3)

tensor([1., 2, 3], channel(vector='x,y,z'))
# Out: (x=1.000, y=2.000, z=3.000) float64

tensor(numpy.zeros([10, 8, 6, 2]), batch('batch'), spatial('x,y'), channel(vector='x,y'))
# Out: (batchᵇ=10, xˢ=8, yˢ=6, vectorᶜ=x,y) float64 const 0.0

tensor([(0, 1), (0, 2), (1, 3)], instance('particles'), channel(vector='x,y'))
# Out: (x=0, y=1); (x=0, y=2); (x=1, y=3) (particlesⁱ=3, vectorᶜ=x,y)

tensor(numpy.random.randn(10))
# Out: (vectorᶜ=10) float64 -0.128 ± 1.197 (-2e+00...2e+00)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tensor(data: Tensor or Shape or tuple or list or numbers.Number,
           *shape: Shape,
           convert: bool = True,
           default_list_dim=channel(&#39;vector&#39;)) -&gt; Tensor:  # TODO assume convert_unsupported, add convert_external=False for constants
    &#34;&#34;&#34;
    Create a Tensor from the specified `data`.
    If `convert=True`, converts `data` to the preferred format of the default backend.

    `data` must be one of the following:
    
    * Number: returns a dimensionless Tensor.
    * Native tensor such as NumPy array, TensorFlow tensor or PyTorch tensor.
    * `tuple` or `list` of numbers: backs the Tensor with native tensor.
    * `tuple` or `list` of non-numbers: creates tensors for the items and stacks them.
    * Tensor: renames dimensions and dimension types if `names` is specified. Converts all internal native values of the tensor if `convert=True`.
    * Shape: creates a 1D tensor listing the dimension sizes.
    
    While specifying `names` is optional in some cases, it is recommended to always specify them.
    
    Dimension types are always inferred from the dimension names if specified.

    Implementations:

    * NumPy: [`numpy.array`](https://numpy.org/doc/stable/reference/generated/numpy.array.html)
    * PyTorch: [`torch.tensor`](https://pytorch.org/docs/stable/generated/torch.tensor.html), [`torch.from_numpy`](https://pytorch.org/docs/stable/generated/torch.from_numpy.html)
    * TensorFlow: [`tf.convert_to_tensor`](https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor)
    * Jax: [`jax.numpy.array`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html)

    See Also:
        `phi.math.wrap()` which uses `convert=False`, `layout()`.

    Args:
        data: native tensor, scalar, sequence, Shape or Tensor
        shape: Ordered dimensions and types. If sizes are defined, they will be checked against `data`.`
        convert: If True, converts the data to the native format of the current default backend.
            If False, wraps the data in a `Tensor` but keeps the given data reference if possible.

    Raises:
        AssertionError: if dimension names are not provided and cannot automatically be inferred
        ValueError: if `data` is not tensor-like

    Returns:
        Tensor containing same values as data

    Examples:
        ```python
        tensor([1, 2, 3], channel(vector=&#39;x,y,z&#39;))
        # Out: (x=1, y=2, z=3)

        tensor([1., 2, 3], channel(vector=&#39;x,y,z&#39;))
        # Out: (x=1.000, y=2.000, z=3.000) float64

        tensor(numpy.zeros([10, 8, 6, 2]), batch(&#39;batch&#39;), spatial(&#39;x,y&#39;), channel(vector=&#39;x,y&#39;))
        # Out: (batchᵇ=10, xˢ=8, yˢ=6, vectorᶜ=x,y) float64 const 0.0

        tensor([(0, 1), (0, 2), (1, 3)], instance(&#39;particles&#39;), channel(vector=&#39;x,y&#39;))
        # Out: (x=0, y=1); (x=0, y=2); (x=1, y=3) (particlesⁱ=3, vectorᶜ=x,y)

        tensor(numpy.random.randn(10))
        # Out: (vectorᶜ=10) float64 -0.128 ± 1.197 (-2e+00...2e+00)
        ```
    &#34;&#34;&#34;
    assert all(isinstance(s, Shape) for s in shape), f&#34;Cannot create tensor because shape needs to be one or multiple Shape instances but got {shape}&#34;
    shape = None if len(shape) == 0 else concat_shapes(*shape)
    if isinstance(data, Tensor):
        if convert:
            backend = data.default_backend
            if backend != default_backend():
                data = data._op1(lambda n: convert_(n, use_dlpack=False))
        if shape is None:
            return data
        else:
            if None in shape.sizes:
                shape = shape.with_sizes(data.shape.sizes)
            return data._with_shape_replaced(shape)
    elif isinstance(data, Shape):
        if shape is None:
            shape = channel(&#39;dims&#39;)
        else:
            assert shape.rank == 1, &#34;Can only convert 1D shapes to Tensors&#34;
        shape = shape.with_size(data.names)
        data = data.sizes
    elif isinstance(data, str):
        return layout(data)
    elif isinstance(data, (numbers.Number, bool)):
        assert not shape, f&#34;Trying to create a zero-dimensional Tensor from value &#39;{data}&#39; but shape={shape}&#34;
        if convert:
            data = default_backend().as_tensor(data, convert_external=True)
        return NativeTensor(data, EMPTY_SHAPE)
    if isinstance(data, (tuple, list)):
        if all(isinstance(d, (bool, int, float, complex)) for d in data):
            array = np.array(data)
            assert array.dtype != object
            data = array
        elif all(isinstance(d, str) for d in data):
            if shape:
                return layout(data, shape)
            else:
                return layout(data, channel(&#39;vector&#39;))
        else:
            inner_shape = [] if shape is None else [shape[1:]]
            tensors = [d if isinstance(d, Tensor) else tensor(d, *inner_shape, convert=convert) for d in data]
            common_shape = merge_shapes(*[e.shape for e in tensors])
            stack_dim = default_list_dim if shape is None else shape[0].with_sizes([len(tensors)])
            assert all(stack_dim not in t.shape for t in tensors), f&#34;Cannot stack tensors with dimension &#39;{stack_dim}&#39; because a tensor already has that dimension.&#34;
            elements = [CollapsedTensor(e, common_shape) if e.shape.rank &lt; common_shape.rank else e for e in tensors]
            from ._ops import cast_same
            elements = cast_same(*elements)
            return TensorStack(elements, stack_dim)
    try:
        backend = choose_backend(data)
        if shape is None:
            assert backend.ndims(data) &lt;= 1, &#34;Specify dimension names for tensors with more than 1 dimension&#34;
            shape = default_list_dim if backend.ndims(data) == 1 else EMPTY_SHAPE
            shape = shape.with_sizes(backend.staticshape(data))
        else:
            # fill in sizes or check them
            sizes = backend.staticshape(data)
            if len(sizes) != len(shape):
                raise IncompatibleShapes(f&#34;Rank of given shape {shape} does not match data with sizes {sizes}&#34;)
            for size, s in zip(sizes, shape.sizes):
                if s is not None:
                    assert s == size, f&#34;Given shape {shape} does not match data with sizes {sizes}. Consider leaving the sizes undefined.&#34;
            shape = shape.with_sizes(sizes, keep_item_names=True)
        if convert:
            data = convert_(data, use_dlpack=False)
        return NativeTensor(data, shape)
    except NoBackendFound:
        raise ValueError(f&#34;{type(data)} is not supported. Only (Tensor, tuple, list, np.ndarray, native tensors) are allowed.\nCurrent backends: {BACKENDS}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.to_complex"><code class="name flex">
<span>def <span class="ident">to_complex</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Converts the given tensor to complex floating point format with the currently specified precision.</p>
<p>The precision can be set globally using <code>math.set_global_precision()</code> and locally using <code>with math.precision()</code>.</p>
<p>See the <code><a title="phi.math" href="#phi.math">phi.math</a></code> module documentation at <a href="https://tum-pbs.github.io/PhiFlow/Math.html">https://tum-pbs.github.io/PhiFlow/Math.html</a></p>
<p>See Also:
<code><a title="phi.math.cast" href="#phi.math.cast">cast()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>values to convert</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of same shape as <code>x</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_complex(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34;
    Converts the given tensor to complex floating point format with the currently specified precision.

    The precision can be set globally using `math.set_global_precision()` and locally using `with math.precision()`.

    See the `phi.math` module documentation at https://tum-pbs.github.io/PhiFlow/Math.html

    See Also:
        `cast()`.

    Args:
        x: values to convert

    Returns:
        `Tensor` of same shape as `x`
    &#34;&#34;&#34;
    return _backend_op1(x, Backend.to_complex)</code></pre>
</details>
</dd>
<dt id="phi.math.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>value: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a serializable form of a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.
The result can be written to a JSON file, for example.</p>
<p>See Also:
<code><a title="phi.math.from_dict" href="#phi.math.from_dict">from_dict()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Serializable Python tree of primitives</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(value: Tensor or Shape):
    &#34;&#34;&#34;
    Returns a serializable form of a `Tensor` or `Shape`.
    The result can be written to a JSON file, for example.

    See Also:
        `from_dict()`.

    Args:
        value: `Tensor` or `Shape`

    Returns:
        Serializable Python tree of primitives
    &#34;&#34;&#34;
    if isinstance(value, Shape):
        return value._to_dict(include_sizes=True)
    elif isinstance(value, Tensor):
        return value._to_dict()
    raise ValueError(f&#34;Cannot convert {value} to a dict&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.to_float"><code class="name flex">
<span>def <span class="ident">to_float</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Converts the given tensor to floating point format with the currently specified precision.</p>
<p>The precision can be set globally using <code>math.set_global_precision()</code> and locally using <code>with math.precision()</code>.</p>
<p>See the <code><a title="phi.math" href="#phi.math">phi.math</a></code> module documentation at <a href="https://tum-pbs.github.io/PhiFlow/Math.html">https://tum-pbs.github.io/PhiFlow/Math.html</a></p>
<p>See Also:
<code><a title="phi.math.cast" href="#phi.math.cast">cast()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> to convert</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> matching <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_float(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34;
    Converts the given tensor to floating point format with the currently specified precision.
    
    The precision can be set globally using `math.set_global_precision()` and locally using `with math.precision()`.
    
    See the `phi.math` module documentation at https://tum-pbs.github.io/PhiFlow/Math.html

    See Also:
        `cast()`.

    Args:
        x: `Tensor` or `PhiTreeNode` to convert

    Returns:
        `Tensor` or `PhiTreeNode` matching `x`.
    &#34;&#34;&#34;
    return _backend_op1(x, Backend.to_float)</code></pre>
</details>
</dd>
<dt id="phi.math.to_int32"><code class="name flex">
<span>def <span class="ident">to_int32</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Converts the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code> to 32-bit integer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_int32(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Converts the `Tensor` or `PhiTreeNode` `x` to 32-bit integer. &#34;&#34;&#34;
    return _backend_op1(x, Backend.to_int32)</code></pre>
</details>
</dd>
<dt id="phi.math.to_int64"><code class="name flex">
<span>def <span class="ident">to_int64</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Converts the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> <code>x</code> to 64-bit integer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_int64(x) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Converts the `Tensor` or `PhiTreeNode` `x` to 64-bit integer. &#34;&#34;&#34;
    return _backend_op1(x, Backend.to_int64)</code></pre>
</details>
</dd>
<dt id="phi.math.transpose"><code class="name flex">
<span>def <span class="ident">transpose</span></span>(<span>x: phi.math._tensors.Tensor, axes)</span>
</code></dt>
<dd>
<div class="desc"><p>Swap the dimension order of <code>x</code>.
This operation is superfluous since tensors will be reshaped under the hood or when getting the native/numpy representations.</p>
<p>Implementations:</p>
<ul>
<li>NumPy: <a href="https://numpy.org/doc/stable/reference/generated/numpy.transpose.html"><code>numpy.transpose</code></a></li>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute"><code>x.permute</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/transpose"><code>tf.transpose</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.transpose.html"><code>jax.numpy.transpose</code></a></li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or native tensor.</dd>
<dt><strong><code>axes</code></strong></dt>
<dd><code>tuple</code> or <code>list</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or native tensor, depending on <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transpose(x: Tensor, axes):
    &#34;&#34;&#34;
    Swap the dimension order of `x`.
    This operation is superfluous since tensors will be reshaped under the hood or when getting the native/numpy representations.

    Implementations:

    * NumPy: [`numpy.transpose`](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)
    * PyTorch: [`x.permute`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute)
    * TensorFlow: [`tf.transpose`](https://www.tensorflow.org/api_docs/python/tf/transpose)
    * Jax: [`jax.numpy.transpose`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.transpose.html)

    Args:
        x: `Tensor` or native tensor.
        axes: `tuple` or `list`

    Returns:
        `Tensor` or native tensor, depending on `x`.
    &#34;&#34;&#34;
    if isinstance(x, Tensor):
        return CollapsedTensor(x, x.shape[axes])  # TODO avoid nesting
    else:
        return choose_backend(x).transpose(x, axes)</code></pre>
</details>
</dd>
<dt id="phi.math.unpack_dim"><code class="name flex">
<span>def <span class="ident">unpack_dim</span></span>(<span>value, dim: str, unpacked_dims: phi.math._shape.Shape, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Decompresses a dimension by unstacking the elements along it.
This function replaces the traditional <code>reshape</code> for these cases.
The compressed dimension <code>dim</code> is assumed to contain elements laid out according to the order of <code>unpacked_dims</code>.</p>
<p>See Also:
<code><a title="phi.math.pack_dims" href="#phi.math.pack_dims">pack_dims()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.magic.Shapable" href="magic.html#phi.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, for which one dimension should be split.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension to be decompressed.</dd>
<dt><strong><code>unpacked_dims</code></strong></dt>
<dd><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>: Ordered dimensions to replace <code>dim</code>, fulfilling <code>unpacked_dims.volume == shape(self)[dim].rank</code>.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as <code>value</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">unpack_dim(math.zeros(instance(points=12)), 'points', spatial(x=4, y=3))
# Out: (xˢ=4, yˢ=3) const 0.0
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unpack_dim(value, dim: str or Shape, unpacked_dims: Shape, **kwargs):
    &#34;&#34;&#34;
    Decompresses a dimension by unstacking the elements along it.
    This function replaces the traditional `reshape` for these cases.
    The compressed dimension `dim` is assumed to contain elements laid out according to the order of `unpacked_dims`.

    See Also:
        `pack_dims()`

    Args:
        value: `phi.math.magic.Shapable`, such as `Tensor`, for which one dimension should be split.
        dim: Dimension to be decompressed.
        unpacked_dims: `Shape`: Ordered dimensions to replace `dim`, fulfilling `unpacked_dims.volume == shape(self)[dim].rank`.
        **kwargs: Additional keyword arguments required by specific implementations.
            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
            Adding batch dimensions must always work without keyword arguments.

    Returns:
        Same type as `value`.

    Examples:
        ```python
        unpack_dim(math.zeros(instance(points=12)), &#39;points&#39;, spatial(x=4, y=3))
        # Out: (xˢ=4, yˢ=3) const 0.0
        ```
    &#34;&#34;&#34;
    assert isinstance(value, Shapable) and isinstance(value, Sliceable) and isinstance(value, Shaped), f&#34;value must be Shapable but got {type(value)}&#34;
    if isinstance(dim, Shape):
        dim = dim.name
    assert isinstance(dim, str), f&#34;dim must be a str or Shape but got {type(dim)}&#34;
    if dim not in shape(value):
        return value  # Nothing to do, maybe expand?
    if unpacked_dims.rank == 0:
        return value[{dim: 0}]  # remove dim
    elif unpacked_dims.rank == 1:
        return rename_dims(value, dim, unpacked_dims, **kwargs)
    # --- First try __unpack_dim__
    if hasattr(value, &#39;__unpack_dim__&#39;):
        result = value.__unpack_dim__(dim, unpacked_dims, **kwargs)
        if result is not NotImplemented:
            return result
    # --- Next try Tree Node ---
    if isinstance(value, PhiTreeNode):
        new_attributes = {a: unpack_dim(getattr(value, a), dim, unpacked_dims, **kwargs) for a in value_attributes(value)}
        return copy_with(value, **new_attributes)
    # --- Fallback: unstack and stack ---
    if shape(value).only(dim).volume &gt; 8:
        warnings.warn(f&#34;pack_dims() default implementation is slow on large dimensions ({shape(value).only(dim)}). Please implement __unpack_dim__() for {type(value).__name__} as defined in phi.math.magic&#34;, RuntimeWarning, stacklevel=2)
    unstacked = unstack(value, dim)
    for dim in reversed(unpacked_dims):
        unstacked = [stack(unstacked[i:i+dim.size], dim, **kwargs) for i in range(0, len(unstacked), dim.size)]
    return unstacked[0]</code></pre>
</details>
</dd>
<dt id="phi.math.unpack_dims"><code class="name flex">
<span>def <span class="ident">unpack_dims</span></span>(<span>value, dim: str, unpacked_dims: phi.math._shape.Shape, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Decompresses a dimension by unstacking the elements along it.
This function replaces the traditional <code>reshape</code> for these cases.
The compressed dimension <code>dim</code> is assumed to contain elements laid out according to the order of <code>unpacked_dims</code>.</p>
<p>See Also:
<code><a title="phi.math.pack_dims" href="#phi.math.pack_dims">pack_dims()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.magic.Shapable" href="magic.html#phi.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, for which one dimension should be split.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension to be decompressed.</dd>
<dt><strong><code>unpacked_dims</code></strong></dt>
<dd><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>: Ordered dimensions to replace <code>dim</code>, fulfilling <code>unpacked_dims.volume == shape(self)[dim].rank</code>.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional keyword arguments required by specific implementations.
Adding spatial dimensions to fields requires the <code>bounds: Box</code> argument specifying the physical extent of the new dimensions.
Adding batch dimensions must always work without keyword arguments.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Same type as <code>value</code>.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">unpack_dim(math.zeros(instance(points=12)), 'points', spatial(x=4, y=3))
# Out: (xˢ=4, yˢ=3) const 0.0
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unpack_dim(value, dim: str or Shape, unpacked_dims: Shape, **kwargs):
    &#34;&#34;&#34;
    Decompresses a dimension by unstacking the elements along it.
    This function replaces the traditional `reshape` for these cases.
    The compressed dimension `dim` is assumed to contain elements laid out according to the order of `unpacked_dims`.

    See Also:
        `pack_dims()`

    Args:
        value: `phi.math.magic.Shapable`, such as `Tensor`, for which one dimension should be split.
        dim: Dimension to be decompressed.
        unpacked_dims: `Shape`: Ordered dimensions to replace `dim`, fulfilling `unpacked_dims.volume == shape(self)[dim].rank`.
        **kwargs: Additional keyword arguments required by specific implementations.
            Adding spatial dimensions to fields requires the `bounds: Box` argument specifying the physical extent of the new dimensions.
            Adding batch dimensions must always work without keyword arguments.

    Returns:
        Same type as `value`.

    Examples:
        ```python
        unpack_dim(math.zeros(instance(points=12)), &#39;points&#39;, spatial(x=4, y=3))
        # Out: (xˢ=4, yˢ=3) const 0.0
        ```
    &#34;&#34;&#34;
    assert isinstance(value, Shapable) and isinstance(value, Sliceable) and isinstance(value, Shaped), f&#34;value must be Shapable but got {type(value)}&#34;
    if isinstance(dim, Shape):
        dim = dim.name
    assert isinstance(dim, str), f&#34;dim must be a str or Shape but got {type(dim)}&#34;
    if dim not in shape(value):
        return value  # Nothing to do, maybe expand?
    if unpacked_dims.rank == 0:
        return value[{dim: 0}]  # remove dim
    elif unpacked_dims.rank == 1:
        return rename_dims(value, dim, unpacked_dims, **kwargs)
    # --- First try __unpack_dim__
    if hasattr(value, &#39;__unpack_dim__&#39;):
        result = value.__unpack_dim__(dim, unpacked_dims, **kwargs)
        if result is not NotImplemented:
            return result
    # --- Next try Tree Node ---
    if isinstance(value, PhiTreeNode):
        new_attributes = {a: unpack_dim(getattr(value, a), dim, unpacked_dims, **kwargs) for a in value_attributes(value)}
        return copy_with(value, **new_attributes)
    # --- Fallback: unstack and stack ---
    if shape(value).only(dim).volume &gt; 8:
        warnings.warn(f&#34;pack_dims() default implementation is slow on large dimensions ({shape(value).only(dim)}). Please implement __unpack_dim__() for {type(value).__name__} as defined in phi.math.magic&#34;, RuntimeWarning, stacklevel=2)
    unstacked = unstack(value, dim)
    for dim in reversed(unpacked_dims):
        unstacked = [stack(unstacked[i:i+dim.size], dim, **kwargs) for i in range(0, len(unstacked), dim.size)]
    return unstacked[0]</code></pre>
</details>
</dd>
<dt id="phi.math.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>value, dim: Union[str, tuple, list, phi.math._shape.Shape, Callable])</span>
</code></dt>
<dd>
<div class="desc"><p>Un-stacks a <code>Sliceable</code> along one or multiple dimensions.</p>
<p>If multiple dimensions are given, the order of elements will be according to the dimension order in <code>dim</code>, i.e. elements along the last dimension will be neighbors in the returned <code>tuple</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.magic.Shapable" href="magic.html#phi.math.magic.Shapable">Shapable</a></code>, such as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimensions as <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or comma-separated <code>str</code> or dimension type, i.e. <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>tuple</code> of <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> objects.</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">unstack(math.zeros(spatial(x=5)), 'x')
# Out: (0.0, 0.0, 0.0, 0.0, 0.0)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack(value, dim: DimFilter):
    &#34;&#34;&#34;
    Un-stacks a `Sliceable` along one or multiple dimensions.

    If multiple dimensions are given, the order of elements will be according to the dimension order in `dim`, i.e. elements along the last dimension will be neighbors in the returned `tuple`.

    Args:
        value: `phi.math.magic.Shapable`, such as `phi.math.Tensor`
        dim: Dimensions as `Shape` or comma-separated `str` or dimension type, i.e. `channel`, `spatial`, `instance`, `batch`.

    Returns:
        `tuple` of `Tensor` objects.

    Examples:
        ```python
        unstack(math.zeros(spatial(x=5)), &#39;x&#39;)
        # Out: (0.0, 0.0, 0.0, 0.0, 0.0)
        ```
    &#34;&#34;&#34;
    assert isinstance(value, Sliceable) and isinstance(value, Shaped), f&#34;Cannot unstack {type(value).__name__}. Must be Sliceable and Shaped, see https://tum-pbs.github.io/PhiFlow/phi/math/magic.html&#34;
    dims = shape(value).only(dim)
    assert dims.rank &gt; 0, &#34;unstack() requires at least one dimension&#34;
    if dims.rank == 1:
        if hasattr(value, &#39;__unstack__&#39;):
            result = value.__unstack__(dims.names)
            if result is not NotImplemented:
                assert isinstance(result, tuple), f&#34;__unstack__ must return a tuple but got {type(result)}&#34;
                assert all([isinstance(item, Sliceable) for item in result]), f&#34;__unstack__ must return a tuple of Sliceable objects but not all items were sliceable in {result}&#34;
                return result
        return tuple([value[{dims.name: i}] for i in range(dims.size)])
    else:  # multiple dimensions
        if hasattr(value, &#39;__pack_dims__&#39;):
            packed_dim = batch(&#39;_unstack&#39;)
            value_packed = value.__pack_dims__(dims.names, packed_dim, pos=None)
            if value_packed is not NotImplemented:
                return unstack(value_packed, packed_dim)
        first_unstacked = unstack(value, dims[0])
        inner_unstacked = [unstack(v, dims.without(dims[0])) for v in first_unstacked]
        return sum(inner_unstacked, ())</code></pre>
</details>
</dd>
<dt id="phi.math.upsample2x"><code class="name flex">
<span>def <span class="ident">upsample2x</span></span>(<span>grid: phi.math._tensors.Tensor, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function spatial&gt;) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Resamples a regular grid to double the number of spatial sample points per dimension.
The grid values at the new points are determined via linear interpolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>half-size grid</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>grid extrapolation</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>dims along which up-sampling is applied. If None, up-sample along all spatial dims.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>double-size grid</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upsample2x(grid: Tensor,
               padding: Extrapolation = extrapolation.BOUNDARY,
               dims: DimFilter = spatial) -&gt; Tensor:
    &#34;&#34;&#34;
    Resamples a regular grid to double the number of spatial sample points per dimension.
    The grid values at the new points are determined via linear interpolation.

    Args:
      grid: half-size grid
      padding: grid extrapolation
      dims: dims along which up-sampling is applied. If None, up-sample along all spatial dims.
      grid: Tensor: 
      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
      dims: tuple or None:  (Default value = None)

    Returns:
      double-size grid

    &#34;&#34;&#34;
    for dim in grid.shape.only(dims):
        left, center, right = shift(grid, (-1, 0, 1), dim.names, padding, None)
        interp_left = 0.25 * left + 0.75 * center
        interp_right = 0.75 * center + 0.25 * right
        stacked = math.stack_tensors([interp_left, interp_right], channel(_interleave=&#39;left,right&#39;))
        grid = math.pack_dims(stacked, (dim.name, &#39;_interleave&#39;), dim)
    return grid</code></pre>
</details>
</dd>
<dt id="phi.math.vec"><code class="name flex">
<span>def <span class="ident">vec</span></span>(<span>name='vector', **components) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Lay out the given values along a channel dimension without converting them to the current backend.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>**components</code></strong></dt>
<dd>Values by component name.</dd>
<dt><strong><code>name</code></strong></dt>
<dd>Dimension name.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">vec(x=1, y=0, z=-1)
# Out: (x=1, y=0, z=-1)

vec(x=1., z=0)
# Out: (x=1.000, z=0.000)

vec(x=tensor([1, 2, 3], instance('particles')), y=0)
# Out: (x=1, y=0); (x=2, y=0); (x=3, y=0) (particlesⁱ=3, vectorᶜ=x,y)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vec(name=&#39;vector&#39;, **components) -&gt; Tensor:
    &#34;&#34;&#34;
    Lay out the given values along a channel dimension without converting them to the current backend.

    Args:
        **components: Values by component name.
        name: Dimension name.

    Returns:
        `Tensor`

    Examples:
        ```python
        vec(x=1, y=0, z=-1)
        # Out: (x=1, y=0, z=-1)

        vec(x=1., z=0)
        # Out: (x=1.000, z=0.000)

        vec(x=tensor([1, 2, 3], instance(&#39;particles&#39;)), y=0)
        # Out: (x=1, y=0); (x=2, y=0); (x=3, y=0) (particlesⁱ=3, vectorᶜ=x,y)
        ```
    &#34;&#34;&#34;
    return stack(components, channel(name), expand_values=True)</code></pre>
</details>
</dd>
<dt id="phi.math.vec_abs"><code class="name flex">
<span>def <span class="ident">vec_abs</span></span>(<span>vec: phi.math._tensors.Tensor, vec_dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function channel&gt;, eps: float = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the vector length of <code><a title="phi.math.vec" href="#phi.math.vec">vec()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>eps</code></strong></dt>
<dd>Minimum vector length. Use to avoid <code>inf</code> gradients for zero-length vectors.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vec_abs(vec: Tensor, vec_dim: DimFilter = channel, eps: float or Tensor = None):
    &#34;&#34;&#34;
    Computes the vector length of `vec`.

    Args:
        eps: Minimum vector length. Use to avoid `inf` gradients for zero-length vectors.
    &#34;&#34;&#34;
    squared = vec_squared(vec, vec_dim)
    if eps is not None:
        squared = math.maximum(squared, eps)
    return math.sqrt(squared)</code></pre>
</details>
</dd>
<dt id="phi.math.vec_length"><code class="name flex">
<span>def <span class="ident">vec_length</span></span>(<span>vec: phi.math._tensors.Tensor, vec_dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function channel&gt;, eps: float = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the vector length of <code><a title="phi.math.vec" href="#phi.math.vec">vec()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>eps</code></strong></dt>
<dd>Minimum vector length. Use to avoid <code>inf</code> gradients for zero-length vectors.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vec_abs(vec: Tensor, vec_dim: DimFilter = channel, eps: float or Tensor = None):
    &#34;&#34;&#34;
    Computes the vector length of `vec`.

    Args:
        eps: Minimum vector length. Use to avoid `inf` gradients for zero-length vectors.
    &#34;&#34;&#34;
    squared = vec_squared(vec, vec_dim)
    if eps is not None:
        squared = math.maximum(squared, eps)
    return math.sqrt(squared)</code></pre>
</details>
</dd>
<dt id="phi.math.vec_normalize"><code class="name flex">
<span>def <span class="ident">vec_normalize</span></span>(<span>vec: phi.math._tensors.Tensor, vec_dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function channel&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Normalizes the vectors in <code><a title="phi.math.vec" href="#phi.math.vec">vec()</a></code>. If <code>vec_dim</code> is None, the combined channel dimensions of <code><a title="phi.math.vec" href="#phi.math.vec">vec()</a></code> are interpreted as a vector.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vec_normalize(vec: Tensor, vec_dim: DimFilter = channel):
    &#34;&#34;&#34; Normalizes the vectors in `vec`. If `vec_dim` is None, the combined channel dimensions of `vec` are interpreted as a vector. &#34;&#34;&#34;
    return vec / vec_abs(vec, vec_dim=vec_dim)</code></pre>
</details>
</dd>
<dt id="phi.math.vec_squared"><code class="name flex">
<span>def <span class="ident">vec_squared</span></span>(<span>vec: phi.math._tensors.Tensor, vec_dim: Union[str, tuple, list, phi.math._shape.Shape, Callable] = &lt;function channel&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the squared length of <code><a title="phi.math.vec" href="#phi.math.vec">vec()</a></code>. If <code>vec_dim</code> is None, the combined channel dimensions of <code><a title="phi.math.vec" href="#phi.math.vec">vec()</a></code> are interpreted as a vector.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vec_squared(vec: Tensor, vec_dim: DimFilter = channel):
    &#34;&#34;&#34; Computes the squared length of `vec`. If `vec_dim` is None, the combined channel dimensions of `vec` are interpreted as a vector. &#34;&#34;&#34;
    return math.sum_(vec ** 2, dim=vec_dim)</code></pre>
</details>
</dd>
<dt id="phi.math.where"><code class="name flex">
<span>def <span class="ident">where</span></span>(<span>condition: phi.math._tensors.Tensor, value_true: phi.math._tensors.Tensor, value_false: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a tensor by choosing either values from <code>value_true</code> or <code>value_false</code> depending on <code>condition</code>.
If <code>condition</code> is not of type boolean, non-zero values are interpreted as True.</p>
<p>This function requires non-None values for <code>value_true</code> and <code>value_false</code>.
To get the indices of True / non-zero values, use :func:<code><a title="phi.math.nonzero" href="#phi.math.nonzero">nonzero()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>condition</code></strong></dt>
<dd>determines where to choose values from value_true or from value_false</dd>
<dt><strong><code>value_true</code></strong></dt>
<dd>Values to pick where <code>condition != 0 / True</code></dd>
<dt><strong><code>value_false</code></strong></dt>
<dd>Values to pick where <code>condition == 0 / False</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> containing dimensions of all inputs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def where(condition: Tensor or float or int, value_true: Tensor or float or int, value_false: Tensor or float or int):
    &#34;&#34;&#34;
    Builds a tensor by choosing either values from `value_true` or `value_false` depending on `condition`.
    If `condition` is not of type boolean, non-zero values are interpreted as True.
    
    This function requires non-None values for `value_true` and `value_false`.
    To get the indices of True / non-zero values, use :func:`nonzero`.

    Args:
      condition: determines where to choose values from value_true or from value_false
      value_true: Values to pick where `condition != 0 / True`
      value_false: Values to pick where `condition == 0 / False`

    Returns:
        `Tensor` containing dimensions of all inputs.
    &#34;&#34;&#34;
    condition = wrap(condition)
    value_true = wrap(value_true)
    value_false = wrap(value_false)

    def inner_where(c: Tensor, vt: Tensor, vf: Tensor):
        if vt._is_tracer or vf._is_tracer or c._is_tracer:
            return c * vt + (1 - c) * vf  # ToDo this does not take NaN into account
        shape, (c, vt, vf) = broadcastable_native_tensors(c, vt, vf)
        result = choose_backend(c, vt, vf).where(c, vt, vf)
        return NativeTensor(result, shape)

    return broadcast_op(inner_where, [condition, value_true, value_false])</code></pre>
</details>
</dd>
<dt id="phi.math.wrap"><code class="name flex">
<span>def <span class="ident">wrap</span></span>(<span>data: phi.math._tensors.Tensor, *shape: phi.math._shape.Shape) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Short for <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code> with <code>convert=False</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wrap(data: Tensor or Shape or tuple or list or numbers.Number,
         *shape: Shape) -&gt; Tensor:
    &#34;&#34;&#34; Short for `phi.math.tensor()` with `convert=False`. &#34;&#34;&#34;
    return tensor(data, *shape, convert=False)  # TODO inline, simplify</code></pre>
</details>
</dd>
<dt id="phi.math.zeros"><code class="name flex">
<span>def <span class="ident">zeros</span></span>(<span>*shape: phi.math._shape.Shape, dtype=None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Define a tensor with specified shape with value <code>0.0</code> / <code>0</code> / <code>False</code> everywhere.</p>
<p>This method may not immediately allocate the memory to store the values.</p>
<p>See Also:
<code><a title="phi.math.zeros_like" href="#phi.math.zeros_like">zeros_like()</a></code>, <code><a title="phi.math.ones" href="#phi.math.ones">ones()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*shape</code></strong></dt>
<dd>This (possibly empty) sequence of <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>s is concatenated, preserving the order.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Data type as <code><a title="phi.math.DType" href="#phi.math.DType">DType</a></code> object. Defaults to <code>float</code> matching the current precision setting.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zeros(*shape: Shape, dtype=None) -&gt; Tensor:
    &#34;&#34;&#34;
    Define a tensor with specified shape with value `0.0` / `0` / `False` everywhere.
    
    This method may not immediately allocate the memory to store the values.

    See Also:
        `zeros_like()`, `ones()`.

    Args:
        *shape: This (possibly empty) sequence of `Shape`s is concatenated, preserving the order.
        dtype: Data type as `DType` object. Defaults to `float` matching the current precision setting.

    Returns:
        `Tensor`
    &#34;&#34;&#34;
    return _initialize(lambda shape: CollapsedTensor(NativeTensor(default_backend().zeros((), dtype=DType.as_dtype(dtype)), EMPTY_SHAPE), shape), shape)</code></pre>
</details>
</dd>
<dt id="phi.math.zeros_like"><code class="name flex">
<span>def <span class="ident">zeros_like</span></span>(<span>obj: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Create a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> containing only <code>0.0</code> / <code>0</code> / <code>False</code> with the same shape and dtype as <code>obj</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zeros_like(obj: Tensor or PhiTreeNode) -&gt; Tensor or PhiTreeNode:
    &#34;&#34;&#34; Create a `Tensor` containing only `0.0` / `0` / `False` with the same shape and dtype as `obj`. &#34;&#34;&#34;
    nest, values = disassemble_tree(obj)
    zeros_ = []
    for val in values:
        val = wrap(val)
        with val.default_backend:
            zeros_.append(zeros(val.shape, dtype=val.dtype))
    return assemble_tree(nest, zeros_)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="phi.math.ConvergenceException"><code class="flex name class">
<span>class <span class="ident">ConvergenceException</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for exceptions raised when a solve does not converge.</p>
<p>See Also:
<code><a title="phi.math.Diverged" href="#phi.math.Diverged">Diverged</a></code>, <code><a title="phi.math.NotConverged" href="#phi.math.NotConverged">NotConverged</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConvergenceException(RuntimeError):
    &#34;&#34;&#34;
    Base class for exceptions raised when a solve does not converge.

    See Also:
        `Diverged`, `NotConverged`.
    &#34;&#34;&#34;

    def __init__(self, result: SolveInfo):
        RuntimeError.__init__(self, result.msg)
        self.result: SolveInfo = result
        &#34;&#34;&#34; `SolveInfo` holding information about the solve. &#34;&#34;&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.RuntimeError</li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li>phi.math._functional.Diverged</li>
<li>phi.math._functional.NotConverged</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.ConvergenceException.result"><code class="name">var <span class="ident">result</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.SolveInfo" href="#phi.math.SolveInfo">SolveInfo</a></code> holding information about the solve.</p></div>
</dd>
</dl>
</dd>
<dt id="phi.math.DType"><code class="flex name class">
<span>class <span class="ident">DType</span></span>
<span>(</span><span>kind: type, bits: int = None, precision: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Instances of <code><a title="phi.math.DType" href="#phi.math.DType">DType</a></code> represent the kind and size of data elements.
The data type of a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> can be obtained via <code><a title="phi.math.Tensor.dtype" href="#phi.math.Tensor.dtype">Tensor.dtype</a></code>.</p>
<p>The following kinds of data types are supported:</p>
<ul>
<li><code>float</code> with 32 / 64 bits</li>
<li><code>complex</code> with 64 / 128 bits</li>
<li><code>int</code> with 8 / 16 / 32 / 64 bits</li>
<li><code>bool</code> with 8 bits</li>
<li><code>str</code> with 8<em>n</em> bits</li>
</ul>
<p>Unlike with many computing libraries, there are no global variables corresponding to the available types.
Instead, data types can simply be instantiated as needed.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kind</code></strong></dt>
<dd>Python type, one of <code>(bool, int, float, complex, str)</code></dd>
<dt><strong><code>bits</code></strong></dt>
<dd>number of bits per element, a multiple of 8.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DType:
    &#34;&#34;&#34;
    Instances of `DType` represent the kind and size of data elements.
    The data type of a `Tensor` can be obtained via `phi.math.Tensor.dtype`.

    The following kinds of data types are supported:

    * `float` with 32 / 64 bits
    * `complex` with 64 / 128 bits
    * `int` with 8 / 16 / 32 / 64 bits
    * `bool` with 8 bits
    * `str` with 8*n* bits

    Unlike with many computing libraries, there are no global variables corresponding to the available types.
    Instead, data types can simply be instantiated as needed.
    &#34;&#34;&#34;

    def __init__(self, kind: type, bits: int = None, precision: int = None):
        &#34;&#34;&#34;
        Args:
            kind: Python type, one of `(bool, int, float, complex, str)`
            bits: number of bits per element, a multiple of 8.
        &#34;&#34;&#34;
        assert kind in (bool, int, float, complex, str, object)
        if kind is bool:
            assert bits is None, &#34;Bits may not be set for bool or object&#34;
            assert precision is None, f&#34;Precision may only be specified for float or complex but got {kind}, precision={precision}&#34;
            bits = 8
        elif kind == object:
            assert bits is None, &#34;bits may not be set for bool or object&#34;
            assert precision is None, f&#34;Precision may only be specified for float or complex but got {kind}, precision={precision}&#34;
            bits = int(np.round(np.log2(sys.maxsize))) + 1
        elif precision is not None:
            assert bits is None, &#34;Specify either bits or precision when creating a DType but not both.&#34;
            assert kind in [float, complex], f&#34;Precision may only be specified for float or complex but got {kind}, precision={precision}&#34;
            if kind == float:
                bits = precision
            else:
                bits = precision * 2
        else:
            assert isinstance(bits, int)
        self.kind = kind
        &#34;&#34;&#34; Python class corresponding to the type of data, ignoring precision. One of (bool, int, float, complex, str) &#34;&#34;&#34;
        self.bits = bits
        &#34;&#34;&#34; Number of bits used to store a single value of this type. See `DType.itemsize`. &#34;&#34;&#34;

    @property
    def precision(self):
        &#34;&#34;&#34; Floating point precision. Only defined if `kind in (float, complex)`. For complex values, returns half of `DType.bits`. &#34;&#34;&#34;
        if self.kind == float:
            return self.bits
        if self.kind == complex:
            return self.bits // 2
        else:
            return None

    @property
    def itemsize(self):
        &#34;&#34;&#34; Number of bytes used to storea single value of this type. See `DType.bits`. &#34;&#34;&#34;
        assert self.bits % 8 == 0
        return self.bits // 8

    def __eq__(self, other):
        return isinstance(other, DType) and self.kind == other.kind and self.bits == other.bits

    def __ne__(self, other):
        return not self == other

    def __hash__(self):
        return hash(self.kind) + hash(self.bits)

    def __repr__(self):
        return f&#34;{self.kind.__name__}{self.bits}&#34;

    @staticmethod
    def as_dtype(value: &#39;DType&#39; or tuple or type or None) -&gt; &#39;DType&#39; or None:
        if isinstance(value, DType):
            return value
        elif value is int:
            return DType(int, 32)
        elif value is float:
            from phi.math import get_precision
            return DType(float, get_precision())
        elif value is complex:
            from phi.math import get_precision
            return DType(complex, 2 * get_precision())
        elif value is None:
            return None
        elif isinstance(value, tuple):
            return DType(*value)
        elif value is str:
            raise ValueError(&#34;str DTypes must specify precision&#34;)
        else:
            return DType(value)  # bool, object</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="phi.math.DType.as_dtype"><code class="name flex">
<span>def <span class="ident">as_dtype</span></span>(<span>value: <a title="phi.math.DType" href="#phi.math.DType">DType</a>) ‑> phi.math.backend._dtype.DType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def as_dtype(value: &#39;DType&#39; or tuple or type or None) -&gt; &#39;DType&#39; or None:
    if isinstance(value, DType):
        return value
    elif value is int:
        return DType(int, 32)
    elif value is float:
        from phi.math import get_precision
        return DType(float, get_precision())
    elif value is complex:
        from phi.math import get_precision
        return DType(complex, 2 * get_precision())
    elif value is None:
        return None
    elif isinstance(value, tuple):
        return DType(*value)
    elif value is str:
        raise ValueError(&#34;str DTypes must specify precision&#34;)
    else:
        return DType(value)  # bool, object</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.DType.bits"><code class="name">var <span class="ident">bits</span></code></dt>
<dd>
<div class="desc"><p>Number of bits used to store a single value of this type. See <code><a title="phi.math.DType.itemsize" href="#phi.math.DType.itemsize">DType.itemsize</a></code>.</p></div>
</dd>
<dt id="phi.math.DType.itemsize"><code class="name">var <span class="ident">itemsize</span></code></dt>
<dd>
<div class="desc"><p>Number of bytes used to storea single value of this type. See <code><a title="phi.math.DType.bits" href="#phi.math.DType.bits">DType.bits</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def itemsize(self):
    &#34;&#34;&#34; Number of bytes used to storea single value of this type. See `DType.bits`. &#34;&#34;&#34;
    assert self.bits % 8 == 0
    return self.bits // 8</code></pre>
</details>
</dd>
<dt id="phi.math.DType.kind"><code class="name">var <span class="ident">kind</span></code></dt>
<dd>
<div class="desc"><p>Python class corresponding to the type of data, ignoring precision. One of (bool, int, float, complex, str)</p></div>
</dd>
<dt id="phi.math.DType.precision"><code class="name">var <span class="ident">precision</span></code></dt>
<dd>
<div class="desc"><p>Floating point precision. Only defined if <code>kind in (float, complex)</code>. For complex values, returns half of <code><a title="phi.math.DType.bits" href="#phi.math.DType.bits">DType.bits</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def precision(self):
    &#34;&#34;&#34; Floating point precision. Only defined if `kind in (float, complex)`. For complex values, returns half of `DType.bits`. &#34;&#34;&#34;
    if self.kind == float:
        return self.bits
    if self.kind == complex:
        return self.bits // 2
    else:
        return None</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.Dict"><code class="flex name class">
<span>class <span class="ident">Dict</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Dictionary of <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code> values.
Dicts are not themselves tensors and do not have a shape.
Use <code><a title="phi.math.layout" href="#phi.math.layout">layout()</a></code> to treat <code>dict</code> instances like tensors.</p>
<p>In addition to dictionary functions, supports mathematical operators with other <code><a title="phi.math.Dict" href="#phi.math.Dict">Dict</a></code>s and lookup via <code>.key</code> syntax.
<code><a title="phi.math.Dict" href="#phi.math.Dict">Dict</a></code> implements <code>PhiTreeNode</code> so instances can be passed to math operations like <code><a title="phi.math.sin" href="#phi.math.sin">sin()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dict(dict):
    &#34;&#34;&#34;
    Dictionary of `Tensor` or `PhiTreeNode` values.
    Dicts are not themselves tensors and do not have a shape.
    Use `layout()` to treat `dict` instances like tensors.

    In addition to dictionary functions, supports mathematical operators with other `Dict`s and lookup via `.key` syntax.
    `Dict` implements `PhiTreeNode` so instances can be passed to math operations like `sin`.
    &#34;&#34;&#34;

    def __value_attrs__(self):
        return tuple(self.keys())
    
    # --- Dict[key] ---

    def __getattr__(self, key):
        try:
            return self[key]
        except KeyError as k:
            raise AttributeError(k)

    def __setattr__(self, key, value):
        self[key] = value

    def __delattr__(self, key):
        try:
            del self[key]
        except KeyError as k:
            raise AttributeError(k)
        
    # --- operators ---
    
    def __neg__(self):
        return Dict({k: -v for k, v in self.items()})
    
    def __invert__(self):
        return Dict({k: ~v for k, v in self.items()})
    
    def __abs__(self):
        return Dict({k: abs(v) for k, v in self.items()})
    
    def __round__(self, n=None):
        return Dict({k: round(v) for k, v in self.items()})

    def __add__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val + other[key] for key, val in self.items()})
        else:
            return Dict({key: val + other for key, val in self.items()})

    def __radd__(self, other):
        if isinstance(other, Dict):
            return Dict({key: other[key] + val for key, val in self.items()})
        else:
            return Dict({key: other + val for key, val in self.items()})

    def __sub__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val - other[key] for key, val in self.items()})
        else:
            return Dict({key: val - other for key, val in self.items()})

    def __rsub__(self, other):
        if isinstance(other, Dict):
            return Dict({key: other[key] - val for key, val in self.items()})
        else:
            return Dict({key: other - val for key, val in self.items()})

    def __mul__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val * other[key] for key, val in self.items()})
        else:
            return Dict({key: val * other for key, val in self.items()})

    def __rmul__(self, other):
        if isinstance(other, Dict):
            return Dict({key: other[key] * val for key, val in self.items()})
        else:
            return Dict({key: other * val for key, val in self.items()})

    def __truediv__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val / other[key] for key, val in self.items()})
        else:
            return Dict({key: val / other for key, val in self.items()})

    def __rtruediv__(self, other):
        if isinstance(other, Dict):
            return Dict({key: other[key] / val for key, val in self.items()})
        else:
            return Dict({key: other / val for key, val in self.items()})

    def __floordiv__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val // other[key] for key, val in self.items()})
        else:
            return Dict({key: val // other for key, val in self.items()})

    def __rfloordiv__(self, other):
        if isinstance(other, Dict):
            return Dict({key: other[key] // val for key, val in self.items()})
        else:
            return Dict({key: other // val for key, val in self.items()})

    def __pow__(self, power, modulo=None):
        assert modulo is None
        if isinstance(power, Dict):
            return Dict({key: val ** power[key] for key, val in self.items()})
        else:
            return Dict({key: val ** power for key, val in self.items()})

    def __rpow__(self, other):
        if isinstance(other, Dict):
            return Dict({key: other[key] ** val for key, val in self.items()})
        else:
            return Dict({key: other ** val for key, val in self.items()})

    def __mod__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val % other[key] for key, val in self.items()})
        else:
            return Dict({key: val % other for key, val in self.items()})

    def __rmod__(self, other):
        if isinstance(other, Dict):
            return Dict({key: other[key] % val for key, val in self.items()})
        else:
            return Dict({key: other % val for key, val in self.items()})

    def __eq__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val == other[key] for key, val in self.items()})
        else:
            return Dict({key: val == other for key, val in self.items()})

    def __ne__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val != other[key] for key, val in self.items()})
        else:
            return Dict({key: val != other for key, val in self.items()})

    def __lt__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val &lt; other[key] for key, val in self.items()})
        else:
            return Dict({key: val &lt; other for key, val in self.items()})

    def __le__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val &lt;= other[key] for key, val in self.items()})
        else:
            return Dict({key: val &lt;= other for key, val in self.items()})

    def __gt__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val &gt; other[key] for key, val in self.items()})
        else:
            return Dict({key: val &gt; other for key, val in self.items()})

    def __ge__(self, other):
        if isinstance(other, Dict):
            return Dict({key: val &gt;= other[key] for key, val in self.items()})
        else:
            return Dict({key: val &gt;= other for key, val in self.items()})

    # --- overridden methods ---

    def copy(self):
        return Dict(self)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="phi.math.Dict.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>D.copy() -&gt; a shallow copy of D</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy(self):
    return Dict(self)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.Diverged"><code class="flex name class">
<span>class <span class="ident">Diverged</span></span>
</code></dt>
<dd>
<div class="desc"><p>Raised if the optimization was stopped prematurely and cannot continue.
This may indicate that no solution exists.</p>
<p>The values of the last estimate <code>x</code> may or may not be finite.</p>
<p>This exception inherits from <code><a title="phi.math.ConvergenceException" href="#phi.math.ConvergenceException">ConvergenceException</a></code>.</p>
<p>See Also:
<code><a title="phi.math.NotConverged" href="#phi.math.NotConverged">NotConverged</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Diverged(ConvergenceException):
    &#34;&#34;&#34;
    Raised if the optimization was stopped prematurely and cannot continue.
    This may indicate that no solution exists.

    The values of the last estimate `x` may or may not be finite.

    This exception inherits from `ConvergenceException`.

    See Also:
        `NotConverged`.
    &#34;&#34;&#34;

    def __init__(self, result: SolveInfo):
        ConvergenceException.__init__(self, result)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>phi.math._functional.ConvergenceException</li>
<li>builtins.RuntimeError</li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="phi.math.IncompatibleShapes"><code class="flex name class">
<span>class <span class="ident">IncompatibleShapes</span></span>
<span>(</span><span>message, *shapes: phi.math._shape.Shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Raised when the shape of a tensor does not match the other arguments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IncompatibleShapes(ValueError):
    &#34;&#34;&#34;
    Raised when the shape of a tensor does not match the other arguments.
    &#34;&#34;&#34;
    def __init__(self, message, *shapes: Shape):
        ValueError.__init__(self, message)
        self.shapes = shapes</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.ValueError</li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="phi.math.LinearFunction"><code class="flex name class">
<span>class <span class="ident">LinearFunction</span></span>
</code></dt>
<dd>
<div class="desc"><p>Just-in-time compiled linear function of <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> arguments and return values.</p>
<p>Use <code><a title="phi.math.jit_compile_linear" href="#phi.math.jit_compile_linear">jit_compile_linear()</a></code> to create a linear function representation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearFunction(Generic[X, Y], Callable[[X], Y]):
    &#34;&#34;&#34;
    Just-in-time compiled linear function of `Tensor` arguments and return values.

    Use `jit_compile_linear()` to create a linear function representation.
    &#34;&#34;&#34;

    def __init__(self, f, auxiliary_args: Set[str]):
        self.f = f
        self.f_params = function_parameters(f)
        self.auxiliary_args = auxiliary_args
        self.tracers: Dict[SignatureKey, ShiftLinTracer] = {}
        self.nl_jit = JitFunction(f, self.auxiliary_args)  # for backends that do not support sparse matrices

    def _trace(self, in_key: SignatureKey, prefer_numpy: bool) -&gt; &#39;ShiftLinTracer&#39;:
        assert in_key.shapes[0].is_uniform, f&#34;math.jit_compile_linear() only supports uniform tensors for function input and output but input shape was {in_key.shapes[0]}&#34;
        with NUMPY if prefer_numpy else in_key.backend:
            x = math.ones(in_key.shapes[0])
            tracer = ShiftLinTracer(x, {EMPTY_SHAPE: math.ones()}, x.shape, math.zeros(x.shape))
        x_kwargs = assemble_tree(in_key.tree, [tracer])
        result = self.f(**x_kwargs, **in_key.auxiliary_kwargs)
        _, result_tensors = disassemble_tree(result)
        assert len(result_tensors) == 1, f&#34;Linear function must return a single Tensor or tensor-like but got {result}&#34;
        result_tensor = result_tensors[0]
        assert isinstance(result_tensor, ShiftLinTracer), f&#34;Tracing linear function &#39;{f_name(self.f)}&#39; failed. Make sure only linear operations are used.&#34;
        return result_tensor

    def _get_or_trace(self, key: SignatureKey, prefer_numpy: bool):
        if not key.tracing and key in self.tracers:
            return self.tracers[key]
        else:
            tracer = self._trace(key, prefer_numpy=prefer_numpy)
            if not key.tracing:
                self.tracers[key] = tracer
                if len(self.tracers) &gt;= 4:
                    warnings.warn(f&#34;&#34;&#34;Φ-lin: The compiled linear function &#39;{f_name(self.f)}&#39; was traced {len(self.tracers)} times.
Performing many traces may be slow and cause memory leaks.
Tensors in conditioning arguments (all except the first parameter unless specified otherwise) are compared by reference, not by tensor values.
Auxiliary arguments: {key.auxiliary_kwargs}
Multiple linear traces can be avoided by jit-compiling the code that calls the linear function.&#34;&#34;&#34;, RuntimeWarning, stacklevel=3)
            return tracer

    def __call__(self, *args: X, **kwargs) -&gt; Y:
        key, tensors, natives, x = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
        assert tensors, &#34;Linear function requires at least one argument&#34;
        if any(isinstance(t, ShiftLinTracer) for t in tensors):
            # TODO: if t is identity, use cached ShiftLinTracer, otherwise multiply two ShiftLinTracers
            return self.f(*args, **kwargs)
        if not key.backend.supports(Backend.sparse_coo_tensor):
            # warnings.warn(f&#34;Sparse matrices are not supported by {backend}. Falling back to regular jit compilation.&#34;, RuntimeWarning)
            if not all_available(*tensors):  # avoid nested tracing, Typical case jax.scipy.sparse.cg(LinearFunction). Nested traces cannot be reused which results in lots of traces per cg.
                PHI_LOGGER.debug(f&#34;Φ-lin: Running &#39;{f_name(self.f)}&#39; as-is with {key.backend} because it is being traced.&#34;)
                return self.f(*args, **kwargs)
            else:
                return self.nl_jit(*args, **kwargs)
        tracer = self._get_or_trace(key, prefer_numpy=False)
        return tracer.apply(tensors[0])

    def sparse_matrix(self, *args, format: str = None, prefer_numpy=False, **kwargs):
        key, *_ = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
        tracer = self._get_or_trace(key, prefer_numpy=prefer_numpy)
        assert math.close(tracer.bias, 0), &#34;This is an affine function and cannot be represented by a single matrix. Use sparse_matrix_and_bias() instead.&#34;
        return tracer.get_sparse_matrix(format)

    def sparse_matrix_and_bias(self, *args, format: str = None, prefer_numpy=False, **kwargs):
        key, *_ = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
        tracer = self._get_or_trace(key, prefer_numpy=prefer_numpy)
        return tracer.get_sparse_matrix(format), tracer.bias

    def stencil_inspector(self, *args, prefer_numpy=True, **kwargs):
        key, _, _, _ = key_from_args(*args, cache=True, **kwargs)
        tracer = self._get_or_trace(key, prefer_numpy=prefer_numpy)

        def print_stencil(**indices):
            pos = spatial(**indices)
            print(f&#34;{f_name(self.f)}: {pos} = {&#39; + &#39;.join(f&#39;{val[indices]} * {vector_add(pos, offset)}&#39; for offset, val in tracer.val.items() if (val[indices] != 0).all)}&#34;)

        return print_stencil

    def __repr__(self):
        return f&#34;lin({f_name(self.f)})&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>collections.abc.Callable</li>
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="phi.math.LinearFunction.sparse_matrix"><code class="name flex">
<span>def <span class="ident">sparse_matrix</span></span>(<span>self, *args, format: str = None, prefer_numpy=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sparse_matrix(self, *args, format: str = None, prefer_numpy=False, **kwargs):
    key, *_ = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
    tracer = self._get_or_trace(key, prefer_numpy=prefer_numpy)
    assert math.close(tracer.bias, 0), &#34;This is an affine function and cannot be represented by a single matrix. Use sparse_matrix_and_bias() instead.&#34;
    return tracer.get_sparse_matrix(format)</code></pre>
</details>
</dd>
<dt id="phi.math.LinearFunction.sparse_matrix_and_bias"><code class="name flex">
<span>def <span class="ident">sparse_matrix_and_bias</span></span>(<span>self, *args, format: str = None, prefer_numpy=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sparse_matrix_and_bias(self, *args, format: str = None, prefer_numpy=False, **kwargs):
    key, *_ = key_from_args(args, kwargs, self.f_params, cache=False, aux=self.auxiliary_args)
    tracer = self._get_or_trace(key, prefer_numpy=prefer_numpy)
    return tracer.get_sparse_matrix(format), tracer.bias</code></pre>
</details>
</dd>
<dt id="phi.math.LinearFunction.stencil_inspector"><code class="name flex">
<span>def <span class="ident">stencil_inspector</span></span>(<span>self, *args, prefer_numpy=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stencil_inspector(self, *args, prefer_numpy=True, **kwargs):
    key, _, _, _ = key_from_args(*args, cache=True, **kwargs)
    tracer = self._get_or_trace(key, prefer_numpy=prefer_numpy)

    def print_stencil(**indices):
        pos = spatial(**indices)
        print(f&#34;{f_name(self.f)}: {pos} = {&#39; + &#39;.join(f&#39;{val[indices]} * {vector_add(pos, offset)}&#39; for offset, val in tracer.val.items() if (val[indices] != 0).all)}&#34;)

    return print_stencil</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.NotConverged"><code class="flex name class">
<span>class <span class="ident">NotConverged</span></span>
</code></dt>
<dd>
<div class="desc"><p>Raised during optimization if the desired accuracy was not reached within the maximum number of iterations.</p>
<p>This exception inherits from <code><a title="phi.math.ConvergenceException" href="#phi.math.ConvergenceException">ConvergenceException</a></code>.</p>
<p>See Also:
<code><a title="phi.math.Diverged" href="#phi.math.Diverged">Diverged</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NotConverged(ConvergenceException):
    &#34;&#34;&#34;
    Raised during optimization if the desired accuracy was not reached within the maximum number of iterations.

    This exception inherits from `ConvergenceException`.

    See Also:
        `Diverged`.
    &#34;&#34;&#34;

    def __init__(self, result: SolveInfo):
        ConvergenceException.__init__(self, result)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>phi.math._functional.ConvergenceException</li>
<li>builtins.RuntimeError</li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="phi.math.Shape"><code class="flex name class">
<span>class <span class="ident">Shape</span></span>
</code></dt>
<dd>
<div class="desc"><p>Shapes enumerate dimensions, each consisting of a name, size and type.</p>
<p>There are four types of dimensions: <code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code>, and <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>.</p>
<p>To construct a <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>, use <code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code> or <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, depending on the desired dimension type.
To create a shape with multiple types, use <code><a title="phi.math.merge_shapes" href="#phi.math.merge_shapes">merge_shapes()</a></code>, <code><a title="phi.math.concat_shapes" href="#phi.math.concat_shapes">concat_shapes()</a></code> or the syntax <code>shape1 &amp; shape2</code>.</p>
<p>The <code>__init__</code> constructor is for internal use only.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Shape:
    &#34;&#34;&#34;
    Shapes enumerate dimensions, each consisting of a name, size and type.

    There are four types of dimensions: `batch`, `spatial`, `channel`, and `instance`.
    &#34;&#34;&#34;

    def __init__(self, sizes: tuple, names: tuple, types: tuple, item_names: tuple):
        &#34;&#34;&#34;
        To construct a `Shape`, use `batch`, `spatial`, `channel` or `instance`, depending on the desired dimension type.
        To create a shape with multiple types, use `merge_shapes()`, `concat_shapes()` or the syntax `shape1 &amp; shape2`.

        The `__init__` constructor is for internal use only.
        &#34;&#34;&#34;
        if len(sizes) &gt; 0 and any(s is not None and not isinstance(s, int) for s in sizes):
            from ._tensors import Tensor
            sizes = tuple([s if isinstance(s, Tensor) or s is None else int(s) for s in sizes])  # TODO replace this by an assert
        self.sizes: tuple = sizes
        &#34;&#34;&#34;
        Ordered dimension sizes as `tuple`.
        The size of a dimension can be an `int` or a `Tensor` for [non-uniform shapes](https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors).
        
        See Also:
            `Shape.get_size()`, `Shape.size`, `Shape.shape`.
        &#34;&#34;&#34;
        self.names: Tuple[str] = names
        &#34;&#34;&#34;
        Ordered dimension names as `tuple[str]`.
        
        See Also:
            `Shape.name`.
        &#34;&#34;&#34;
        self.types: Tuple[str] = types  # undocumented, may be private
        self.item_names: Tuple[str or &#39;Shape&#39;] = (None,) * len(sizes) if item_names is None else item_names
        # Debug asserts
        # assert len(sizes) == len(names) == len(types) == len(item_names), f&#34;sizes={sizes}, names={names}, types={types}, item_names={item_names}&#34;
        # assert all(isinstance(n, str) for n in names), f&#34;All names must be of type string but got {names}&#34;
        # assert isinstance(self.item_names, tuple)
        # assert all([items is None or isinstance(items, tuple) for items in self.item_names])
        # assert all([items is None or all([isinstance(n, str) for n in items]) for items in self.item_names])
        # for size in sizes:
        #     if size is not None and not isinstance(size, int):
        #         assert size.rank &gt; 0

    def _to_dict(self, include_sizes=True):
        result = dict(names=self.names, types=self.types, item_names=self.item_names)
        if include_sizes:
            if not all([isinstance(s, int)] for s in self.sizes):
                raise NotImplementedError()
            result[&#39;sizes&#39;] = self.sizes
        return result

    @staticmethod
    def _from_dict(dict_: dict):
        names = tuple(dict_[&#39;names&#39;])
        sizes = tuple(dict_[&#39;sizes&#39;]) if &#39;sizes&#39; in dict_ else (None,) * len(names)
        item_names = tuple([None if n is None else tuple(n) for n in dict_[&#39;item_names&#39;]])
        return Shape(sizes, names, tuple(dict_[&#39;types&#39;]), item_names)

    @property
    def _named_sizes(self):
        return zip(self.names, self.sizes)

    @property
    def _dimensions(self):
        return zip(self.sizes, self.names, self.types, self.item_names)

    def __len__(self):
        return len(self.sizes)

    def __contains__(self, item):
        if isinstance(item, str):
            return item in self.names
        elif isinstance(item, Shape):
            return all([d in self.names for d in item.names])
        else:
            raise ValueError(item)

    def __iter__(self):
        return iter(self[i] for i in range(self.rank))

    def index(self, dim: str or &#39;Shape&#39; or None) -&gt; int:
        &#34;&#34;&#34;
        Finds the index of the dimension within this `Shape`.

        See Also:
            `Shape.indices()`.

        Args:
            dim: Dimension name or single-dimension `Shape`.

        Returns:
            Index as `int`.
        &#34;&#34;&#34;
        if dim is None:
            return None
        elif isinstance(dim, str):
            if dim not in self.names:
                raise ValueError(f&#34;Shape {self} has no dimension &#39;{dim}&#39;&#34;)
            return self.names.index(dim)
        elif isinstance(dim, Shape):
            assert dim.rank == 1, f&#34;index() requires a single dimension as input but got {dim}. Use indices() for multiple dimensions.&#34;
            return self.names.index(dim.name)
        else:
            raise ValueError(f&#34;index() requires a single dimension as input but got {dim}&#34;)

    def indices(self, dims: tuple or list or &#39;Shape&#39;) -&gt; Tuple[int]:
        &#34;&#34;&#34;
        Finds the indices of the given dimensions within this `Shape`.

        See Also:
            `Shape.index()`.

        Args:
            dims: Sequence of dimensions as `tuple`, `list` or `Shape`.

        Returns:
            Indices as `tuple[int]`.
        &#34;&#34;&#34;
        if isinstance(dims, (list, tuple)):
            return tuple([self.index(n) for n in dims])
        elif isinstance(dims, Shape):
            return tuple([self.index(n) for n in dims.names])
        else:
            raise ValueError(f&#34;indices() requires a sequence of dimensions but got {dims}&#34;)

    def get_size(self, dim: str or &#39;Shape&#39;):
        &#34;&#34;&#34;
        See Also:
            `Shape.get_sizes()`, `Shape.size`

        Args:
            dim: Dimension, either as name `str` or single-dimension `Shape`.

        Returns:
            Size associated with `dim` as `int` or `Tensor`.
        &#34;&#34;&#34;
        if isinstance(dim, str):
            return self.sizes[self.names.index(dim)]
        elif isinstance(dim, Shape):
            assert dim.rank == 1, f&#34;get_size() requires a single dimension but got {dim}. Use indices() to get multiple sizes.&#34;
            return self.sizes[self.names.index(dim.name)]
        else:
            raise ValueError(f&#34;get_size() requires a single dimension but got {dim}. Use indices() to get multiple sizes.&#34;)

    def get_sizes(self, dims: tuple or list or &#39;Shape&#39;) -&gt; tuple:
        &#34;&#34;&#34;
        See Also:
            `Shape.get_size()`

        Args:
            dims: Dimensions as `tuple`, `list` or `Shape`.

        Returns:
            `tuple`
        &#34;&#34;&#34;
        assert isinstance(dims, (tuple, list, Shape)), f&#34;get_sizes() requires a sequence of dimensions but got {dims}&#34;
        return tuple([self.get_size(dim) for dim in dims])

    def get_type(self, dim: str or &#39;Shape&#39;) -&gt; str:
        # undocumented, use get_dim_type() instead.
        if isinstance(dim, str):
            return self.types[self.names.index(dim)]
        elif isinstance(dim, Shape):
            assert dim.rank == 1, f&#34;Shape.get_type() only accepts single-dimension Shapes but got {dim}&#34;
            return self.types[self.names.index(dim.name)]
        else:
            raise ValueError(dim)

    def get_dim_type(self, dim: str or &#39;Shape&#39;) -&gt; Callable:
        &#34;&#34;&#34;
        Args:
            dim: Dimension, either as name `str` or single-dimension `Shape`.

        Returns:
            Dimension type, one of `batch`, `spatial`, `instance`, `channel`.
        &#34;&#34;&#34;
        return {BATCH_DIM: batch, SPATIAL_DIM: spatial, INSTANCE_DIM: instance, CHANNEL_DIM: channel}[self.get_type(dim)]

    def get_types(self, dims: tuple or list or &#39;Shape&#39;) -&gt; tuple:
        # undocumented, do not use
        if isinstance(dims, (tuple, list)):
            return tuple(self.get_type(n) for n in dims)
        elif isinstance(dims, Shape):
            return tuple(self.get_type(n) for n in dims.names)
        else:
            raise ValueError(dims)

    def get_item_names(self, dim: str or &#39;Shape&#39; or int, fallback_spatial=False) -&gt; tuple or None:
        &#34;&#34;&#34;
        Args:
            fallback_spatial: If `True` and no item names are defined for `dim` and `dim` is a channel dimension, the spatial dimension names are interpreted as item names along `dim` in the order they are listed in this `Shape`.
            dim: Dimension, either as `int` index, `str` name or single-dimension `Shape`.

        Returns:
            Item names as `tuple` or `None` if not defined.
        &#34;&#34;&#34;
        if isinstance(dim, int):
            result = self.item_names[dim]
        elif isinstance(dim, str):
            result = self.item_names[self.index(dim)]
        elif isinstance(dim, Shape):
            assert dim.rank == 1, f&#34;Shape.get_type() only accepts single-dimension Shapes but got {dim}&#34;
            result = self.item_names[self.names.index(dim.name)]
        else:
            raise ValueError(dim)
        if result is not None:
            return result
        elif fallback_spatial and self.spatial_rank == self.get_size(dim) and self.get_type(dim) == CHANNEL_DIM:
            return self.spatial.names
        else:
            return None

    def flipped(self, dims: List[str] or Tuple[str]):
        item_names = list(self.item_names)
        for dim in dims:
            if dim in self.names:
                dim_i_n = self.get_item_names(dim)
                if dim_i_n is not None:
                    item_names[self.index(dim)] = tuple(reversed(dim_i_n))
        return Shape(self.sizes, self.names, self.types, tuple(item_names))

    def __getitem__(self, selection):
        if isinstance(selection, int):
            return Shape((self.sizes[selection],), (self.names[selection],), (self.types[selection],), (self.item_names[selection],))
        elif isinstance(selection, slice):
            return Shape(self.sizes[selection], self.names[selection], self.types[selection], self.item_names[selection])
        elif isinstance(selection, str):
            if &#39;,&#39; in selection:
                selection = [self.index(s.strip()) for s in selection.split(&#39;,&#39;)]
            else:
                selection = self.index(selection)
            return self[selection]
        elif isinstance(selection, (tuple, list)):
            selection = [self.index(s) if isinstance(s, str) else s for s in selection]
            return Shape(tuple([self.sizes[i] for i in selection]), tuple([self.names[i] for i in selection]), tuple([self.types[i] for i in selection]), tuple([self.item_names[i] for i in selection]))
        raise AssertionError(&#34;Can only access shape elements as shape[int] or shape[slice]&#34;)

    @property
    def reversed(self):
        return Shape(tuple(reversed(self.sizes)), tuple(reversed(self.names)), tuple(reversed(self.types)), tuple(reversed(self.item_names)))

    @property
    def batch(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the batch dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == BATCH_DIM]]

    @property
    def non_batch(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the non-batch dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != BATCH_DIM]]

    @property
    def spatial(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the spatial dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == SPATIAL_DIM]]

    @property
    def non_spatial(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the non-spatial dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != SPATIAL_DIM]]

    @property
    def instance(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the instance dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == INSTANCE_DIM]]

    @property
    def non_instance(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the non-instance dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != INSTANCE_DIM]]

    @property
    def channel(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the channel dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == CHANNEL_DIM]]

    @property
    def non_channel(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the non-channel dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != CHANNEL_DIM]]

    @property
    def non_singleton(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only non-singleton dimensions as a new `Shape` object.
        Dimensions are singleton if their size is exactly `1`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, s in enumerate(self.sizes) if not _size_equal(s, 1)]]

    def unstack(self, dim=&#39;dims&#39;) -&gt; Tuple[&#39;Shape&#39;]:
        &#34;&#34;&#34;
        Slices this `Shape` along a dimension.
        The dimension listing the sizes of the shape is referred to as `&#39;dims&#39;`.

        Non-uniform tensor shapes may be unstacked along other dimensions as well, see
        https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors

        Args:
            dim: dimension to unstack

        Returns:
            slices of this shape
        &#34;&#34;&#34;
        if dim == &#39;dims&#39;:
            return tuple(Shape((self.sizes[i],), (self.names[i],), (self.types[i],), (self.item_names[i],)) for i in range(self.rank))
        if dim not in self:
            return tuple([self])
        else:
            from ._tensors import Tensor
            inner = self.without(dim)
            sizes = []
            dim_size = self.get_size(dim)
            for size in inner.sizes:
                if isinstance(size, Tensor) and dim in size.shape:
                    sizes.append(size.unstack(dim))
                    dim_size = size.shape.get_size(dim)
                else:
                    sizes.append(size)
            assert isinstance(dim_size, int)
            shapes = tuple(Shape(tuple([int(size[i]) if isinstance(size, tuple) else size for size in sizes]), inner.names, inner.types, inner.item_names) for i in range(dim_size))
            return shapes

    @property
    def name(self) -&gt; str:
        &#34;&#34;&#34;
        Only for Shapes containing exactly one single dimension.
        Returns the name of the dimension.

        See Also:
            `Shape.names`.
        &#34;&#34;&#34;
        assert self.rank == 1, f&#34;Shape.name is only defined for shapes of rank 1. shape={self}&#34;
        return self.names[0]

    @property
    def size(self) -&gt; int:
        &#34;&#34;&#34;
        Only for Shapes containing exactly one single dimension.
        Returns the size of the dimension.

        See Also:
            `Shape.sizes`, `Shape.get_size()`.
        &#34;&#34;&#34;
        assert self.rank == 1, &#34;Shape.size is only defined for shapes of rank 1.&#34;
        return self.sizes[0]

    @property
    def type(self) -&gt; int:
        &#34;&#34;&#34;
        Only for Shapes containing exactly one single dimension.
        Returns the type of the dimension.

        See Also:
            `Shape.get_type()`.
        &#34;&#34;&#34;
        assert self.rank == 1, &#34;Shape.type is only defined for shapes of rank 1.&#34;
        return self.types[0]

    def __int__(self):
        assert self.rank == 1, &#34;int(Shape) is only defined for shapes of rank 1.&#34;
        return self.sizes[0]

    def mask(self, names: tuple or list or set or &#39;Shape&#39;):
        &#34;&#34;&#34;
        Returns a binary sequence corresponding to the names of this Shape.
        A value of 1 means that a dimension of this Shape is contained in `names`.

        Args:
          names: instance of dimension
          names: tuple or list or set: 

        Returns:
          binary sequence

        &#34;&#34;&#34;
        if isinstance(names, str):
            names = [names]
        elif isinstance(names, Shape):
            names = names.names
        mask = [1 if name in names else 0 for name in self.names]
        return tuple(mask)

    def __repr__(self):
        def size_repr(size, items):
            if items is not None:
                if len(items) &lt;= 4:
                    return &#34;,&#34;.join(items)
                else:
                    return f&#34;{size}:{items[0]}..{items[-1]}&#34;
            else:
                return size

        strings = [f&#34;{name}{TYPE_ABBR.get(dim_type, &#39;?&#39;)}={size_repr(size, items)}&#34; for size, name, dim_type, items in self._dimensions]
        return &#39;(&#39; + &#39;, &#39;.join(strings) + &#39;)&#39;

    def __eq__(self, other):
        if not isinstance(other, Shape):
            return False
        if self.names != other.names or self.types != other.types:
            return False
        for size1, size2 in zip(self.sizes, other.sizes):
            equal = size1 == size2
            assert isinstance(equal, (bool, math.Tensor))
            if isinstance(equal, math.Tensor):
                equal = equal.all
            if not equal:
                return False
        for names1, names2 in zip(self.item_names, other.item_names):
            if names1 != names2:
                return False
        return True

    def __ne__(self, other):
        return not self == other

    def __bool__(self):
        return self.rank &gt; 0

    def _reorder(self, names: tuple or list or &#39;Shape&#39;) -&gt; &#39;Shape&#39;:
        assert len(names) == self.rank
        if isinstance(names, Shape):
            names = names.names
        order = [self.index(n) for n in names]
        return self[order]

    def _order_group(self, names: tuple or list or &#39;Shape&#39;) -&gt; list:
        &#34;&#34;&#34; Reorders the dimensions of this `Shape` so that `names` are clustered together and occur in the specified order. &#34;&#34;&#34;
        if isinstance(names, Shape):
            names = names.names
        result = []
        for dim in self.names:
            if dim not in result:
                if dim in names:
                    result.extend(names)
                else:
                    result.append(dim)
        return result

    def __and__(self, other):
        return merge_shapes(self, other)

    def _expand(self, dim: &#39;Shape&#39;, pos=None) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;**Deprecated.** Use `phi.math.merge_shapes()` or `phi.math.concat_shapes()` instead. &#34;&#34;&#34;
        warnings.warn(&#34;Shape.expand() is deprecated. Use merge_shapes() or concat_shapes() instead.&#34;, DeprecationWarning)
        if not dim:
            return self
        assert dim.name not in self, f&#34;Cannot expand shape {self} by {dim} because dimension already exists.&#34;
        assert isinstance(dim, Shape) and dim.rank == 1, f&#34;Shape.expand() requires a single dimension as a Shape but got {dim}&#34;
        if pos is None:
            same_type_dims = self[[i for i, t in enumerate(self.types) if t == dim.type]]
            if len(same_type_dims) &gt; 0:
                pos = self.index(same_type_dims.names[0])
            else:
                pos = {BATCH_DIM: 0, INSTANCE_DIM: self.batch_rank, SPATIAL_DIM: self.batch.rank + self.instance_rank, CHANNEL_DIM: self.rank + 1}[dim.type]
        elif pos &lt; 0:
            pos += self.rank + 1
        sizes = list(self.sizes)
        names = list(self.names)
        types = list(self.types)
        item_names = list(self.item_names)
        sizes.insert(pos, dim.size)
        names.insert(pos, dim.name)
        types.insert(pos, dim.type)
        item_names.insert(pos, dim.item_names[0])
        return Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))

    def without(self, dims: &#39;DimFilter&#39;) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Builds a new shape from this one that is missing all given dimensions.
        Dimensions in `dims` that are not part of this Shape are ignored.
        
        The complementary operation is `Shape.only()`.

        Args:
          dims: Single dimension (str) or instance of dimensions (tuple, list, Shape)
          dims: Dimensions to exclude as `str` or `tuple` or `list` or `Shape`. Dimensions that are not included in this shape are ignored.

        Returns:
          Shape without specified dimensions
        &#34;&#34;&#34;
        if callable(dims):
            dims = dims(self)
        if isinstance(dims, str):
            dims = parse_dim_order(dims)
        if isinstance(dims, (tuple, list)):
            return self[[i for i in range(self.rank) if self.names[i] not in dims]]
        elif isinstance(dims, Shape):
            return self[[i for i in range(self.rank) if self.names[i] not in dims.names]]
        elif dims is None:  # subtract none
            return self
        else:
            raise ValueError(dims)

    def only(self, dims: &#39;DimFilter&#39;):
        &#34;&#34;&#34;
        Builds a new shape from this one that only contains the given dimensions.
        Dimensions in `dims` that are not part of this Shape are ignored.
        
        The complementary operation is :func:`Shape.without`.

        Args:
          dims: comma-separated dimension names (str) or instance of dimensions (tuple, list, Shape) or filter function.

        Returns:
          Shape containing only specified dimensions

        &#34;&#34;&#34;
        if callable(dims):
            dims = dims(self)
        if isinstance(dims, str):
            dims = parse_dim_order(dims)
        if isinstance(dims, (tuple, list)):
            return self[[i for i in range(self.rank) if self.names[i] in dims]]
        elif isinstance(dims, Shape):
            return self[[i for i in range(self.rank) if self.names[i] in dims.names]]
        elif dims is None:  # keep none
            return EMPTY_SHAPE
        else:
            raise ValueError(dims)

    @property
    def rank(self) -&gt; int:
        &#34;&#34;&#34;
        Returns the number of dimensions.
        Equal to `len(shape)`.

        See `Shape.is_empty`, `Shape.batch_rank`, `Shape.spatial_rank`, `Shape.channel_rank`.
        &#34;&#34;&#34;
        return len(self.sizes)

    @property
    def batch_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of batch dimensions &#34;&#34;&#34;
        r = 0
        for ty in self.types:
            if ty == BATCH_DIM:
                r += 1
        return r

    @property
    def instance_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of instance dimensions &#34;&#34;&#34;
        r = 0
        for ty in self.types:
            if ty == INSTANCE_DIM:
                r += 1
        return r

    @property
    def spatial_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of spatial dimensions &#34;&#34;&#34;
        r = 0
        for ty in self.types:
            if ty == SPATIAL_DIM:
                r += 1
        return r

    @property
    def channel_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of channel dimensions &#34;&#34;&#34;
        r = 0
        for ty in self.types:
            if ty == CHANNEL_DIM:
                r += 1
        return r

    @property
    def well_defined(self):
        &#34;&#34;&#34;
        Returns `True` if no dimension size is `None`.

        Shapes with undefined sizes may be used in `phi.math.tensor()`, `phi.math.wrap()`, `phi.math.stack()` or `phi.math.concat()`.

        To create an undefined size, call a constructor function (`batch()`, `spatial()`, `channel()`, `instance()`)
        with positional `str` arguments, e.g. `spatial(&#39;x&#39;)`.
        &#34;&#34;&#34;
        for size in self.sizes:
            if size is None:
                return False
        return True

    @property
    def shape(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Higher-order `Shape`.
        The returned shape will always contain the channel dimension `dims` with a size equal to the `Shape.rank` of this shape.

        For uniform shapes, `Shape.shape` will only contain the dimension `dims` but the shapes of [non-uniform shapes](https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors)
        may contain additional dimensions.

        See Also:
            `Shape.is_uniform`.

        Returns:
            `Shape`.
        &#34;&#34;&#34;
        from phi.math import Tensor
        shape = Shape((self.rank,), (&#39;dims&#39;,), (CHANNEL_DIM,), (self.names,))
        for size in self.sizes:
            if isinstance(size, Tensor):
                shape = shape &amp; size.shape
        return shape

    @property
    def is_uniform(self) -&gt; bool:
        &#34;&#34;&#34;
        A shape is uniform if it all sizes have a single integer value.

        See Also:
            `Shape.is_non_uniform`, `Shape.shape`.
        &#34;&#34;&#34;
        return not self.is_non_uniform

    @property
    def is_non_uniform(self) -&gt; bool:
        &#34;&#34;&#34;
        A shape is non-uniform if the size of any dimension varies along another dimension.

        See Also:
            `Shape.is_uniform`, `Shape.shape`.
        &#34;&#34;&#34;
        from phi.math import Tensor
        for size in self.sizes:
            if isinstance(size, Tensor) and size.rank &gt; 0:
                return True
        return False

    def with_size(self, size: int or None):
        &#34;&#34;&#34;
        Only for single-dimension shapes.
        Returns a `Shape` representing this dimension but with a different size.

        See Also:
            `Shape.with_sizes()`.

        Args:
            size: Replacement size for this dimension.

        Returns:
            `Shape`
        &#34;&#34;&#34;
        assert self.rank == 1, &#34;Shape.with_size() is only defined for shapes of rank 1.&#34;
        return self.with_sizes([size])

    def with_sizes(self, sizes: tuple or list or &#39;Shape&#39;, keep_item_names=True):
        &#34;&#34;&#34;
        Returns a new `Shape` matching the dimension names and types of `self` but with different sizes.

        See Also:
            `Shape.with_size()`.

        Args:
            sizes: One of

                * `tuple` / `list` of same length as `self` containing replacement sizes.
                * `Shape` of any rank. Replaces sizes for dimensions shared by `sizes` and `self`.

        Returns:
            `Shape` with same names and types as `self`.
        &#34;&#34;&#34;
        if isinstance(sizes, Shape):
            item_names = [sizes.get_item_names(dim) if dim in sizes else self.get_item_names(dim) for dim in self.names]
            sizes = [sizes.get_size(dim) if dim in sizes else s for dim, s in self._named_sizes]
            return Shape(tuple(sizes), self.names, self.types, tuple(item_names))
        else:
            assert len(sizes) == len(self.sizes), f&#34;Cannot create shape from {self} with sizes {sizes}&#34;
            sizes_ = []
            item_names = []
            for i, obj in enumerate(sizes):
                new_size, new_item_names = Shape._size_and_item_names_from_obj(obj, self.sizes[i], self.item_names[i], keep_item_names)
                sizes_.append(new_size)
                item_names.append(new_item_names)
            return Shape(tuple(sizes_), self.names, self.types, tuple(item_names))

    @staticmethod
    def _size_and_item_names_from_obj(obj, prev_size, prev_item_names, keep_item_names=True):
        if isinstance(obj, str):
            obj = [s.strip() for s in obj.split(&#39;,&#39;)]
        if isinstance(obj, (tuple, list)):
            return len(obj), tuple(obj)
        elif isinstance(obj, Number):
            return obj, prev_item_names if keep_item_names and (prev_size is None or _size_equal(obj, prev_size)) else None
        elif isinstance(obj, math.Tensor) or obj is None:
            return obj, None
        else:
            raise ValueError(f&#34;sizes can only contain int, str or Tensor but got {type(obj)}&#34;)

    def without_sizes(self):
        &#34;&#34;&#34;
        Returns:
            `Shape` with all sizes undefined (`None`)
        &#34;&#34;&#34;
        return Shape((None,) * self.rank, self.names, self.types, (None,) * self.rank)

    def _replace_single_size(self, dim: str, size: int, keep_item_names: bool = False):
        new_sizes = list(self.sizes)
        new_sizes[self.index(dim)] = size
        return self.with_sizes(new_sizes, keep_item_names=keep_item_names)

    def with_dim_size(self, dim: str or &#39;Shape&#39;, size: int or &#39;math.Tensor&#39; or str or tuple or list, keep_item_names=True):
        &#34;&#34;&#34;
        Returns a new `Shape` that has a different size for `dim`.

        Args:
            dim: Dimension for which to replace the size, `Shape` or `str`.
            size: New size, `int` or `Tensor`

        Returns:
            `Shape` with same names and types as `self`.
        &#34;&#34;&#34;
        if isinstance(dim, Shape):
            dim = dim.name
        assert isinstance(dim, str)
        new_size, new_item_names = Shape._size_and_item_names_from_obj(size, self.get_size(dim), self.get_item_names(dim), keep_item_names)
        return self.replace(dim, Shape((new_size,), (dim,), (self.get_type(dim),), (new_item_names,)))

    def _with_names(self, names: str or tuple or list):
        if isinstance(names, str):
            names = parse_dim_names(names, self.rank)
            names = [n if n is not None else o for n, o in zip(names, self.names)]
        return Shape(self.sizes, tuple(names), self.types, self.item_names)

    def _replace_names_and_types(self,
                                 dims: &#39;Shape&#39; or str or tuple or list,
                                 new: &#39;Shape&#39; or str or tuple or list) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Returns a copy of `self` with `dims` replaced by `new`.
        Dimensions that are not present in `self` are ignored.

        The dimension order is preserved.

        Args:
            dims: Dimensions to replace.
            new: New dimensions, must have same length as `dims`.
                If a `Shape` is given, replaces the dimension types and item names as well.

        Returns:
            `Shape` with same rank and dimension order as `self`.
        &#34;&#34;&#34;
        dims = parse_dim_order(dims)
        sizes = [math.rename_dims(s, dims, new) if isinstance(s, math.Tensor) else s for s in self.sizes]
        if isinstance(new, Shape):  # replace names and types
            names = list(self.names)
            types = list(self.types)
            item_names = list(self.item_names)
            for old_name, new_dim in zip(dims, new):
                if old_name in self:
                    names[self.index(old_name)] = new_dim.name
                    types[self.index(old_name)] = new_dim.type
                    item_names[self.index(old_name)] = new_dim.item_names[0]
            return Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))
        else:  # replace only names
            new = parse_dim_order(new)
            names = list(self.names)
            for old_name, new_name in zip(dims, new):
                if old_name in self:
                    names[self.index(old_name)] = new_name
            return Shape(tuple(sizes), tuple(names), self.types, self.item_names)

    def replace(self, dims: &#39;Shape&#39; or str or tuple or list, new: &#39;Shape&#39;) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Returns a copy of `self` with `dims` replaced by `new`.
        Dimensions that are not present in `self` are ignored.

        The dimension order is preserved.

        Args:
            dims: Dimensions to replace.
            new: New dimensions, must have same length as `dims`.
                If a `Shape` is given, replaces the dimension types and item names as well.

        Returns:
            `Shape` with same rank and dimension order as `self`.
        &#34;&#34;&#34;
        dims = parse_dim_order(dims)
        assert isinstance(new, Shape), f&#34;new must be a Shape but got {new}&#34;
        names = list(self.names)
        sizes = list(self.sizes)
        types = list(self.types)
        item_names = list(self.item_names)
        for old_name, new_dim in zip(dims, new):
            if old_name in self:
                names[self.index(old_name)] = new_dim.name
                types[self.index(old_name)] = new_dim.type
                item_names[self.index(old_name)] = new_dim.item_names[0]
                sizes[self.index(old_name)] = new_dim.size
        return Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))

    def _with_types(self, types: &#39;Shape&#39;):
        return Shape(self.sizes, self.names, tuple([types.get_type(name) if name in types else self_type for name, self_type in zip(self.names, self.types)]), self.item_names)

    def _with_item_names(self, item_names: tuple):
        return Shape(self.sizes, self.names, self.types, item_names)

    def _with_item_name(self, dim: str, item_name: tuple):
        if dim not in self:
            return self
        item_names = list(self.item_names)
        item_names[self.index(dim)] = item_name
        return Shape(self.sizes, self.names, self.types, tuple(item_names))

    def _perm(self, names: Tuple[str]):
        assert len(set(names)) == len(names), f&#34;No duplicates allowed but got {names}&#34;
        assert len(names) &gt;= len(self.names), f&#34;Cannot find permutation for {self} given {names} because names {set(self.names) - set(names)} are missing&#34;
        assert len(names) &lt;= len(self.names), f&#34;Cannot find permutation for {self} given {names} because too many names were passed: {names}&#34;
        perm = [self.names.index(name) for name in names]
        return perm

    @property
    def volume(self) -&gt; int or None:
        &#34;&#34;&#34;
        Returns the total number of values contained in a tensor of this shape.
        This is the product of all dimension sizes.

        Returns:
            volume as `int` or `Tensor` or `None` if the shape is not `Shape.well_defined`
        &#34;&#34;&#34;
        from phi.math import Tensor
        for dim, size in self._named_sizes:
            if isinstance(size, Tensor) and size.rank &gt; 0:
                non_uniform_dim = size.shape.names[0]
                shapes = self.unstack(non_uniform_dim)
                return sum(s.volume for s in shapes)
        result = 1
        for size in self.sizes:
            if size is None:
                return None
            result *= size
        return int(result)

    @property
    def is_empty(self) -&gt; bool:
        &#34;&#34;&#34; True if this shape has no dimensions. Equivalent to `Shape.rank` `== 0`. &#34;&#34;&#34;
        return len(self.sizes) == 0

    def after_pad(self, widths: dict) -&gt; &#39;Shape&#39;:
        sizes = list(self.sizes)
        item_names = list(self.item_names)
        for dim, (lo, up) in widths.items():
            sizes[self.index(dim)] += lo + up
            item_names[self.index(dim)] = None
        return Shape(tuple(sizes), self.names, self.types, tuple(item_names))

    def prepare_gather(self, dim: str, selection):
        if isinstance(selection, Shape):
            selection = selection.name if selection.rank == 1 else selection.names
        if isinstance(selection, str) and &#39;,&#39; in selection:
            selection = parse_dim_order(selection)
        if isinstance(selection, str):  # single item name
            item_names = self.get_item_names(dim, fallback_spatial=True)
            assert item_names is not None, f&#34;No item names defined for dim &#39;{dim}&#39; in tensor {self.shape} and dimension size does not match spatial rank.&#34;
            assert selection in item_names, f&#34;Accessing tensor.{dim}[&#39;{selection}&#39;] failed. Item names are {item_names}.&#34;
            selection = item_names.index(selection)
        if isinstance(selection, (tuple, list)):
            selection = list(selection)
            if any([isinstance(s, str) for s in selection]):
                item_names = self.get_item_names(dim, fallback_spatial=True)
                for i, s in enumerate(selection):
                    if isinstance(s, str):
                        assert item_names is not None, f&#34;Accessing tensor.{dim}[&#39;{s}&#39;] failed because no item names are present on tensor {self.shape}&#34;
                        assert s in item_names, f&#34;Accessing tensor.{dim}[&#39;{s}&#39;] failed. Item names are {item_names}.&#34;
                        selection[i] = item_names.index(s)
            if not selection:  # empty
                selection = slice(0, 0)
        return selection

    def after_gather(self, selection: dict) -&gt; &#39;Shape&#39;:
        result = self
        for sel_dim, selection in selection.items():
            if sel_dim not in self.names:
                continue
            selection = self.prepare_gather(sel_dim, selection)
            if isinstance(selection, int):
                if result.is_uniform:
                    result = result.without(sel_dim)
                else:
                    from phi.math import Tensor
                    gathered_sizes = [(s[{sel_dim: selection}] if isinstance(s, Tensor) else s) for s in result.sizes]
                    gathered_sizes = [(int(s) if isinstance(s, Tensor) and s.rank == 0 else s) for s in gathered_sizes]
                    result = result.with_sizes(gathered_sizes, keep_item_names=True).without(sel_dim)
            elif isinstance(selection, slice):
                step = selection.step or 1
                start = selection.start if isinstance(selection.start, int) else (0 if step &gt; 0 else self.get_size(sel_dim)-1)
                stop = selection.stop if isinstance(selection.stop, int) else (self.get_size(sel_dim) if step &gt; 0 else -1)
                if stop &lt; 0 and step &gt; 0:
                    stop += self.get_size(sel_dim)
                    assert stop &gt;= 0
                if start &lt; 0 and step &gt; 0:
                    start += self.get_size(sel_dim)
                    assert start &gt;= 0
                new_size = math.to_int64(math.ceil(math.wrap((stop - start) / step)))
                if new_size.rank == 0:
                    new_size = int(new_size)  # NumPy array not allowed because not hashable
                result = result._replace_single_size(sel_dim, new_size, keep_item_names=True)
                if step &lt; 0:
                    result = result.flipped([sel_dim])
                if self.get_item_names(sel_dim) is not None:
                    result = result._with_item_name(sel_dim, tuple(self.get_item_names(sel_dim)[selection]))
            elif isinstance(selection, (tuple, list)):
                result = result._replace_single_size(sel_dim, len(selection))
                if self.get_item_names(sel_dim) is not None:
                    result = result._with_item_name(sel_dim, tuple([self.get_item_names(sel_dim)[i] for i in selection]))
            else:
                raise NotImplementedError(f&#34;{type(selection)} not supported. Only (int, slice) allowed.&#34;)
        return result

    def meshgrid(self, names=False):
        &#34;&#34;&#34;
        Builds a sequence containing all multi-indices within a tensor of this shape.
        All indices are returned as `dict` mapping dimension names to `int` indices.

        The corresponding values can be retrieved from Tensors and other Sliceables using `tensor[index]`.

        This function currently only supports uniform tensors.

        Args:
            names: If `True`, replace indices by their item names if available.

        Returns:
            `dict` iterator.
        &#34;&#34;&#34;
        assert self.is_uniform, f&#34;Shape.meshgrid() is currently not supported for non-uniform tensors, {self}&#34;
        indices = [0] * self.rank
        while True:
            if names:
                yield {dim: (names[index] if names is not None else index) for dim, index, names in zip(self.names, indices, self.item_names)}
            else:
                yield {dim: index for dim, index in zip(self.names, indices)}
            for i in range(self.rank-1, -1, -1):
                indices[i] = (indices[i] + 1) % self.sizes[i]
                if indices[i] != 0:
                    break
            else:
                return

    def __add__(self, other):
        return self._op2(other, lambda s, o: s + o, 0)

    def __radd__(self, other):
        return self._op2(other, lambda s, o: o + s, 0)

    def __sub__(self, other):
        return self._op2(other, lambda s, o: s - o, 0)

    def __rsub__(self, other):
        return self._op2(other, lambda s, o: o - s, 0)

    def __mul__(self, other):
        return self._op2(other, lambda s, o: s * o, 1)

    def __rmul__(self, other):
        return self._op2(other, lambda s, o: o * s, 1)

    def _op2(self, other, fun, default: int):
        if isinstance(other, int):
            return Shape(tuple([fun(s, other) for s in self.sizes]), self.names, self.types, (None,) * self.rank)
        elif isinstance(other, Shape):
            merged = self.without_sizes() &amp; other.without_sizes()
            sizes = ()
            for dim in merged.names:
                self_val = self.get_size(dim) if dim in self else default
                other_val = other.get_size(dim) if dim in other else default
                sizes += (fun(self_val, other_val),)
            return merged.with_sizes(sizes)
        else:
            return NotImplemented

    def __hash__(self):
        return hash(self.names)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.Shape.batch"><code class="name">var <span class="ident">batch</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the batch dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.instance" href="#phi.math.Shape.instance">Shape.instance</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_instance" href="#phi.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def batch(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the batch dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == BATCH_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.batch_rank"><code class="name">var <span class="ident">batch_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of batch dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def batch_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of batch dimensions &#34;&#34;&#34;
    r = 0
    for ty in self.types:
        if ty == BATCH_DIM:
            r += 1
    return r</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.channel"><code class="name">var <span class="ident">channel</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the channel dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.instance" href="#phi.math.Shape.instance">Shape.instance</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_instance" href="#phi.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def channel(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the channel dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == CHANNEL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.channel_rank"><code class="name">var <span class="ident">channel_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of channel dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def channel_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of channel dimensions &#34;&#34;&#34;
    r = 0
    for ty in self.types:
        if ty == CHANNEL_DIM:
            r += 1
    return r</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.instance"><code class="name">var <span class="ident">instance</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the instance dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.instance" href="#phi.math.Shape.instance">Shape.instance</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_instance" href="#phi.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def instance(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the instance dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == INSTANCE_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.instance_rank"><code class="name">var <span class="ident">instance_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of instance dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def instance_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of instance dimensions &#34;&#34;&#34;
    r = 0
    for ty in self.types:
        if ty == INSTANCE_DIM:
            r += 1
    return r</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_empty"><code class="name">var <span class="ident">is_empty</span> : bool</code></dt>
<dd>
<div class="desc"><p>True if this shape has no dimensions. Equivalent to <code><a title="phi.math.Shape.rank" href="#phi.math.Shape.rank">Shape.rank</a></code> <code>== 0</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_empty(self) -&gt; bool:
    &#34;&#34;&#34; True if this shape has no dimensions. Equivalent to `Shape.rank` `== 0`. &#34;&#34;&#34;
    return len(self.sizes) == 0</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_non_uniform"><code class="name">var <span class="ident">is_non_uniform</span> : bool</code></dt>
<dd>
<div class="desc"><p>A shape is non-uniform if the size of any dimension varies along another dimension.</p>
<p>See Also:
<code><a title="phi.math.Shape.is_uniform" href="#phi.math.Shape.is_uniform">Shape.is_uniform</a></code>, <code><a title="phi.math.Shape.shape" href="#phi.math.Shape.shape">Shape.shape</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_non_uniform(self) -&gt; bool:
    &#34;&#34;&#34;
    A shape is non-uniform if the size of any dimension varies along another dimension.

    See Also:
        `Shape.is_uniform`, `Shape.shape`.
    &#34;&#34;&#34;
    from phi.math import Tensor
    for size in self.sizes:
        if isinstance(size, Tensor) and size.rank &gt; 0:
            return True
    return False</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_uniform"><code class="name">var <span class="ident">is_uniform</span> : bool</code></dt>
<dd>
<div class="desc"><p>A shape is uniform if it all sizes have a single integer value.</p>
<p>See Also:
<code><a title="phi.math.Shape.is_non_uniform" href="#phi.math.Shape.is_non_uniform">Shape.is_non_uniform</a></code>, <code><a title="phi.math.Shape.shape" href="#phi.math.Shape.shape">Shape.shape</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_uniform(self) -&gt; bool:
    &#34;&#34;&#34;
    A shape is uniform if it all sizes have a single integer value.

    See Also:
        `Shape.is_non_uniform`, `Shape.shape`.
    &#34;&#34;&#34;
    return not self.is_non_uniform</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"><p>Only for Shapes containing exactly one single dimension.
Returns the name of the dimension.</p>
<p>See Also:
<code><a title="phi.math.Shape.names" href="#phi.math.Shape.names">Shape.names</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def name(self) -&gt; str:
    &#34;&#34;&#34;
    Only for Shapes containing exactly one single dimension.
    Returns the name of the dimension.

    See Also:
        `Shape.names`.
    &#34;&#34;&#34;
    assert self.rank == 1, f&#34;Shape.name is only defined for shapes of rank 1. shape={self}&#34;
    return self.names[0]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.names"><code class="name">var <span class="ident">names</span></code></dt>
<dd>
<div class="desc"><p>Ordered dimension names as <code>tuple[str]</code>.</p>
<p>See Also:
<code><a title="phi.math.Shape.name" href="#phi.math.Shape.name">Shape.name</a></code>.</p></div>
</dd>
<dt id="phi.math.Shape.non_batch"><code class="name">var <span class="ident">non_batch</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the non-batch dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.instance" href="#phi.math.Shape.instance">Shape.instance</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_instance" href="#phi.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_batch(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the non-batch dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != BATCH_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_channel"><code class="name">var <span class="ident">non_channel</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the non-channel dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.instance" href="#phi.math.Shape.instance">Shape.instance</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_instance" href="#phi.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_channel(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the non-channel dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != CHANNEL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_instance"><code class="name">var <span class="ident">non_instance</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the non-instance dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.instance" href="#phi.math.Shape.instance">Shape.instance</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_instance" href="#phi.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_instance(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the non-instance dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != INSTANCE_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_singleton"><code class="name">var <span class="ident">non_singleton</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only non-singleton dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.
Dimensions are singleton if their size is exactly <code>1</code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_singleton(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only non-singleton dimensions as a new `Shape` object.
    Dimensions are singleton if their size is exactly `1`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, s in enumerate(self.sizes) if not _size_equal(s, 1)]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_spatial"><code class="name">var <span class="ident">non_spatial</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the non-spatial dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.instance" href="#phi.math.Shape.instance">Shape.instance</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_instance" href="#phi.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_spatial(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the non-spatial dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != SPATIAL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.rank"><code class="name">var <span class="ident">rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Returns the number of dimensions.
Equal to <code>len(<a title="phi.math.shape" href="#phi.math.shape">shape()</a>)</code>.</p>
<p>See <code><a title="phi.math.Shape.is_empty" href="#phi.math.Shape.is_empty">Shape.is_empty</a></code>, <code><a title="phi.math.Shape.batch_rank" href="#phi.math.Shape.batch_rank">Shape.batch_rank</a></code>, <code><a title="phi.math.Shape.spatial_rank" href="#phi.math.Shape.spatial_rank">Shape.spatial_rank</a></code>, <code><a title="phi.math.Shape.channel_rank" href="#phi.math.Shape.channel_rank">Shape.channel_rank</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rank(self) -&gt; int:
    &#34;&#34;&#34;
    Returns the number of dimensions.
    Equal to `len(shape)`.

    See `Shape.is_empty`, `Shape.batch_rank`, `Shape.spatial_rank`, `Shape.channel_rank`.
    &#34;&#34;&#34;
    return len(self.sizes)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.reversed"><code class="name">var <span class="ident">reversed</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def reversed(self):
    return Shape(tuple(reversed(self.sizes)), tuple(reversed(self.names)), tuple(reversed(self.types)), tuple(reversed(self.item_names)))</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.shape"><code class="name">var <span class="ident">shape</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Higher-order <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.
The returned shape will always contain the channel dimension <code>dims</code> with a size equal to the <code><a title="phi.math.Shape.rank" href="#phi.math.Shape.rank">Shape.rank</a></code> of this shape.</p>
<p>For uniform shapes, <code><a title="phi.math.Shape.shape" href="#phi.math.Shape.shape">Shape.shape</a></code> will only contain the dimension <code>dims</code> but the shapes of <a href="https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors">non-uniform shapes</a>
may contain additional dimensions.</p>
<p>See Also:
<code><a title="phi.math.Shape.is_uniform" href="#phi.math.Shape.is_uniform">Shape.is_uniform</a></code>.</p>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Higher-order `Shape`.
    The returned shape will always contain the channel dimension `dims` with a size equal to the `Shape.rank` of this shape.

    For uniform shapes, `Shape.shape` will only contain the dimension `dims` but the shapes of [non-uniform shapes](https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors)
    may contain additional dimensions.

    See Also:
        `Shape.is_uniform`.

    Returns:
        `Shape`.
    &#34;&#34;&#34;
    from phi.math import Tensor
    shape = Shape((self.rank,), (&#39;dims&#39;,), (CHANNEL_DIM,), (self.names,))
    for size in self.sizes:
        if isinstance(size, Tensor):
            shape = shape &amp; size.shape
    return shape</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.size"><code class="name">var <span class="ident">size</span> : int</code></dt>
<dd>
<div class="desc"><p>Only for Shapes containing exactly one single dimension.
Returns the size of the dimension.</p>
<p>See Also:
<code><a title="phi.math.Shape.sizes" href="#phi.math.Shape.sizes">Shape.sizes</a></code>, <code><a title="phi.math.Shape.get_size" href="#phi.math.Shape.get_size">Shape.get_size()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def size(self) -&gt; int:
    &#34;&#34;&#34;
    Only for Shapes containing exactly one single dimension.
    Returns the size of the dimension.

    See Also:
        `Shape.sizes`, `Shape.get_size()`.
    &#34;&#34;&#34;
    assert self.rank == 1, &#34;Shape.size is only defined for shapes of rank 1.&#34;
    return self.sizes[0]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.sizes"><code class="name">var <span class="ident">sizes</span></code></dt>
<dd>
<div class="desc"><p>Ordered dimension sizes as <code>tuple</code>.
The size of a dimension can be an <code>int</code> or a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> for <a href="https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors">non-uniform shapes</a>.</p>
<p>See Also:
<code><a title="phi.math.Shape.get_size" href="#phi.math.Shape.get_size">Shape.get_size()</a></code>, <code><a title="phi.math.Shape.size" href="#phi.math.Shape.size">Shape.size</a></code>, <code><a title="phi.math.Shape.shape" href="#phi.math.Shape.shape">Shape.shape</a></code>.</p></div>
</dd>
<dt id="phi.math.Shape.spatial"><code class="name">var <span class="ident">spatial</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the spatial dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.instance" href="#phi.math.Shape.instance">Shape.instance</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_instance" href="#phi.math.Shape.non_instance">Shape.non_instance</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def spatial(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the spatial dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.instance`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_instance`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == SPATIAL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.spatial_rank"><code class="name">var <span class="ident">spatial_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def spatial_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of spatial dimensions &#34;&#34;&#34;
    r = 0
    for ty in self.types:
        if ty == SPATIAL_DIM:
            r += 1
    return r</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.type"><code class="name">var <span class="ident">type</span> : int</code></dt>
<dd>
<div class="desc"><p>Only for Shapes containing exactly one single dimension.
Returns the type of the dimension.</p>
<p>See Also:
<code><a title="phi.math.Shape.get_type" href="#phi.math.Shape.get_type">Shape.get_type()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def type(self) -&gt; int:
    &#34;&#34;&#34;
    Only for Shapes containing exactly one single dimension.
    Returns the type of the dimension.

    See Also:
        `Shape.get_type()`.
    &#34;&#34;&#34;
    assert self.rank == 1, &#34;Shape.type is only defined for shapes of rank 1.&#34;
    return self.types[0]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.volume"><code class="name">var <span class="ident">volume</span> : int</code></dt>
<dd>
<div class="desc"><p>Returns the total number of values contained in a tensor of this shape.
This is the product of all dimension sizes.</p>
<h2 id="returns">Returns</h2>
<p>volume as <code>int</code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>None</code> if the shape is not <code><a title="phi.math.Shape.well_defined" href="#phi.math.Shape.well_defined">Shape.well_defined</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def volume(self) -&gt; int or None:
    &#34;&#34;&#34;
    Returns the total number of values contained in a tensor of this shape.
    This is the product of all dimension sizes.

    Returns:
        volume as `int` or `Tensor` or `None` if the shape is not `Shape.well_defined`
    &#34;&#34;&#34;
    from phi.math import Tensor
    for dim, size in self._named_sizes:
        if isinstance(size, Tensor) and size.rank &gt; 0:
            non_uniform_dim = size.shape.names[0]
            shapes = self.unstack(non_uniform_dim)
            return sum(s.volume for s in shapes)
    result = 1
    for size in self.sizes:
        if size is None:
            return None
        result *= size
    return int(result)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.well_defined"><code class="name">var <span class="ident">well_defined</span></code></dt>
<dd>
<div class="desc"><p>Returns <code>True</code> if no dimension size is <code>None</code>.</p>
<p>Shapes with undefined sizes may be used in <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code>, <code><a title="phi.math.wrap" href="#phi.math.wrap">wrap()</a></code>, <code><a title="phi.math.stack" href="#phi.math.stack">stack()</a></code> or <code><a title="phi.math.concat" href="#phi.math.concat">concat()</a></code>.</p>
<p>To create an undefined size, call a constructor function (<code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>)
with positional <code>str</code> arguments, e.g. <code>spatial('x')</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def well_defined(self):
    &#34;&#34;&#34;
    Returns `True` if no dimension size is `None`.

    Shapes with undefined sizes may be used in `phi.math.tensor()`, `phi.math.wrap()`, `phi.math.stack()` or `phi.math.concat()`.

    To create an undefined size, call a constructor function (`batch()`, `spatial()`, `channel()`, `instance()`)
    with positional `str` arguments, e.g. `spatial(&#39;x&#39;)`.
    &#34;&#34;&#34;
    for size in self.sizes:
        if size is None:
            return False
    return True</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.Shape.after_gather"><code class="name flex">
<span>def <span class="ident">after_gather</span></span>(<span>self, selection: dict) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def after_gather(self, selection: dict) -&gt; &#39;Shape&#39;:
    result = self
    for sel_dim, selection in selection.items():
        if sel_dim not in self.names:
            continue
        selection = self.prepare_gather(sel_dim, selection)
        if isinstance(selection, int):
            if result.is_uniform:
                result = result.without(sel_dim)
            else:
                from phi.math import Tensor
                gathered_sizes = [(s[{sel_dim: selection}] if isinstance(s, Tensor) else s) for s in result.sizes]
                gathered_sizes = [(int(s) if isinstance(s, Tensor) and s.rank == 0 else s) for s in gathered_sizes]
                result = result.with_sizes(gathered_sizes, keep_item_names=True).without(sel_dim)
        elif isinstance(selection, slice):
            step = selection.step or 1
            start = selection.start if isinstance(selection.start, int) else (0 if step &gt; 0 else self.get_size(sel_dim)-1)
            stop = selection.stop if isinstance(selection.stop, int) else (self.get_size(sel_dim) if step &gt; 0 else -1)
            if stop &lt; 0 and step &gt; 0:
                stop += self.get_size(sel_dim)
                assert stop &gt;= 0
            if start &lt; 0 and step &gt; 0:
                start += self.get_size(sel_dim)
                assert start &gt;= 0
            new_size = math.to_int64(math.ceil(math.wrap((stop - start) / step)))
            if new_size.rank == 0:
                new_size = int(new_size)  # NumPy array not allowed because not hashable
            result = result._replace_single_size(sel_dim, new_size, keep_item_names=True)
            if step &lt; 0:
                result = result.flipped([sel_dim])
            if self.get_item_names(sel_dim) is not None:
                result = result._with_item_name(sel_dim, tuple(self.get_item_names(sel_dim)[selection]))
        elif isinstance(selection, (tuple, list)):
            result = result._replace_single_size(sel_dim, len(selection))
            if self.get_item_names(sel_dim) is not None:
                result = result._with_item_name(sel_dim, tuple([self.get_item_names(sel_dim)[i] for i in selection]))
        else:
            raise NotImplementedError(f&#34;{type(selection)} not supported. Only (int, slice) allowed.&#34;)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.after_pad"><code class="name flex">
<span>def <span class="ident">after_pad</span></span>(<span>self, widths: dict) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def after_pad(self, widths: dict) -&gt; &#39;Shape&#39;:
    sizes = list(self.sizes)
    item_names = list(self.item_names)
    for dim, (lo, up) in widths.items():
        sizes[self.index(dim)] += lo + up
        item_names[self.index(dim)] = None
    return Shape(tuple(sizes), self.names, self.types, tuple(item_names))</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.flipped"><code class="name flex">
<span>def <span class="ident">flipped</span></span>(<span>self, dims: List[str])</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flipped(self, dims: List[str] or Tuple[str]):
    item_names = list(self.item_names)
    for dim in dims:
        if dim in self.names:
            dim_i_n = self.get_item_names(dim)
            if dim_i_n is not None:
                item_names[self.index(dim)] = tuple(reversed(dim_i_n))
    return Shape(self.sizes, self.names, self.types, tuple(item_names))</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.get_dim_type"><code class="name flex">
<span>def <span class="ident">get_dim_type</span></span>(<span>self, dim: str) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension, either as name <code>str</code> or single-dimension <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Dimension type, one of <code><a title="phi.math.batch" href="#phi.math.batch">batch()</a></code>, <code><a title="phi.math.spatial" href="#phi.math.spatial">spatial()</a></code>, <code><a title="phi.math.instance" href="#phi.math.instance">instance()</a></code>, <code><a title="phi.math.channel" href="#phi.math.channel">channel()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dim_type(self, dim: str or &#39;Shape&#39;) -&gt; Callable:
    &#34;&#34;&#34;
    Args:
        dim: Dimension, either as name `str` or single-dimension `Shape`.

    Returns:
        Dimension type, one of `batch`, `spatial`, `instance`, `channel`.
    &#34;&#34;&#34;
    return {BATCH_DIM: batch, SPATIAL_DIM: spatial, INSTANCE_DIM: instance, CHANNEL_DIM: channel}[self.get_type(dim)]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.get_item_names"><code class="name flex">
<span>def <span class="ident">get_item_names</span></span>(<span>self, dim: str, fallback_spatial=False) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>fallback_spatial</code></strong></dt>
<dd>If <code>True</code> and no item names are defined for <code>dim</code> and <code>dim</code> is a channel dimension, the spatial dimension names are interpreted as item names along <code>dim</code> in the order they are listed in this <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension, either as <code>int</code> index, <code>str</code> name or single-dimension <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Item names as <code>tuple</code> or <code>None</code> if not defined.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_item_names(self, dim: str or &#39;Shape&#39; or int, fallback_spatial=False) -&gt; tuple or None:
    &#34;&#34;&#34;
    Args:
        fallback_spatial: If `True` and no item names are defined for `dim` and `dim` is a channel dimension, the spatial dimension names are interpreted as item names along `dim` in the order they are listed in this `Shape`.
        dim: Dimension, either as `int` index, `str` name or single-dimension `Shape`.

    Returns:
        Item names as `tuple` or `None` if not defined.
    &#34;&#34;&#34;
    if isinstance(dim, int):
        result = self.item_names[dim]
    elif isinstance(dim, str):
        result = self.item_names[self.index(dim)]
    elif isinstance(dim, Shape):
        assert dim.rank == 1, f&#34;Shape.get_type() only accepts single-dimension Shapes but got {dim}&#34;
        result = self.item_names[self.names.index(dim.name)]
    else:
        raise ValueError(dim)
    if result is not None:
        return result
    elif fallback_spatial and self.spatial_rank == self.get_size(dim) and self.get_type(dim) == CHANNEL_DIM:
        return self.spatial.names
    else:
        return None</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.get_size"><code class="name flex">
<span>def <span class="ident">get_size</span></span>(<span>self, dim: str)</span>
</code></dt>
<dd>
<div class="desc"><p>See Also:
<code><a title="phi.math.Shape.get_sizes" href="#phi.math.Shape.get_sizes">Shape.get_sizes()</a></code>, <code><a title="phi.math.Shape.size" href="#phi.math.Shape.size">Shape.size</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension, either as name <code>str</code> or single-dimension <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Size associated with <code>dim</code> as <code>int</code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_size(self, dim: str or &#39;Shape&#39;):
    &#34;&#34;&#34;
    See Also:
        `Shape.get_sizes()`, `Shape.size`

    Args:
        dim: Dimension, either as name `str` or single-dimension `Shape`.

    Returns:
        Size associated with `dim` as `int` or `Tensor`.
    &#34;&#34;&#34;
    if isinstance(dim, str):
        return self.sizes[self.names.index(dim)]
    elif isinstance(dim, Shape):
        assert dim.rank == 1, f&#34;get_size() requires a single dimension but got {dim}. Use indices() to get multiple sizes.&#34;
        return self.sizes[self.names.index(dim.name)]
    else:
        raise ValueError(f&#34;get_size() requires a single dimension but got {dim}. Use indices() to get multiple sizes.&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.get_sizes"><code class="name flex">
<span>def <span class="ident">get_sizes</span></span>(<span>self, dims: tuple) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>See Also:
<code><a title="phi.math.Shape.get_size" href="#phi.math.Shape.get_size">Shape.get_size()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions as <code>tuple</code>, <code>list</code> or <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>tuple</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_sizes(self, dims: tuple or list or &#39;Shape&#39;) -&gt; tuple:
    &#34;&#34;&#34;
    See Also:
        `Shape.get_size()`

    Args:
        dims: Dimensions as `tuple`, `list` or `Shape`.

    Returns:
        `tuple`
    &#34;&#34;&#34;
    assert isinstance(dims, (tuple, list, Shape)), f&#34;get_sizes() requires a sequence of dimensions but got {dims}&#34;
    return tuple([self.get_size(dim) for dim in dims])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.get_type"><code class="name flex">
<span>def <span class="ident">get_type</span></span>(<span>self, dim: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_type(self, dim: str or &#39;Shape&#39;) -&gt; str:
    # undocumented, use get_dim_type() instead.
    if isinstance(dim, str):
        return self.types[self.names.index(dim)]
    elif isinstance(dim, Shape):
        assert dim.rank == 1, f&#34;Shape.get_type() only accepts single-dimension Shapes but got {dim}&#34;
        return self.types[self.names.index(dim.name)]
    else:
        raise ValueError(dim)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.get_types"><code class="name flex">
<span>def <span class="ident">get_types</span></span>(<span>self, dims: tuple) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_types(self, dims: tuple or list or &#39;Shape&#39;) -&gt; tuple:
    # undocumented, do not use
    if isinstance(dims, (tuple, list)):
        return tuple(self.get_type(n) for n in dims)
    elif isinstance(dims, Shape):
        return tuple(self.get_type(n) for n in dims.names)
    else:
        raise ValueError(dims)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.index"><code class="name flex">
<span>def <span class="ident">index</span></span>(<span>self, dim: str) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the index of the dimension within this <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</p>
<p>See Also:
<code><a title="phi.math.Shape.indices" href="#phi.math.Shape.indices">Shape.indices()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension name or single-dimension <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Index as <code>int</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def index(self, dim: str or &#39;Shape&#39; or None) -&gt; int:
    &#34;&#34;&#34;
    Finds the index of the dimension within this `Shape`.

    See Also:
        `Shape.indices()`.

    Args:
        dim: Dimension name or single-dimension `Shape`.

    Returns:
        Index as `int`.
    &#34;&#34;&#34;
    if dim is None:
        return None
    elif isinstance(dim, str):
        if dim not in self.names:
            raise ValueError(f&#34;Shape {self} has no dimension &#39;{dim}&#39;&#34;)
        return self.names.index(dim)
    elif isinstance(dim, Shape):
        assert dim.rank == 1, f&#34;index() requires a single dimension as input but got {dim}. Use indices() for multiple dimensions.&#34;
        return self.names.index(dim.name)
    else:
        raise ValueError(f&#34;index() requires a single dimension as input but got {dim}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.indices"><code class="name flex">
<span>def <span class="ident">indices</span></span>(<span>self, dims: tuple) ‑> Tuple[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the indices of the given dimensions within this <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</p>
<p>See Also:
<code><a title="phi.math.Shape.index" href="#phi.math.Shape.index">Shape.index()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>Sequence of dimensions as <code>tuple</code>, <code>list</code> or <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Indices as <code>tuple[int]</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def indices(self, dims: tuple or list or &#39;Shape&#39;) -&gt; Tuple[int]:
    &#34;&#34;&#34;
    Finds the indices of the given dimensions within this `Shape`.

    See Also:
        `Shape.index()`.

    Args:
        dims: Sequence of dimensions as `tuple`, `list` or `Shape`.

    Returns:
        Indices as `tuple[int]`.
    &#34;&#34;&#34;
    if isinstance(dims, (list, tuple)):
        return tuple([self.index(n) for n in dims])
    elif isinstance(dims, Shape):
        return tuple([self.index(n) for n in dims.names])
    else:
        raise ValueError(f&#34;indices() requires a sequence of dimensions but got {dims}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.mask"><code class="name flex">
<span>def <span class="ident">mask</span></span>(<span>self, names: tuple)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a binary sequence corresponding to the names of this Shape.
A value of 1 means that a dimension of this Shape is contained in <code>names</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>names</code></strong></dt>
<dd>instance of dimension</dd>
<dt><strong><code>names</code></strong></dt>
<dd>tuple or list or set: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>binary sequence</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mask(self, names: tuple or list or set or &#39;Shape&#39;):
    &#34;&#34;&#34;
    Returns a binary sequence corresponding to the names of this Shape.
    A value of 1 means that a dimension of this Shape is contained in `names`.

    Args:
      names: instance of dimension
      names: tuple or list or set: 

    Returns:
      binary sequence

    &#34;&#34;&#34;
    if isinstance(names, str):
        names = [names]
    elif isinstance(names, Shape):
        names = names.names
    mask = [1 if name in names else 0 for name in self.names]
    return tuple(mask)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.meshgrid"><code class="name flex">
<span>def <span class="ident">meshgrid</span></span>(<span>self, names=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a sequence containing all multi-indices within a tensor of this shape.
All indices are returned as <code>dict</code> mapping dimension names to <code>int</code> indices.</p>
<p>The corresponding values can be retrieved from Tensors and other Sliceables using <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a>[index]</code>.</p>
<p>This function currently only supports uniform tensors.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>names</code></strong></dt>
<dd>If <code>True</code>, replace indices by their item names if available.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>dict</code> iterator.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def meshgrid(self, names=False):
    &#34;&#34;&#34;
    Builds a sequence containing all multi-indices within a tensor of this shape.
    All indices are returned as `dict` mapping dimension names to `int` indices.

    The corresponding values can be retrieved from Tensors and other Sliceables using `tensor[index]`.

    This function currently only supports uniform tensors.

    Args:
        names: If `True`, replace indices by their item names if available.

    Returns:
        `dict` iterator.
    &#34;&#34;&#34;
    assert self.is_uniform, f&#34;Shape.meshgrid() is currently not supported for non-uniform tensors, {self}&#34;
    indices = [0] * self.rank
    while True:
        if names:
            yield {dim: (names[index] if names is not None else index) for dim, index, names in zip(self.names, indices, self.item_names)}
        else:
            yield {dim: index for dim, index in zip(self.names, indices)}
        for i in range(self.rank-1, -1, -1):
            indices[i] = (indices[i] + 1) % self.sizes[i]
            if indices[i] != 0:
                break
        else:
            return</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.only"><code class="name flex">
<span>def <span class="ident">only</span></span>(<span>self, dims: DimFilter)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new shape from this one that only contains the given dimensions.
Dimensions in <code>dims</code> that are not part of this Shape are ignored.</p>
<p>The complementary operation is :func:<code><a title="phi.math.Shape.without" href="#phi.math.Shape.without">Shape.without()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>comma-separated dimension names (str) or instance of dimensions (tuple, list, Shape) or filter function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape containing only specified dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def only(self, dims: &#39;DimFilter&#39;):
    &#34;&#34;&#34;
    Builds a new shape from this one that only contains the given dimensions.
    Dimensions in `dims` that are not part of this Shape are ignored.
    
    The complementary operation is :func:`Shape.without`.

    Args:
      dims: comma-separated dimension names (str) or instance of dimensions (tuple, list, Shape) or filter function.

    Returns:
      Shape containing only specified dimensions

    &#34;&#34;&#34;
    if callable(dims):
        dims = dims(self)
    if isinstance(dims, str):
        dims = parse_dim_order(dims)
    if isinstance(dims, (tuple, list)):
        return self[[i for i in range(self.rank) if self.names[i] in dims]]
    elif isinstance(dims, Shape):
        return self[[i for i in range(self.rank) if self.names[i] in dims.names]]
    elif dims is None:  # keep none
        return EMPTY_SHAPE
    else:
        raise ValueError(dims)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.prepare_gather"><code class="name flex">
<span>def <span class="ident">prepare_gather</span></span>(<span>self, dim: str, selection)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_gather(self, dim: str, selection):
    if isinstance(selection, Shape):
        selection = selection.name if selection.rank == 1 else selection.names
    if isinstance(selection, str) and &#39;,&#39; in selection:
        selection = parse_dim_order(selection)
    if isinstance(selection, str):  # single item name
        item_names = self.get_item_names(dim, fallback_spatial=True)
        assert item_names is not None, f&#34;No item names defined for dim &#39;{dim}&#39; in tensor {self.shape} and dimension size does not match spatial rank.&#34;
        assert selection in item_names, f&#34;Accessing tensor.{dim}[&#39;{selection}&#39;] failed. Item names are {item_names}.&#34;
        selection = item_names.index(selection)
    if isinstance(selection, (tuple, list)):
        selection = list(selection)
        if any([isinstance(s, str) for s in selection]):
            item_names = self.get_item_names(dim, fallback_spatial=True)
            for i, s in enumerate(selection):
                if isinstance(s, str):
                    assert item_names is not None, f&#34;Accessing tensor.{dim}[&#39;{s}&#39;] failed because no item names are present on tensor {self.shape}&#34;
                    assert s in item_names, f&#34;Accessing tensor.{dim}[&#39;{s}&#39;] failed. Item names are {item_names}.&#34;
                    selection[i] = item_names.index(s)
        if not selection:  # empty
            selection = slice(0, 0)
    return selection</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.replace"><code class="name flex">
<span>def <span class="ident">replace</span></span>(<span>self, dims: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>, new: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a copy of <code>self</code> with <code>dims</code> replaced by <code>new</code>.
Dimensions that are not present in <code>self</code> are ignored.</p>
<p>The dimension order is preserved.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions to replace.</dd>
<dt><strong><code>new</code></strong></dt>
<dd>New dimensions, must have same length as <code>dims</code>.
If a <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> is given, replaces the dimension types and item names as well.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> with same rank and dimension order as <code>self</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def replace(self, dims: &#39;Shape&#39; or str or tuple or list, new: &#39;Shape&#39;) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Returns a copy of `self` with `dims` replaced by `new`.
    Dimensions that are not present in `self` are ignored.

    The dimension order is preserved.

    Args:
        dims: Dimensions to replace.
        new: New dimensions, must have same length as `dims`.
            If a `Shape` is given, replaces the dimension types and item names as well.

    Returns:
        `Shape` with same rank and dimension order as `self`.
    &#34;&#34;&#34;
    dims = parse_dim_order(dims)
    assert isinstance(new, Shape), f&#34;new must be a Shape but got {new}&#34;
    names = list(self.names)
    sizes = list(self.sizes)
    types = list(self.types)
    item_names = list(self.item_names)
    for old_name, new_dim in zip(dims, new):
        if old_name in self:
            names[self.index(old_name)] = new_dim.name
            types[self.index(old_name)] = new_dim.type
            item_names[self.index(old_name)] = new_dim.item_names[0]
            sizes[self.index(old_name)] = new_dim.size
    return Shape(tuple(sizes), tuple(names), tuple(types), tuple(item_names))</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>self, dim='dims') ‑> Tuple[phi.math._shape.Shape]</span>
</code></dt>
<dd>
<div class="desc"><p>Slices this <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> along a dimension.
The dimension listing the sizes of the shape is referred to as <code>'dims'</code>.</p>
<p>Non-uniform tensor shapes may be unstacked along other dimensions as well, see
<a href="https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors">https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>dimension to unstack</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>slices of this shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack(self, dim=&#39;dims&#39;) -&gt; Tuple[&#39;Shape&#39;]:
    &#34;&#34;&#34;
    Slices this `Shape` along a dimension.
    The dimension listing the sizes of the shape is referred to as `&#39;dims&#39;`.

    Non-uniform tensor shapes may be unstacked along other dimensions as well, see
    https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors

    Args:
        dim: dimension to unstack

    Returns:
        slices of this shape
    &#34;&#34;&#34;
    if dim == &#39;dims&#39;:
        return tuple(Shape((self.sizes[i],), (self.names[i],), (self.types[i],), (self.item_names[i],)) for i in range(self.rank))
    if dim not in self:
        return tuple([self])
    else:
        from ._tensors import Tensor
        inner = self.without(dim)
        sizes = []
        dim_size = self.get_size(dim)
        for size in inner.sizes:
            if isinstance(size, Tensor) and dim in size.shape:
                sizes.append(size.unstack(dim))
                dim_size = size.shape.get_size(dim)
            else:
                sizes.append(size)
        assert isinstance(dim_size, int)
        shapes = tuple(Shape(tuple([int(size[i]) if isinstance(size, tuple) else size for size in sizes]), inner.names, inner.types, inner.item_names) for i in range(dim_size))
        return shapes</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_dim_size"><code class="name flex">
<span>def <span class="ident">with_dim_size</span></span>(<span>self, dim: str, size: int, keep_item_names=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> that has a different size for <code>dim</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension for which to replace the size, <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> or <code>str</code>.</dd>
<dt><strong><code>size</code></strong></dt>
<dd>New size, <code>int</code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> with same names and types as <code>self</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_dim_size(self, dim: str or &#39;Shape&#39;, size: int or &#39;math.Tensor&#39; or str or tuple or list, keep_item_names=True):
    &#34;&#34;&#34;
    Returns a new `Shape` that has a different size for `dim`.

    Args:
        dim: Dimension for which to replace the size, `Shape` or `str`.
        size: New size, `int` or `Tensor`

    Returns:
        `Shape` with same names and types as `self`.
    &#34;&#34;&#34;
    if isinstance(dim, Shape):
        dim = dim.name
    assert isinstance(dim, str)
    new_size, new_item_names = Shape._size_and_item_names_from_obj(size, self.get_size(dim), self.get_item_names(dim), keep_item_names)
    return self.replace(dim, Shape((new_size,), (dim,), (self.get_type(dim),), (new_item_names,)))</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_size"><code class="name flex">
<span>def <span class="ident">with_size</span></span>(<span>self, size: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Only for single-dimension shapes.
Returns a <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> representing this dimension but with a different size.</p>
<p>See Also:
<code><a title="phi.math.Shape.with_sizes" href="#phi.math.Shape.with_sizes">Shape.with_sizes()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong></dt>
<dd>Replacement size for this dimension.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_size(self, size: int or None):
    &#34;&#34;&#34;
    Only for single-dimension shapes.
    Returns a `Shape` representing this dimension but with a different size.

    See Also:
        `Shape.with_sizes()`.

    Args:
        size: Replacement size for this dimension.

    Returns:
        `Shape`
    &#34;&#34;&#34;
    assert self.rank == 1, &#34;Shape.with_size() is only defined for shapes of rank 1.&#34;
    return self.with_sizes([size])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_sizes"><code class="name flex">
<span>def <span class="ident">with_sizes</span></span>(<span>self, sizes: tuple, keep_item_names=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> matching the dimension names and types of <code>self</code> but with different sizes.</p>
<p>See Also:
<code><a title="phi.math.Shape.with_size" href="#phi.math.Shape.with_size">Shape.with_size()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sizes</code></strong></dt>
<dd>
<p>One of</p>
<ul>
<li><code>tuple</code> / <code>list</code> of same length as <code>self</code> containing replacement sizes.</li>
<li><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> of any rank. Replaces sizes for dimensions shared by <code>sizes</code> and <code>self</code>.</li>
</ul>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> with same names and types as <code>self</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_sizes(self, sizes: tuple or list or &#39;Shape&#39;, keep_item_names=True):
    &#34;&#34;&#34;
    Returns a new `Shape` matching the dimension names and types of `self` but with different sizes.

    See Also:
        `Shape.with_size()`.

    Args:
        sizes: One of

            * `tuple` / `list` of same length as `self` containing replacement sizes.
            * `Shape` of any rank. Replaces sizes for dimensions shared by `sizes` and `self`.

    Returns:
        `Shape` with same names and types as `self`.
    &#34;&#34;&#34;
    if isinstance(sizes, Shape):
        item_names = [sizes.get_item_names(dim) if dim in sizes else self.get_item_names(dim) for dim in self.names]
        sizes = [sizes.get_size(dim) if dim in sizes else s for dim, s in self._named_sizes]
        return Shape(tuple(sizes), self.names, self.types, tuple(item_names))
    else:
        assert len(sizes) == len(self.sizes), f&#34;Cannot create shape from {self} with sizes {sizes}&#34;
        sizes_ = []
        item_names = []
        for i, obj in enumerate(sizes):
            new_size, new_item_names = Shape._size_and_item_names_from_obj(obj, self.sizes[i], self.item_names[i], keep_item_names)
            sizes_.append(new_size)
            item_names.append(new_item_names)
        return Shape(tuple(sizes_), self.names, self.types, tuple(item_names))</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.without"><code class="name flex">
<span>def <span class="ident">without</span></span>(<span>self, dims: DimFilter) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new shape from this one that is missing all given dimensions.
Dimensions in <code>dims</code> that are not part of this Shape are ignored.</p>
<p>The complementary operation is <code><a title="phi.math.Shape.only" href="#phi.math.Shape.only">Shape.only()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>Single dimension (str) or instance of dimensions (tuple, list, Shape)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions to exclude as <code>str</code> or <code>tuple</code> or <code>list</code> or <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>. Dimensions that are not included in this shape are ignored.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape without specified dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def without(self, dims: &#39;DimFilter&#39;) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Builds a new shape from this one that is missing all given dimensions.
    Dimensions in `dims` that are not part of this Shape are ignored.
    
    The complementary operation is `Shape.only()`.

    Args:
      dims: Single dimension (str) or instance of dimensions (tuple, list, Shape)
      dims: Dimensions to exclude as `str` or `tuple` or `list` or `Shape`. Dimensions that are not included in this shape are ignored.

    Returns:
      Shape without specified dimensions
    &#34;&#34;&#34;
    if callable(dims):
        dims = dims(self)
    if isinstance(dims, str):
        dims = parse_dim_order(dims)
    if isinstance(dims, (tuple, list)):
        return self[[i for i in range(self.rank) if self.names[i] not in dims]]
    elif isinstance(dims, Shape):
        return self[[i for i in range(self.rank) if self.names[i] not in dims.names]]
    elif dims is None:  # subtract none
        return self
    else:
        raise ValueError(dims)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.without_sizes"><code class="name flex">
<span>def <span class="ident">without_sizes</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> with all sizes undefined (<code>None</code>)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def without_sizes(self):
    &#34;&#34;&#34;
    Returns:
        `Shape` with all sizes undefined (`None`)
    &#34;&#34;&#34;
    return Shape((None,) * self.rank, self.names, self.types, (None,) * self.rank)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.Solve"><code class="flex name class">
<span>class <span class="ident">Solve</span></span>
<span>(</span><span>method: str, relative_tolerance: float, absolute_tolerance: float, max_iterations: int = 1000, x0: ~X = None, suppress: tuple = (), preprocess_y: Callable = None, preprocess_y_args: tuple = (), gradient_solve: <a title="phi.math.Solve" href="#phi.math.Solve">Solve</a>[Y, X] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Specifies parameters and stopping criteria for solving a minimization problem or system of equations.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Solve(Generic[X, Y]):  # TODO move to phi.math._functional, put Tensors there
    &#34;&#34;&#34;
    Specifies parameters and stopping criteria for solving a minimization problem or system of equations.
    &#34;&#34;&#34;

    def __init__(self,
                 method: str,
                 relative_tolerance: float or Tensor,
                 absolute_tolerance: float or Tensor,
                 max_iterations: int or Tensor = 1000,
                 x0: X or Any = None,
                 suppress: tuple or list = (),
                 preprocess_y: Callable = None,
                 preprocess_y_args: tuple = (),
                 gradient_solve: &#39;Solve[Y, X]&#39; or None = None):
        assert isinstance(method, str)
        self.method: str = method
        &#34;&#34;&#34; Optimization method to use. Available solvers depend on the solve function that is used to perform the solve. &#34;&#34;&#34;
        self.relative_tolerance: Tensor = math.to_float(wrap(relative_tolerance))
        &#34;&#34;&#34; Relative tolerance for linear solves only. This must be `0` for minimization problems.
        For systems of equations *f(x)=y*, the final tolerance is `max(relative_tolerance * norm(y), absolute_tolerance)`. &#34;&#34;&#34;
        self.absolute_tolerance: Tensor = math.to_float(wrap(absolute_tolerance))
        &#34;&#34;&#34; Absolut tolerance for optimization problems and linear solves.
        For systems of equations *f(x)=y*, the final tolerance is `max(relative_tolerance * norm(y), absolute_tolerance)`. &#34;&#34;&#34;
        self.max_iterations: Tensor = math.to_int32(wrap(max_iterations))
        &#34;&#34;&#34; Maximum number of iterations to perform before raising a `NotConverged` error is raised. &#34;&#34;&#34;
        self.x0 = x0
        &#34;&#34;&#34; Initial guess for the method, of same type and dimensionality as the solve result.
         This property must be set to a value compatible with the solution `x` before running a method. &#34;&#34;&#34;
        self.preprocess_y: Callable = preprocess_y
        &#34;&#34;&#34; Function to be applied to the right-hand-side vector of an equation system before solving the system.
        This property is propagated to gradient solves by default. &#34;&#34;&#34;
        self.preprocess_y_args: tuple = preprocess_y_args
        assert all(issubclass(err, ConvergenceException) for err in suppress)
        self.suppress: tuple = tuple(suppress)
        &#34;&#34;&#34; Error types to suppress; `tuple` of `ConvergenceException` types. For these errors, the solve function will instead return the partial result without raising the error. &#34;&#34;&#34;
        self._gradient_solve: Solve[Y, X] = gradient_solve
        self.id = str(uuid.uuid4())

    @property
    def gradient_solve(self) -&gt; &#39;Solve[Y, X]&#39;:
        &#34;&#34;&#34;
        Parameters to use for the gradient pass when an implicit gradient is computed.
        If `None`, a duplicate of this `Solve` is created for the gradient solve.

        In any case, the gradient solve information will be stored in `gradient_solve.result`.
        &#34;&#34;&#34;
        if self._gradient_solve is None:
            self._gradient_solve = Solve(self.method, self.relative_tolerance, self.absolute_tolerance, self.max_iterations, None, self.suppress, self.preprocess_y, self.preprocess_y_args)
        return self._gradient_solve

    def __repr__(self):
        return f&#34;{self.method} with tolerance {self.relative_tolerance} (rel), {self.absolute_tolerance} (abs), max_iterations={self.max_iterations}&#34;

    def __eq__(self, other):
        if not isinstance(other, Solve):
            return False
        if self.method != other.method \
                or (self.absolute_tolerance != other.absolute_tolerance).any \
                or (self.relative_tolerance != other.relative_tolerance).any \
                or (self.max_iterations != other.max_iterations).any \
                or self.preprocess_y is not other.preprocess_y \
                or self.suppress != other.suppress:
            return False
        return self.x0 == other.x0

    def __variable_attrs__(self):
        return &#39;x0&#39;, &#39;preprocess_y_args&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>typing.Generic</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.Solve.absolute_tolerance"><code class="name">var <span class="ident">absolute_tolerance</span></code></dt>
<dd>
<div class="desc"><p>Absolut tolerance for optimization problems and linear solves.
For systems of equations <em>f(x)=y</em>, the final tolerance is <code>max(relative_tolerance * norm(y), absolute_tolerance)</code>.</p></div>
</dd>
<dt id="phi.math.Solve.gradient_solve"><code class="name">var <span class="ident">gradient_solve</span> : phi.math._functional.Solve[~Y, ~X]</code></dt>
<dd>
<div class="desc"><p>Parameters to use for the gradient pass when an implicit gradient is computed.
If <code>None</code>, a duplicate of this <code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code> is created for the gradient solve.</p>
<p>In any case, the gradient solve information will be stored in <code>gradient_solve.result</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def gradient_solve(self) -&gt; &#39;Solve[Y, X]&#39;:
    &#34;&#34;&#34;
    Parameters to use for the gradient pass when an implicit gradient is computed.
    If `None`, a duplicate of this `Solve` is created for the gradient solve.

    In any case, the gradient solve information will be stored in `gradient_solve.result`.
    &#34;&#34;&#34;
    if self._gradient_solve is None:
        self._gradient_solve = Solve(self.method, self.relative_tolerance, self.absolute_tolerance, self.max_iterations, None, self.suppress, self.preprocess_y, self.preprocess_y_args)
    return self._gradient_solve</code></pre>
</details>
</dd>
<dt id="phi.math.Solve.max_iterations"><code class="name">var <span class="ident">max_iterations</span></code></dt>
<dd>
<div class="desc"><p>Maximum number of iterations to perform before raising a <code><a title="phi.math.NotConverged" href="#phi.math.NotConverged">NotConverged</a></code> error is raised.</p></div>
</dd>
<dt id="phi.math.Solve.method"><code class="name">var <span class="ident">method</span></code></dt>
<dd>
<div class="desc"><p>Optimization method to use. Available solvers depend on the solve function that is used to perform the solve.</p></div>
</dd>
<dt id="phi.math.Solve.preprocess_y"><code class="name">var <span class="ident">preprocess_y</span></code></dt>
<dd>
<div class="desc"><p>Function to be applied to the right-hand-side vector of an equation system before solving the system.
This property is propagated to gradient solves by default.</p></div>
</dd>
<dt id="phi.math.Solve.relative_tolerance"><code class="name">var <span class="ident">relative_tolerance</span></code></dt>
<dd>
<div class="desc"><p>Relative tolerance for linear solves only. This must be <code>0</code> for minimization problems.
For systems of equations <em>f(x)=y</em>, the final tolerance is <code>max(relative_tolerance * norm(y), absolute_tolerance)</code>.</p></div>
</dd>
<dt id="phi.math.Solve.suppress"><code class="name">var <span class="ident">suppress</span></code></dt>
<dd>
<div class="desc"><p>Error types to suppress; <code>tuple</code> of <code><a title="phi.math.ConvergenceException" href="#phi.math.ConvergenceException">ConvergenceException</a></code> types. For these errors, the solve function will instead return the partial result without raising the error.</p></div>
</dd>
<dt id="phi.math.Solve.x0"><code class="name">var <span class="ident">x0</span></code></dt>
<dd>
<div class="desc"><p>Initial guess for the method, of same type and dimensionality as the solve result.
This property must be set to a value compatible with the solution <code>x</code> before running a method.</p></div>
</dd>
</dl>
</dd>
<dt id="phi.math.SolveInfo"><code class="flex name class">
<span>class <span class="ident">SolveInfo</span></span>
</code></dt>
<dd>
<div class="desc"><p>Stores information about the solution or trajectory of a solve.</p>
<p>When representing the full optimization trajectory, all tracked quantities will have an additional <code>trajectory</code> batch dimension.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SolveInfo(Generic[X, Y]):
    &#34;&#34;&#34;
    Stores information about the solution or trajectory of a solve.

    When representing the full optimization trajectory, all tracked quantities will have an additional `trajectory` batch dimension.
    &#34;&#34;&#34;

    def __init__(self,
                 solve: Solve,
                 x: X,
                 residual: Y or None,
                 iterations: Tensor or None,
                 function_evaluations: Tensor or None,
                 converged: Tensor,
                 diverged: Tensor,
                 method: str,
                 msg: str,
                 solve_time: float):
        # tuple.__new__(SolveInfo, (x, residual, iterations, function_evaluations, converged, diverged))
        self.solve: Solve[X, Y] = solve
        &#34;&#34;&#34; `Solve`, Parameters specified for the solve. &#34;&#34;&#34;
        self.x: X = x
        &#34;&#34;&#34; `Tensor` or `PhiTreeNode`, solution estimate. &#34;&#34;&#34;
        self.residual: Y = residual
        &#34;&#34;&#34; `Tensor` or `PhiTreeNode`, residual vector for systems of equations or function value for minimization problems. &#34;&#34;&#34;
        self.iterations: Tensor = iterations
        &#34;&#34;&#34; `Tensor`, number of performed iterations to reach this state. &#34;&#34;&#34;
        self.function_evaluations: Tensor = function_evaluations
        &#34;&#34;&#34; `Tensor`, how often the function (or its gradient function) was called. &#34;&#34;&#34;
        self.converged: Tensor = converged
        &#34;&#34;&#34; `Tensor`, whether the residual is within the specified tolerance. &#34;&#34;&#34;
        self.diverged: Tensor = diverged
        &#34;&#34;&#34; `Tensor`, whether the solve has diverged at this point. &#34;&#34;&#34;
        self.method = method
        &#34;&#34;&#34; `str`, which method and implementation that was used. &#34;&#34;&#34;
        if not msg and all_available(diverged, converged):
            if self.diverged.any:
                msg = f&#34;Solve diverged within {iterations if iterations is not None else &#39;?&#39;} iterations using {method}.&#34;
            elif not self.converged.trajectory[-1].all:
                msg = f&#34;Solve did not converge to rel={solve.relative_tolerance}, abs={solve.absolute_tolerance} within {solve.max_iterations} iterations using {method}. Max residual: {[math.max_(t.trajectory[-1]) for t in disassemble_tree(self.residual)[1]]}&#34;
            else:
                msg = f&#34;Converged within {iterations if iterations is not None else &#39;?&#39;} iterations.&#34;
        self.msg = msg
        &#34;&#34;&#34; `str`, termination message &#34;&#34;&#34;
        self.solve_time = solve_time
        &#34;&#34;&#34; Time spent in Backend solve function (in seconds) &#34;&#34;&#34;

    def __repr__(self):
        return self.msg

    def snapshot(self, index):
        return SolveInfo(self.solve, self.x.trajectory[index], self.residual.trajectory[index], self.iterations.trajectory[index], self.function_evaluations.trajectory[index],
                         self.converged.trajectory[index], self.diverged.trajectory[index], self.method, self.msg, self.solve_time)

    def convergence_check(self, only_warn: bool):
        if not all_available(self.diverged, self.converged):
            return
        if self.diverged.any:
            if Diverged not in self.solve.suppress:
                if only_warn:
                    warnings.warn(self.msg, ConvergenceWarning)
                else:
                    raise Diverged(self)
        if not self.converged.trajectory[-1].all:
            if NotConverged not in self.solve.suppress:
                if only_warn:
                    warnings.warn(self.msg, ConvergenceWarning)
                else:
                    raise NotConverged(self)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>typing.Generic</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.SolveInfo.converged"><code class="name">var <span class="ident">converged</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, whether the residual is within the specified tolerance.</p></div>
</dd>
<dt id="phi.math.SolveInfo.diverged"><code class="name">var <span class="ident">diverged</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, whether the solve has diverged at this point.</p></div>
</dd>
<dt id="phi.math.SolveInfo.function_evaluations"><code class="name">var <span class="ident">function_evaluations</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, how often the function (or its gradient function) was called.</p></div>
</dd>
<dt id="phi.math.SolveInfo.iterations"><code class="name">var <span class="ident">iterations</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, number of performed iterations to reach this state.</p></div>
</dd>
<dt id="phi.math.SolveInfo.method"><code class="name">var <span class="ident">method</span></code></dt>
<dd>
<div class="desc"><p><code>str</code>, which method and implementation that was used.</p></div>
</dd>
<dt id="phi.math.SolveInfo.msg"><code class="name">var <span class="ident">msg</span></code></dt>
<dd>
<div class="desc"><p><code>str</code>, termination message</p></div>
</dd>
<dt id="phi.math.SolveInfo.residual"><code class="name">var <span class="ident">residual</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code>, residual vector for systems of equations or function value for minimization problems.</p></div>
</dd>
<dt id="phi.math.SolveInfo.solve"><code class="name">var <span class="ident">solve</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code>, Parameters specified for the solve.</p></div>
</dd>
<dt id="phi.math.SolveInfo.solve_time"><code class="name">var <span class="ident">solve_time</span></code></dt>
<dd>
<div class="desc"><p>Time spent in Backend solve function (in seconds)</p></div>
</dd>
<dt id="phi.math.SolveInfo.x"><code class="name">var <span class="ident">x</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>PhiTreeNode</code>, solution estimate.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.SolveInfo.convergence_check"><code class="name flex">
<span>def <span class="ident">convergence_check</span></span>(<span>self, only_warn: bool)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convergence_check(self, only_warn: bool):
    if not all_available(self.diverged, self.converged):
        return
    if self.diverged.any:
        if Diverged not in self.solve.suppress:
            if only_warn:
                warnings.warn(self.msg, ConvergenceWarning)
            else:
                raise Diverged(self)
    if not self.converged.trajectory[-1].all:
        if NotConverged not in self.solve.suppress:
            if only_warn:
                warnings.warn(self.msg, ConvergenceWarning)
            else:
                raise NotConverged(self)</code></pre>
</details>
</dd>
<dt id="phi.math.SolveInfo.snapshot"><code class="name flex">
<span>def <span class="ident">snapshot</span></span>(<span>self, index)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def snapshot(self, index):
    return SolveInfo(self.solve, self.x.trajectory[index], self.residual.trajectory[index], self.iterations.trajectory[index], self.function_evaluations.trajectory[index],
                     self.converged.trajectory[index], self.diverged.trajectory[index], self.method, self.msg, self.solve_time)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.SolveTape"><code class="flex name class">
<span>class <span class="ident">SolveTape</span></span>
<span>(</span><span>record_trajectories=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Used to record additional information about solves invoked via <code><a title="phi.math.solve_linear" href="#phi.math.solve_linear">solve_linear()</a></code>, <code><a title="phi.math.solve_nonlinear" href="#phi.math.solve_nonlinear">solve_nonlinear()</a></code> or <code><a title="phi.math.minimize" href="#phi.math.minimize">minimize()</a></code>.
While a <code><a title="phi.math.SolveTape" href="#phi.math.SolveTape">SolveTape</a></code> is active, certain performance optimizations and algorithm implementations may be disabled.</p>
<p>To access a <code><a title="phi.math.SolveInfo" href="#phi.math.SolveInfo">SolveInfo</a></code> of a recorded solve, use</p>
<pre><code class="language-python">solve = Solve(method, ...)
with SolveTape() as solves:
    x = math.solve_linear(f, y, solve)
result: SolveInfo = solves[solve]  # get by Solve
result: SolveInfo = solves[0]  # get by index
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>record_trajectories</code></strong></dt>
<dd>When enabled, the entries of <code><a title="phi.math.SolveInfo" href="#phi.math.SolveInfo">SolveInfo</a></code> will contain an additional batch dimension named <code>trajectory</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SolveTape:
    &#34;&#34;&#34;
    Used to record additional information about solves invoked via `solve_linear()`, `solve_nonlinear()` or `minimize()`.
    While a `SolveTape` is active, certain performance optimizations and algorithm implementations may be disabled.

    To access a `SolveInfo` of a recorded solve, use
    ```python
    solve = Solve(method, ...)
    with SolveTape() as solves:
        x = math.solve_linear(f, y, solve)
    result: SolveInfo = solves[solve]  # get by Solve
    result: SolveInfo = solves[0]  # get by index
    ```
    &#34;&#34;&#34;

    def __init__(self, record_trajectories=False):
        &#34;&#34;&#34;
        Args:
            record_trajectories: When enabled, the entries of `SolveInfo` will contain an additional batch dimension named `trajectory`.
        &#34;&#34;&#34;
        self.record_trajectories = record_trajectories
        self.solves: List[SolveInfo] = []
        self.solve_ids: List[str] = []

    def __enter__(self):
        _SOLVE_TAPES.append(self)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        _SOLVE_TAPES.remove(self)

    def _add(self, solve: Solve, trj: bool, result: SolveInfo):
        if any(s.solve.id == solve.id for s in self.solves):
            warnings.warn(&#34;SolveTape contains two results for the same solve settings. SolveTape[solve] will return the first solve result.&#34;, RuntimeWarning)
        if self.record_trajectories:
            assert trj, &#34;Solve did not record a trajectory.&#34;
            self.solves.append(result)
        elif trj:
            self.solves.append(result.snapshot(-1))
        else:
            self.solves.append(result)
        self.solve_ids.append(solve.id)

    def __getitem__(self, item) -&gt; SolveInfo:
        if isinstance(item, int):
            return self.solves[item]
        else:
            assert isinstance(item, Solve)
            solves = [s for s in self.solves if s.solve.id == item.id]
            if len(solves) == 0:
                raise KeyError(f&#34;No solve recorded with key &#39;{item}&#39;.&#34;)
            assert len(solves) == 1
            return solves[0]

    def __iter__(self):
        return iter(self.solves)

    def __len__(self):
        return len(self.solves)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor"><code class="flex name class">
<span>class <span class="ident">Tensor</span></span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base class to represent structured data of one data type.
This class replaces the native tensor classes <code>numpy.ndarray</code>, <code>torch.Tensor</code>, <code>tensorflow.Tensor</code> or <code>jax.numpy.ndarray</code> as the main data container in Φ<sub>Flow</sub>.</p>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> instances are different from native tensors in two important ways:</p>
<ul>
<li>The dimensions of Tensors have <em>names</em> and <em>types</em>.</li>
<li>Tensors can have non-uniform shapes, meaning that the size of dimensions can vary along other dimensions.</li>
</ul>
<p>To check whether a value is a tensor, use <code>isinstance(value, <a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a>)</code>.</p>
<p>To construct a Tensor, use <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code>, <code><a title="phi.math.wrap" href="#phi.math.wrap">wrap()</a></code> or one of the basic tensor creation functions,
see <a href="https://tum-pbs.github.io/PhiFlow/Math.html#tensor-creation">https://tum-pbs.github.io/PhiFlow/Math.html#tensor-creation</a> .</p>
<p>Tensors are not editable.
When backed by an editable native tensor, e.g. a <code>numpy.ndarray</code>, do not edit the underlying data structure.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Tensor:
    &#34;&#34;&#34;
    Abstract base class to represent structured data of one data type.
    This class replaces the native tensor classes `numpy.ndarray`, `torch.Tensor`, `tensorflow.Tensor` or `jax.numpy.ndarray` as the main data container in Φ&lt;sub&gt;Flow&lt;/sub&gt;.

    `Tensor` instances are different from native tensors in two important ways:

    * The dimensions of Tensors have *names* and *types*.
    * Tensors can have non-uniform shapes, meaning that the size of dimensions can vary along other dimensions.

    To check whether a value is a tensor, use `isinstance(value, Tensor)`.

    To construct a Tensor, use `phi.math.tensor()`, `phi.math.wrap()` or one of the basic tensor creation functions,
    see https://tum-pbs.github.io/PhiFlow/Math.html#tensor-creation .

    Tensors are not editable.
    When backed by an editable native tensor, e.g. a `numpy.ndarray`, do not edit the underlying data structure.
    &#34;&#34;&#34;

    def native(self, order: str or tuple or list or Shape = None):
        &#34;&#34;&#34;
        Returns a native tensor object with the dimensions ordered according to `order`.
        
        Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
        If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

        Args:
            order: (Optional) Order of dimension names as comma-separated string, list or `Shape`.

        Returns:
            Native tensor representation, such as PyTorch tensor or NumPy array.

        Raises:
            ValueError if the tensor cannot be transposed to match target_shape
        &#34;&#34;&#34;
        raise NotImplementedError()

    def numpy(self, order: str or tuple or list or Shape = None) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Converts this tensor to a `numpy.ndarray` with dimensions ordered according to `order`.
        
        *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
        To get a differentiable tensor, use `Tensor.native()` instead.
        
        Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
        If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

        If this `Tensor` is backed by a NumPy array, a reference to this array may be returned.

        See Also:
            `phi.math.numpy()`

        Args:
            order: (Optional) Order of dimension names as comma-separated string, list or `Shape`.

        Returns:
            NumPy representation

        Raises:
            ValueError if the tensor cannot be transposed to match target_shape
        &#34;&#34;&#34;
        native = self.native(order=order)
        return choose_backend(native).numpy(native)

    def __array__(self, dtype=None):  # NumPy conversion
        if self.rank &gt; 1:
            warnings.warn(&#34;Automatic conversion of Φ-Flow tensors to NumPy can cause problems because the dimension order is not guaranteed.&#34;, SyntaxWarning, stacklevel=3)
        return self.numpy(self._shape)

    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):  # NumPy interface
        if len(inputs) != 2:
            return NotImplemented
        if ufunc.__name__ == &#39;multiply&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x * y, lambda x, y: choose_backend(x, y).mul(x, y), &#39;mul&#39;, &#39;*&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y * x, lambda x, y: choose_backend(x, y).mul(y, x), &#39;rmul&#39;, &#39;*&#39;)
        if ufunc.__name__ == &#39;add&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x + y, lambda x, y: choose_backend(x, y).add(x, y), &#39;add&#39;, &#39;+&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y + x, lambda x, y: choose_backend(x, y).add(y, x), &#39;radd&#39;, &#39;+&#39;)
        if ufunc.__name__ == &#39;subtract&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x - y, lambda x, y: choose_backend(x, y).sub(x, y), &#39;add&#39;, &#39;-&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y - x, lambda x, y: choose_backend(x, y).sub(y, x), &#39;rsub&#39;, &#39;-&#39;)
        if ufunc.__name__ in [&#39;divide&#39;, &#39;true_divide&#39;]:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x / y, lambda x, y: choose_backend(x, y).div(x, y), &#39;true_divide&#39;, &#39;/&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y / x, lambda x, y: choose_backend(x, y).div(y, x), &#39;r_true_divide&#39;, &#39;/&#39;)
        if ufunc.__name__ == &#39;floor_divide&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x // y, lambda x, y: choose_backend(x, y).floordiv(x, y), &#39;floor_divide&#39;, &#39;//&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y // x, lambda x, y: choose_backend(x, y).floordiv(y, x), &#39;r_floor_divide&#39;, &#39;//&#39;)
        if ufunc.__name__ == &#39;remainder&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x % y, lambda x, y: choose_backend(x, y).mod(x, y), &#39;remainder&#39;, &#39;%&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y % x, lambda x, y: choose_backend(x, y).mod(y, x), &#39;r_remainder&#39;, &#39;%&#39;)
        if ufunc.__name__ == &#39;equal&#39;:
            if _EQUALITY_BY_REF:
                return wrap(inputs[0] is inputs[1])
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x == y, lambda x, y: choose_backend(x, y).equal(x, y), &#39;equal&#39;, &#39;==&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y == x, lambda x, y: choose_backend(x, y).equal(y, x), &#39;r_equal&#39;, &#39;==&#39;)
        if ufunc.__name__ == &#39;not_equal&#39;:
            if _EQUALITY_BY_REF:
                return wrap(inputs[0] is not inputs[1])
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x != y, lambda x, y: choose_backend(x, y).not_equal(x, y), &#39;equal&#39;, &#39;!=&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y != x, lambda x, y: choose_backend(x, y).not_equal(y, x), &#39;r_equal&#39;, &#39;!=&#39;)
        if ufunc.__name__ == &#39;greater&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x &gt; y, lambda x, y: choose_backend(x, y).greater_than(x, y), &#39;greater&#39;, &#39;&gt;&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y &gt; x, lambda x, y: choose_backend(x, y).greater_than(y, x), &#39;r_greater&#39;, &#39;&gt;&#39;)
        if ufunc.__name__ == &#39;greater_equal&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x &gt;= y, lambda x, y: choose_backend(x, y).greater_or_equal(x, y), &#39;greater_equal&#39;, &#39;&gt;=&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y &gt;= x, lambda x, y: choose_backend(x, y).greater_or_equal(y, x), &#39;r_greater_equal&#39;, &#39;&gt;=&#39;)
        if ufunc.__name__ == &#39;less&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x &lt; y, lambda x, y: choose_backend(x, y).greater_than(y, x), &#39;less&#39;, &#39;&lt;&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y &lt; x, lambda x, y: choose_backend(x, y).greater_than(x, y), &#39;r_less&#39;, &#39;&lt;&#39;)
        if ufunc.__name__ == &#39;less_equal&#39;:
            if inputs[0] is self:
                return self._op2(inputs[1], lambda x, y: x &lt;= y, lambda x, y: choose_backend(x, y).greater_or_equal(y, x), &#39;less_equal&#39;, &#39;&lt;=&#39;)
            else:
                return self._op2(inputs[0], lambda x, y: y &lt;= x, lambda x, y: choose_backend(x, y).greater_or_equal(x, y), &#39;r_less_equal&#39;, &#39;&lt;=&#39;)
        raise NotImplementedError(f&#34;NumPy function &#39;{ufunc.__name__}&#39; is not compatible with Φ-Flow tensors.&#34;)

    @property
    def dtype(self) -&gt; DType:
        &#34;&#34;&#34; Data type of the elements of this `Tensor`. &#34;&#34;&#34;
        raise NotImplementedError()

    @property
    def shape(self) -&gt; Shape:
        &#34;&#34;&#34; The `Shape` lists the dimensions with their sizes, names and types. &#34;&#34;&#34;
        raise NotImplementedError()

    @property
    def default_backend(self) -&gt; Backend:
        from ._ops import choose_backend_t
        return choose_backend_t(self)

    def _with_shape_replaced(self, new_shape: Shape):
        raise NotImplementedError()

    def _with_natives_replaced(self, natives: list):
        &#34;&#34;&#34; Replaces all n _natives() of this Tensor with the first n elements of the list and removes them from the list. &#34;&#34;&#34;
        raise NotImplementedError()

    @property
    def rank(self) -&gt; int:
        &#34;&#34;&#34;
        Number of explicit dimensions of this `Tensor`. Equal to `tensor.shape.rank`.
        This replaces [`numpy.ndarray.ndim`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.ndim.html) /
        [`torch.Tensor.dim`](https://pytorch.org/docs/master/generated/torch.Tensor.dim.html) /
        [`tf.rank()`](https://www.tensorflow.org/api_docs/python/tf/rank) /
        [`jax.numpy.ndim()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndim.html).
        &#34;&#34;&#34;
        return self.shape.rank

    @property
    def _is_tracer(self) -&gt; bool:
        &#34;&#34;&#34;
        Tracers store additional internal information.
        They should not be converted to `native()` in intermediate operations.
        
        TensorStack prevents performing the actual stack operation if one of its component tensors is special.
        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def _to_dict(self):
        return cached(self)._to_dict()

    def __len__(self):
        return self.shape.volume if self.rank == 1 else NotImplemented

    def __bool__(self):
        assert self.rank == 0, f&#34;Cannot convert tensor with non-empty shape {self.shape} to bool. Use tensor.any or tensor.all instead.&#34;
        from ._ops import all_
        if not self.default_backend.supports(Backend.jit_compile):  # NumPy
            return bool(self.native()) if self.rank == 0 else bool(all_(self).native())
        else:
            # __bool__ does not work with TensorFlow tracing.
            # TensorFlow needs to see a tf.Tensor in loop conditions but won&#39;t allow bool() invocations.
            # However, this function must always return a Python bool.
            raise AssertionError(&#34;To evaluate the boolean value of a Tensor, use &#39;Tensor.all&#39;.&#34;)

    @property
    def all(self):
        &#34;&#34;&#34; Whether all values of this `Tensor` are `True` as a native bool. &#34;&#34;&#34;
        from ._ops import all_, cast
        if self.rank == 0:
            return cast(self, DType(bool)).native()
        else:
            return all_(self, dim=self.shape).native()

    @property
    def any(self):
        &#34;&#34;&#34; Whether this `Tensor` contains a `True` value as a native bool. &#34;&#34;&#34;
        from ._ops import any_, cast
        if self.rank == 0:
            return cast(self, DType(bool)).native()
        else:
            return any_(self, dim=self.shape).native()

    @property
    def mean(self):
        &#34;&#34;&#34; Mean value of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import mean
        return mean(self, dim=self.shape).native()

    @property
    def finite_mean(self):
        &#34;&#34;&#34; Mean value of all finite values in this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import finite_mean
        return finite_mean(self, dim=self.shape).native()

    @property
    def std(self):
        &#34;&#34;&#34; Standard deviation of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import std
        return std(self, dim=self.shape).native()

    @property
    def sum(self):
        &#34;&#34;&#34; Sum of all values of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import sum_
        return sum_(self, dim=self.shape).native()

    @property
    def finite_sum(self):
        &#34;&#34;&#34; Sum of all finite values of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import finite_sum
        return finite_sum(self, dim=self.shape).native()

    @property
    def min(self):
        &#34;&#34;&#34; Minimum value of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import min_
        return min_(self, dim=self.shape).native()

    @property
    def finite_min(self):
        &#34;&#34;&#34; Minimum finite value of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import finite_min
        return finite_min(self, dim=self.shape).native()

    @property
    def max(self):
        &#34;&#34;&#34; Maximum value of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import max_
        return max_(self, dim=self.shape).native()

    @property
    def finite_max(self):
        &#34;&#34;&#34; Maximum finite value of this `Tensor` as a native scalar. &#34;&#34;&#34;
        from ._ops import finite_max
        return finite_max(self, dim=self.shape).native()

    @property
    def real(self) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Returns the real part of this tensor.

        See Also:
            `phi.math.real()`
        &#34;&#34;&#34;
        from ._ops import real
        return real(self)

    @property
    def imag(self) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Returns the imaginary part of this tensor.
        If this tensor does not store complex numbers, returns a zero tensor with the same shape and dtype as this tensor.

        See Also:
            `phi.math.imag()`
        &#34;&#34;&#34;
        from ._ops import imag
        return imag(self)

    @property
    def available(self) -&gt; bool:
        &#34;&#34;&#34;
        A tensor is available if it stores concrete values and these can currently be read.

        Tracers used inside jit compilation are typically not available.

        See Also:
            `phi.math.jit_compile()`.
        &#34;&#34;&#34;
        from ._ops import all_available
        return all_available(self)

    @property
    def device(self) -&gt; ComputeDevice or None:
        &#34;&#34;&#34;
        Returns the `ComputeDevice` that this tensor is allocated on.
        The device belongs to this tensor&#39;s `default_backend`.

        See Also:
            `Tensor.default_backend`.
        &#34;&#34;&#34;
        natives = self._natives()
        if not natives:
            return None
        return self.default_backend.get_device(natives[0])

    def __int__(self):
        return int(self.native()) if self.shape.volume == 1 else NotImplemented

    def __float__(self):
        return float(self.native()) if self.shape.volume == 1 else NotImplemented

    def __complex__(self):
        return complex(self.native()) if self.shape.volume == 1 else NotImplemented

    def __index__(self):
        assert self.shape.volume == 1, f&#34;Only scalar tensors can be converted to index but has shape {self.shape}&#34;
        assert self.dtype.kind == int, f&#34;Only int tensors can be converted to index but dtype is {self.dtype}&#34;
        return int(self.native())

    def __repr__(self):
        return format_tensor(self, PrintOptions())

    def _repr_pretty_(self, printer, cycle):
        printer.text(format_tensor(self, PrintOptions(colors=DEFAULT_COLORS)))

    def __format__(self, format_spec: str):
        specs = format_spec.split(&#39;:&#39;)
        layout_ = &#39;auto&#39;
        for possible_layout in [&#39;summary&#39;, &#39;full&#39;, &#39;row&#39;, &#39;numpy&#39;]:
            if possible_layout in specs:
                assert layout_ == &#39;auto&#39;, f&#34;Two layout identifiers encountered in &#39;{format_spec}&#39;&#34;
                layout_ = possible_layout
        include_shape = &#39;shape&#39; in specs or (False if &#39;no-shape&#39; in specs else None)
        include_dtype = &#39;dtype&#39; in specs or (False if &#39;no-dtype&#39; in specs else None)
        color = &#39;color&#39; in specs or (False if &#39;no-color&#39; in specs else None)
        threshold = 8
        float_format = None
        for spec in specs:
            if spec.startswith(&#39;threshold=&#39;):
                threshold = int(spec[len(&#39;threshold=&#39;):])
            elif &#39;.&#39; in spec:
                float_format = spec
        return format_tensor(self, PrintOptions(layout_, float_format, threshold, color, include_shape, include_dtype))

    def __getitem__(self, item) -&gt; &#39;Tensor&#39;:
        if isinstance(item, Tensor):
            from ._ops import gather
            return gather(self, item)
        item = slicing_dict(self, item)
        selections = {}
        sliced = self
        for dim, selection in item.items():
            if dim not in self.shape:
                continue
            selection = self.shape.prepare_gather(dim, selection)
            # Either handle slicing directly or add it to the dict
            if isinstance(selection, (tuple, list)):
                from ._magic_ops import stack
                result = [sliced[{dim: i}] for i in selection]
                stack_dim = sliced.shape[dim].after_gather({dim: selection})
                sliced = stack(result, stack_dim)
            elif isinstance(selection, Tensor) and selection.dtype.kind == bool:
                from ._ops import boolean_mask
                sliced = boolean_mask(sliced, dim, selection)
            elif isinstance(selection, Tensor) and selection.dtype.kind == int:
                from ._ops import gather
                sliced = gather(sliced, selection, dims=dim)
            else:
                selections[dim] = selection
        return sliced._getitem(selections) if selections else sliced

    def _getitem(self, selection: dict) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Slice the tensor along specified dimensions.

        Args:
          selection: dim_name: str -&gt; int or slice
          selection: dict: 

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError()

    def __setitem__(self, key, value):
        raise NotImplementedError(&#34;Tensors are not editable to preserve the autodiff chain. This feature might be added in the future. To update part of a tensor, use math.where() or math.scatter()&#34;)

    def flip(self, *dims: str) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Reverses the order of elements along one or multiple dimensions.

        Args:
            *dims: dimensions to flip

        Returns:
            `Tensor` of the same `Shape`
        &#34;&#34;&#34;
        raise NotImplementedError()

    def __unstack__(self, dims: Tuple[str, ...]) -&gt; Tuple[&#39;Tensor&#39;, ...]:  # from phi.math.magic.Sliceable
        if len(dims) == 1:
            return self.unstack(dims[0])
        else:
            return NotImplemented

    def unstack(self, dimension: str):
        &#34;&#34;&#34;
        Splits this tensor along the specified dimension.
        The returned tensors have the same dimensions as this tensor save the unstacked dimension.

        Raises an error if the dimension is not part of the `Shape` of this `Tensor`.

        See Also:
            `TensorDim.unstack()`

        Args:
          dimension(str or int or TensorDim): name of dimension or Dimension or None for component dimension

        Returns:
          tuple of tensors

        &#34;&#34;&#34;
        raise NotImplementedError()

    def __stack__(self, values: tuple, dim: Shape, **_kwargs) -&gt; &#39;Tensor&#39;:
        from ._ops import stack_tensors
        return stack_tensors(values, dim)

    def __expand__(self, dims: Shape, **kwargs) -&gt; &#39;Tensor&#39;:
        from ._ops import expand_tensor
        return expand_tensor(self, dims)

    def __concat__(self, values: tuple, dim: str, **kwargs) -&gt; &#39;Tensor&#39;:
        from ._ops import concat_tensor
        return concat_tensor(values, dim)

    def __replace_dims__(self, dims: Tuple[str, ...], new_dims: Shape, **kwargs) -&gt; &#39;Tensor&#39;:
        from ._magic_ops import rename_dims
        return self._with_shape_replaced(rename_dims(self.shape, dims, new_dims))

    def __unpack_dim__(self, dim: str, unpacked_dims: Shape, **kwargs) -&gt; &#39;Tensor&#39;:
        native = self.native(self.shape.names)
        new_shape = self.shape.without(dim)
        i = self.shape.index(dim)
        for d in unpacked_dims:
            new_shape = new_shape._expand(d, pos=i)
            i += 1
        native_reshaped = choose_backend(native).reshape(native, new_shape.sizes)
        return NativeTensor(native_reshaped, new_shape)

    def __pack_dims__(self, dims: Tuple[str, ...], packed_dim: Shape, pos: int or None, **kwargs) -&gt; &#39;Tensor&#39;:
        order = self.shape._order_group(dims)
        if self.shape.is_uniform:
            native = self.native(order)
            if pos is None:
                pos = min(self.shape.indices(dims))
            new_shape = self.shape.without(dims)._expand(packed_dim.with_sizes([self.shape.only(dims).volume]), pos)
            native = choose_backend(native).reshape(native, new_shape.sizes)
            return NativeTensor(native, new_shape)
        else:
            from ._ops import concat_tensor
            from ._magic_ops import pack_dims
            value = cached(self)
            assert isinstance(value, TensorStack)
            assert value.stack_dim.name in dims
            concat_dim = value.shape.without(value.stack_dim)[0]
            c = concat_tensor(value._tensors, concat_dim)
            return pack_dims(c, [d for d in dims if d != value.stack_dim.name], packed_dim, pos=pos)

    def __cast__(self, dtype: DType):
        return self._op1(lambda native: choose_backend(native).cast(native, dtype=dtype))

    def dimension(self, name: str or Shape) -&gt; &#39;TensorDim&#39;:
        &#34;&#34;&#34;
        Returns a reference to a specific dimension of this tensor.
        This is equivalent to the syntax `tensor.&lt;name&gt;`.

        The dimension need not be part of the `Tensor.shape` in which case its size is 1.

        Args:
            name: dimension name

        Returns:
            `TensorDim` corresponding to a dimension of this tensor
        &#34;&#34;&#34;
        if isinstance(name, str):
            return TensorDim(self, name)
        elif isinstance(name, Shape):
            return TensorDim(self, name.name)
        else:
            raise ValueError(name)

    def pack(self, dims, packed_dim):
        &#34;&#34;&#34; See `pack_dims()` &#34;&#34;&#34;
        from ._ops import pack_dims
        return pack_dims(self, dims, packed_dim)

    def unpack(self, dim, unpacked_dims):
        &#34;&#34;&#34; See `unpack_dim()` &#34;&#34;&#34;
        from ._ops import unpack_dim
        return unpack_dim(self, dim, unpacked_dims)

    def __getattr__(self, name):
        if name.startswith(&#39;__&#39;):  # called by hasattr in magic ops
            raise AttributeError
        if name.startswith(&#39;_&#39;):
            raise AttributeError(f&#34;&#39;{type(self)}&#39; object has no attribute &#39;{name}&#39;&#34;)
        if name == &#39;is_tensor_like&#39;:  # TensorFlow replaces abs() while tracing and checks for this attribute
            raise AttributeError(f&#34;&#39;{type(self)}&#39; object has no attribute &#39;{name}&#39;&#34;)
        assert name not in (&#39;shape&#39;, &#39;_shape&#39;, &#39;tensor&#39;), name
        return TensorDim(self, name)

    def __add__(self, other):
        return self._op2(other, lambda x, y: x + y, lambda x, y: choose_backend(x, y).add(x, y), &#39;add&#39;, &#39;+&#39;)

    def __radd__(self, other):
        return self._op2(other, lambda x, y: y + x, lambda x, y: choose_backend(x, y).add(y, x), &#39;radd&#39;, &#39;+&#39;)

    def __sub__(self, other):
        return self._op2(other, lambda x, y: x - y, lambda x, y: choose_backend(x, y).sub(x, y), &#39;sub&#39;, &#39;-&#39;)

    def __rsub__(self, other):
        return self._op2(other, lambda x, y: y - x, lambda x, y: choose_backend(x, y).sub(y, x), &#39;rsub&#39;, &#39;-&#39;)

    def __and__(self, other):
        return self._op2(other, lambda x, y: x &amp; y, lambda x, y: choose_backend(x, y).and_(x, y), &#39;and&#39;, &#39;&amp;&#39;)

    def __rand__(self, other):
        return self._op2(other, lambda x, y: y &amp; x, lambda x, y: choose_backend(x, y).and_(y, x), &#39;rand&#39;, &#39;&amp;&#39;)

    def __or__(self, other):
        return self._op2(other, lambda x, y: x | y, lambda x, y: choose_backend(x, y).or_(x, y), &#39;or&#39;, &#39;|&#39;)

    def __ror__(self, other):
        return self._op2(other, lambda x, y: y | x, lambda x, y: choose_backend(x, y).or_(y, x), &#39;ror&#39;, &#39;|&#39;)

    def __xor__(self, other):
        return self._op2(other, lambda x, y: x ^ y, lambda x, y: choose_backend(x, y).xor(x, y), &#39;xor&#39;, &#39;^&#39;)

    def __rxor__(self, other):
        return self._op2(other, lambda x, y: y ^ x, lambda x, y: choose_backend(x, y).xor(y, x), &#39;rxor&#39;, &#39;^&#39;)

    def __mul__(self, other):
        return self._op2(other, lambda x, y: x * y, lambda x, y: choose_backend(x, y).mul(x, y), &#39;mul&#39;, &#39;*&#39;)

    def __rmul__(self, other):
        return self._op2(other, lambda x, y: y * x, lambda x, y: choose_backend(x, y).mul(y, x), &#39;rmul&#39;, &#39;*&#39;)

    def __truediv__(self, other):
        return self._op2(other, lambda x, y: x / y, lambda x, y: choose_backend(x, y).div(x, y), &#39;truediv&#39;, &#39;/&#39;)

    def __rtruediv__(self, other):
        return self._op2(other, lambda x, y: y / x, lambda x, y: choose_backend(x, y).div(y, x), &#39;rtruediv&#39;, &#39;/&#39;)

    def __divmod__(self, other):
        return self._op2(other, lambda x, y: divmod(x, y), lambda x, y: divmod(x, y), &#39;divmod&#39;, &#39;divmod&#39;)

    def __rdivmod__(self, other):
        return self._op2(other, lambda x, y: divmod(y, x), lambda x, y: divmod(y, x), &#39;rdivmod&#39;, &#39;divmod&#39;)

    def __floordiv__(self, other):
        return self._op2(other, lambda x, y: x // y, lambda x, y: choose_backend(x, y).floordiv(x, y), &#39;floordiv&#39;, &#39;//&#39;)

    def __rfloordiv__(self, other):
        return self._op2(other, lambda x, y: y // x, lambda x, y: choose_backend(x, y).floordiv(y, x), &#39;rfloordiv&#39;, &#39;//&#39;)

    def __pow__(self, power, modulo=None):
        assert modulo is None
        return self._op2(power, lambda x, y: x ** y, lambda x, y: choose_backend(x, y).pow(x, y), &#39;pow&#39;, &#39;**&#39;)

    def __rpow__(self, other):
        return self._op2(other, lambda x, y: y ** x, lambda x, y: choose_backend(x, y).pow(y, x), &#39;rpow&#39;, &#39;**&#39;)

    def __mod__(self, other):
        return self._op2(other, lambda x, y: x % y, lambda x, y: choose_backend(x, y).mod(x, y), &#39;mod&#39;, &#39;%&#39;)

    def __rmod__(self, other):
        return self._op2(other, lambda x, y: y % x, lambda x, y: choose_backend(x, y).mod(y, x), &#39;rmod&#39;, &#39;%&#39;)

    def __eq__(self, other):
        if _EQUALITY_BY_REF:
            return wrap(self is other)
        return self._op2(other, lambda x, y: x == y, lambda x, y: choose_backend(x, y).equal(x, y), &#39;eq&#39;, &#39;==&#39;)

    def __ne__(self, other):
        if _EQUALITY_BY_REF:
            return wrap(self is not other)
        return self._op2(other, lambda x, y: x != y, lambda x, y: choose_backend(x, y).not_equal(x, y), &#39;ne&#39;, &#39;!=&#39;)

    def __lt__(self, other):
        return self._op2(other, lambda x, y: x &lt; y, lambda x, y: choose_backend(x, y).greater_than(y, x), &#39;lt&#39;, &#39;&lt;&#39;)

    def __le__(self, other):
        return self._op2(other, lambda x, y: x &lt;= y, lambda x, y: choose_backend(x, y).greater_or_equal(y, x), &#39;le&#39;, &#39;&lt;=&#39;)

    def __gt__(self, other):
        return self._op2(other, lambda x, y: x &gt; y, lambda x, y: choose_backend(x, y).greater_than(x, y), &#39;gt&#39;, &#39;&gt;&#39;)

    def __ge__(self, other):
        return self._op2(other, lambda x, y: x &gt;= y, lambda x, y: choose_backend(x, y).greater_or_equal(x, y), &#39;ge&#39;, &#39;&gt;=&#39;)

    def __abs__(self):
        return self._op1(lambda t: choose_backend(t).abs(t))

    def __round__(self, n=None):
        return self._op1(lambda t: choose_backend(t).round(t))

    def __copy__(self):
        return self._op1(lambda t: choose_backend(t).copy(t, only_mutable=True))

    def __deepcopy__(self, memodict={}):
        return self._op1(lambda t: choose_backend(t).copy(t, only_mutable=False))

    def __neg__(self):
        return self._op1(lambda t: -t)

    def __invert__(self):
        return self._op1(lambda t: ~t)

    def __reversed__(self):
        assert self.shape.channel.rank == 1
        return self[::-1]

    def __iter__(self):
        if self.rank == 1:
            return iter(self.native())
        elif self.rank == 0:
            return iter([self.native()])
        else:
            from ._ops import reshaped_native
            native = reshaped_native(self, [self.shape])
            return iter(native)

    def _tensor(self, other):
        if isinstance(other, Tensor):
            return other
        elif isinstance(other, (tuple, list)) and any(isinstance(v, Tensor) for v in other):
            if &#39;vector&#39; in self.shape:
                outer_dim = self.shape[&#39;vector&#39;]
            elif self.shape.channel_rank == 1:
                outer_dim = self.shape.channel
            else:
                raise ValueError(f&#34;Cannot combine tensor of shape {self.shape} with tuple {tuple([type(v).__name__ for v in other])}&#34;)
            remaining_shape = self.shape.without(outer_dim)
            other_items = [v if isinstance(v, Tensor) else compatible_tensor(v, compat_shape=remaining_shape, compat_natives=self._natives(), convert=False) for v in other]
            other_stacked = stack(other_items, outer_dim, expand_values=True)
            return other_stacked
        else:
            return compatible_tensor(other, compat_shape=self.shape, compat_natives=self._natives(), convert=False)

    def _op1(self, native_function):
        &#34;&#34;&#34;
        Transform the values of this tensor given a function that can be applied to any native tensor.

        Args:
          native_function:

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def _op2(self, other, operator: Callable, native_function: Callable, op_name: str = &#39;unknown&#39;, op_symbol: str = &#39;?&#39;) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Apply a broadcast operation on two tensors.

        Args:
            other: second argument
            operator: function (Tensor, Tensor) -&gt; Tensor, used to propagate the operation to children tensors to have Python choose the callee
            native_function: function (native tensor, native tensor) -&gt; native tensor
            op_name: Name of the python function without leading and trailing `__`.
                Examples: &#39;add&#39;, &#39;radd&#39;, &#39;sub&#39;, &#39;mul&#39;, &#39;and&#39;, &#39;eq&#39;, &#39;ge&#39;.
            op_symbol: Operation symbol, such as &#39;+&#39;, &#39;-&#39;, &#39;&amp;&#39;, &#39;%&#39;, &#39;&gt;=&#39;

        Returns:
            `Tensor`
        &#34;&#34;&#34;
        raise NotImplementedError()

    def _natives(self) -&gt; tuple:
        raise NotImplementedError(self.__class__)

    def _expand(self):
        &#34;&#34;&#34; Expands all compressed tensors to their defined size as if they were being used in `Tensor.native()`. &#34;&#34;&#34;
        warnings.warn(&#34;Tensor._expand() is deprecated, use cached(Tensor) instead.&#34;, DeprecationWarning)
        raise NotImplementedError(self.__class__)

    def _tensor_reduce(self,
                       dims: Tuple[str],
                       dtype: type or None,
                       native_function: Callable,
                       collapsed_function: Callable = lambda inner_reduced, collapsed_dims_to_reduce: inner_reduced,
                       unaffected_function: Callable = lambda value: value):
        raise NotImplementedError(self.__class__)

    def _simplify(self):
        return self</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li>phi.math._functional.ShiftLinTracer</li>
<li>phi.math._tensors.CollapsedTensor</li>
<li>phi.math._tensors.Layout</li>
<li>phi.math._tensors.NativeTensor</li>
<li>phi.math._tensors.TensorStack</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.Tensor.all"><code class="name">var <span class="ident">all</span></code></dt>
<dd>
<div class="desc"><p>Whether all values of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> are <code>True</code> as a native bool.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def all(self):
    &#34;&#34;&#34; Whether all values of this `Tensor` are `True` as a native bool. &#34;&#34;&#34;
    from ._ops import all_, cast
    if self.rank == 0:
        return cast(self, DType(bool)).native()
    else:
        return all_(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.any"><code class="name">var <span class="ident">any</span></code></dt>
<dd>
<div class="desc"><p>Whether this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> contains a <code>True</code> value as a native bool.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def any(self):
    &#34;&#34;&#34; Whether this `Tensor` contains a `True` value as a native bool. &#34;&#34;&#34;
    from ._ops import any_, cast
    if self.rank == 0:
        return cast(self, DType(bool)).native()
    else:
        return any_(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.available"><code class="name">var <span class="ident">available</span> : bool</code></dt>
<dd>
<div class="desc"><p>A tensor is available if it stores concrete values and these can currently be read.</p>
<p>Tracers used inside jit compilation are typically not available.</p>
<p>See Also:
<code><a title="phi.math.jit_compile" href="#phi.math.jit_compile">jit_compile()</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def available(self) -&gt; bool:
    &#34;&#34;&#34;
    A tensor is available if it stores concrete values and these can currently be read.

    Tracers used inside jit compilation are typically not available.

    See Also:
        `phi.math.jit_compile()`.
    &#34;&#34;&#34;
    from ._ops import all_available
    return all_available(self)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.default_backend"><code class="name">var <span class="ident">default_backend</span> : phi.math.backend._backend.Backend</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def default_backend(self) -&gt; Backend:
    from ._ops import choose_backend_t
    return choose_backend_t(self)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.device"><code class="name">var <span class="ident">device</span> : phi.math.backend._backend.ComputeDevice</code></dt>
<dd>
<div class="desc"><p>Returns the <code>ComputeDevice</code> that this tensor is allocated on.
The device belongs to this tensor's <code>default_backend</code>.</p>
<p>See Also:
<code><a title="phi.math.Tensor.default_backend" href="#phi.math.Tensor.default_backend">Tensor.default_backend</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def device(self) -&gt; ComputeDevice or None:
    &#34;&#34;&#34;
    Returns the `ComputeDevice` that this tensor is allocated on.
    The device belongs to this tensor&#39;s `default_backend`.

    See Also:
        `Tensor.default_backend`.
    &#34;&#34;&#34;
    natives = self._natives()
    if not natives:
        return None
    return self.default_backend.get_device(natives[0])</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.dtype"><code class="name">var <span class="ident">dtype</span> : phi.math.backend._dtype.DType</code></dt>
<dd>
<div class="desc"><p>Data type of the elements of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dtype(self) -&gt; DType:
    &#34;&#34;&#34; Data type of the elements of this `Tensor`. &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.finite_max"><code class="name">var <span class="ident">finite_max</span></code></dt>
<dd>
<div class="desc"><p>Maximum finite value of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def finite_max(self):
    &#34;&#34;&#34; Maximum finite value of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import finite_max
    return finite_max(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.finite_mean"><code class="name">var <span class="ident">finite_mean</span></code></dt>
<dd>
<div class="desc"><p>Mean value of all finite values in this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def finite_mean(self):
    &#34;&#34;&#34; Mean value of all finite values in this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import finite_mean
    return finite_mean(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.finite_min"><code class="name">var <span class="ident">finite_min</span></code></dt>
<dd>
<div class="desc"><p>Minimum finite value of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def finite_min(self):
    &#34;&#34;&#34; Minimum finite value of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import finite_min
    return finite_min(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.finite_sum"><code class="name">var <span class="ident">finite_sum</span></code></dt>
<dd>
<div class="desc"><p>Sum of all finite values of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def finite_sum(self):
    &#34;&#34;&#34; Sum of all finite values of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import finite_sum
    return finite_sum(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.imag"><code class="name">var <span class="ident">imag</span> : phi.math._tensors.Tensor</code></dt>
<dd>
<div class="desc"><p>Returns the imaginary part of this tensor.
If this tensor does not store complex numbers, returns a zero tensor with the same shape and dtype as this tensor.</p>
<p>See Also:
<code><a title="phi.math.imag" href="#phi.math.imag">imag()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def imag(self) -&gt; &#39;Tensor&#39;:
    &#34;&#34;&#34;
    Returns the imaginary part of this tensor.
    If this tensor does not store complex numbers, returns a zero tensor with the same shape and dtype as this tensor.

    See Also:
        `phi.math.imag()`
    &#34;&#34;&#34;
    from ._ops import imag
    return imag(self)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.max"><code class="name">var <span class="ident">max</span></code></dt>
<dd>
<div class="desc"><p>Maximum value of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def max(self):
    &#34;&#34;&#34; Maximum value of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import max_
    return max_(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.mean"><code class="name">var <span class="ident">mean</span></code></dt>
<dd>
<div class="desc"><p>Mean value of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def mean(self):
    &#34;&#34;&#34; Mean value of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import mean
    return mean(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.min"><code class="name">var <span class="ident">min</span></code></dt>
<dd>
<div class="desc"><p>Minimum value of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def min(self):
    &#34;&#34;&#34; Minimum value of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import min_
    return min_(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.rank"><code class="name">var <span class="ident">rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of explicit dimensions of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>. Equal to <code>tensor.shape.rank</code>.
This replaces <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.ndim.html"><code>numpy.ndarray.ndim</code></a> /
<a href="https://pytorch.org/docs/master/generated/torch.Tensor.dim.html"><code>torch.Tensor.dim</code></a> /
<a href="https://www.tensorflow.org/api_docs/python/tf/rank"><code>tf.rank()</code></a> /
<a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndim.html"><code>jax.numpy.ndim()</code></a>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rank(self) -&gt; int:
    &#34;&#34;&#34;
    Number of explicit dimensions of this `Tensor`. Equal to `tensor.shape.rank`.
    This replaces [`numpy.ndarray.ndim`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.ndim.html) /
    [`torch.Tensor.dim`](https://pytorch.org/docs/master/generated/torch.Tensor.dim.html) /
    [`tf.rank()`](https://www.tensorflow.org/api_docs/python/tf/rank) /
    [`jax.numpy.ndim()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndim.html).
    &#34;&#34;&#34;
    return self.shape.rank</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.real"><code class="name">var <span class="ident">real</span> : phi.math._tensors.Tensor</code></dt>
<dd>
<div class="desc"><p>Returns the real part of this tensor.</p>
<p>See Also:
<code><a title="phi.math.real" href="#phi.math.real">real()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def real(self) -&gt; &#39;Tensor&#39;:
    &#34;&#34;&#34;
    Returns the real part of this tensor.

    See Also:
        `phi.math.real()`
    &#34;&#34;&#34;
    from ._ops import real
    return real(self)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.shape"><code class="name">var <span class="ident">shape</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>The <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> lists the dimensions with their sizes, names and types.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self) -&gt; Shape:
    &#34;&#34;&#34; The `Shape` lists the dimensions with their sizes, names and types. &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.std"><code class="name">var <span class="ident">std</span></code></dt>
<dd>
<div class="desc"><p>Standard deviation of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def std(self):
    &#34;&#34;&#34; Standard deviation of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import std
    return std(self, dim=self.shape).native()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.sum"><code class="name">var <span class="ident">sum</span></code></dt>
<dd>
<div class="desc"><p>Sum of all values of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> as a native scalar.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sum(self):
    &#34;&#34;&#34; Sum of all values of this `Tensor` as a native scalar. &#34;&#34;&#34;
    from ._ops import sum_
    return sum_(self, dim=self.shape).native()</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.Tensor.dimension"><code class="name flex">
<span>def <span class="ident">dimension</span></span>(<span>self, name: str) ‑> phi.math._tensors.TensorDim</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a reference to a specific dimension of this tensor.
This is equivalent to the syntax <code>tensor.&lt;name&gt;</code>.</p>
<p>The dimension need not be part of the <code><a title="phi.math.Tensor.shape" href="#phi.math.Tensor.shape">Tensor.shape</a></code> in which case its size is 1.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>dimension name</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>TensorDim</code> corresponding to a dimension of this tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dimension(self, name: str or Shape) -&gt; &#39;TensorDim&#39;:
    &#34;&#34;&#34;
    Returns a reference to a specific dimension of this tensor.
    This is equivalent to the syntax `tensor.&lt;name&gt;`.

    The dimension need not be part of the `Tensor.shape` in which case its size is 1.

    Args:
        name: dimension name

    Returns:
        `TensorDim` corresponding to a dimension of this tensor
    &#34;&#34;&#34;
    if isinstance(name, str):
        return TensorDim(self, name)
    elif isinstance(name, Shape):
        return TensorDim(self, name.name)
    else:
        raise ValueError(name)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.flip"><code class="name flex">
<span>def <span class="ident">flip</span></span>(<span>self, *dims: str) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Reverses the order of elements along one or multiple dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*dims</code></strong></dt>
<dd>dimensions to flip</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of the same <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flip(self, *dims: str) -&gt; &#39;Tensor&#39;:
    &#34;&#34;&#34;
    Reverses the order of elements along one or multiple dimensions.

    Args:
        *dims: dimensions to flip

    Returns:
        `Tensor` of the same `Shape`
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.native"><code class="name flex">
<span>def <span class="ident">native</span></span>(<span>self, order: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a native tensor object with the dimensions ordered according to <code>order</code>.</p>
<p>Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
If a dimension of the tensor is not listed in <code>order</code>, a <code>ValueError</code> is raised.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>order</code></strong></dt>
<dd>(Optional) Order of dimension names as comma-separated string, list or <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Native tensor representation, such as PyTorch tensor or NumPy array.</p>
<h2 id="raises">Raises</h2>
<p>ValueError if the tensor cannot be transposed to match target_shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def native(self, order: str or tuple or list or Shape = None):
    &#34;&#34;&#34;
    Returns a native tensor object with the dimensions ordered according to `order`.
    
    Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
    If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

    Args:
        order: (Optional) Order of dimension names as comma-separated string, list or `Shape`.

    Returns:
        Native tensor representation, such as PyTorch tensor or NumPy array.

    Raises:
        ValueError if the tensor cannot be transposed to match target_shape
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.numpy"><code class="name flex">
<span>def <span class="ident">numpy</span></span>(<span>self, order: str = None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Converts this tensor to a <code>numpy.ndarray</code> with dimensions ordered according to <code>order</code>.</p>
<p><em>Note</em>: Using this function breaks the autograd chain. The returned tensor is not differentiable.
To get a differentiable tensor, use <code><a title="phi.math.Tensor.native" href="#phi.math.Tensor.native">Tensor.native()</a></code> instead.</p>
<p>Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
If a dimension of the tensor is not listed in <code>order</code>, a <code>ValueError</code> is raised.</p>
<p>If this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> is backed by a NumPy array, a reference to this array may be returned.</p>
<p>See Also:
<code><a title="phi.math.numpy" href="#phi.math.numpy">numpy()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>order</code></strong></dt>
<dd>(Optional) Order of dimension names as comma-separated string, list or <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>NumPy representation</p>
<h2 id="raises">Raises</h2>
<p>ValueError if the tensor cannot be transposed to match target_shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def numpy(self, order: str or tuple or list or Shape = None) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Converts this tensor to a `numpy.ndarray` with dimensions ordered according to `order`.
    
    *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
    To get a differentiable tensor, use `Tensor.native()` instead.
    
    Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
    If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

    If this `Tensor` is backed by a NumPy array, a reference to this array may be returned.

    See Also:
        `phi.math.numpy()`

    Args:
        order: (Optional) Order of dimension names as comma-separated string, list or `Shape`.

    Returns:
        NumPy representation

    Raises:
        ValueError if the tensor cannot be transposed to match target_shape
    &#34;&#34;&#34;
    native = self.native(order=order)
    return choose_backend(native).numpy(native)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.pack"><code class="name flex">
<span>def <span class="ident">pack</span></span>(<span>self, dims, packed_dim)</span>
</code></dt>
<dd>
<div class="desc"><p>See <code><a title="phi.math.pack_dims" href="#phi.math.pack_dims">pack_dims()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pack(self, dims, packed_dim):
    &#34;&#34;&#34; See `pack_dims()` &#34;&#34;&#34;
    from ._ops import pack_dims
    return pack_dims(self, dims, packed_dim)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.unpack"><code class="name flex">
<span>def <span class="ident">unpack</span></span>(<span>self, dim, unpacked_dims)</span>
</code></dt>
<dd>
<div class="desc"><p>See <code><a title="phi.math.unpack_dim" href="#phi.math.unpack_dim">unpack_dim()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unpack(self, dim, unpacked_dims):
    &#34;&#34;&#34; See `unpack_dim()` &#34;&#34;&#34;
    from ._ops import unpack_dim
    return unpack_dim(self, dim, unpacked_dims)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>self, dimension: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Splits this tensor along the specified dimension.
The returned tensors have the same dimensions as this tensor save the unstacked dimension.</p>
<p>Raises an error if the dimension is not part of the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p>
<p>See Also:
<code>TensorDim.unstack()</code></p>
<h2 id="args">Args</h2>
<p>dimension(str or int or TensorDim): name of dimension or Dimension or None for component dimension</p>
<h2 id="returns">Returns</h2>
<p>tuple of tensors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack(self, dimension: str):
    &#34;&#34;&#34;
    Splits this tensor along the specified dimension.
    The returned tensors have the same dimensions as this tensor save the unstacked dimension.

    Raises an error if the dimension is not part of the `Shape` of this `Tensor`.

    See Also:
        `TensorDim.unstack()`

    Args:
      dimension(str or int or TensorDim): name of dimension or Dimension or None for component dimension

    Returns:
      tuple of tensors

    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="phi" href="../index.html">phi</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="phi.math.backend" href="backend/index.html">phi.math.backend</a></code></li>
<li><code><a title="phi.math.extrapolation" href="extrapolation.html">phi.math.extrapolation</a></code></li>
<li><code><a title="phi.math.magic" href="magic.html">phi.math.magic</a></code></li>
</ul>
</li>
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="phi.math.INF" href="#phi.math.INF">INF</a></code></li>
<li><code><a title="phi.math.NAN" href="#phi.math.NAN">NAN</a></code></li>
<li><code><a title="phi.math.NUMPY" href="#phi.math.NUMPY">NUMPY</a></code></li>
<li><code><a title="phi.math.PI" href="#phi.math.PI">PI</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="phi.math.abs" href="#phi.math.abs">abs</a></code></li>
<li><code><a title="phi.math.abs_square" href="#phi.math.abs_square">abs_square</a></code></li>
<li><code><a title="phi.math.all" href="#phi.math.all">all</a></code></li>
<li><code><a title="phi.math.all_available" href="#phi.math.all_available">all_available</a></code></li>
<li><code><a title="phi.math.any" href="#phi.math.any">any</a></code></li>
<li><code><a title="phi.math.arccos" href="#phi.math.arccos">arccos</a></code></li>
<li><code><a title="phi.math.arcsin" href="#phi.math.arcsin">arcsin</a></code></li>
<li><code><a title="phi.math.assert_close" href="#phi.math.assert_close">assert_close</a></code></li>
<li><code><a title="phi.math.batch" href="#phi.math.batch">batch</a></code></li>
<li><code><a title="phi.math.boolean_mask" href="#phi.math.boolean_mask">boolean_mask</a></code></li>
<li><code><a title="phi.math.cast" href="#phi.math.cast">cast</a></code></li>
<li><code><a title="phi.math.ceil" href="#phi.math.ceil">ceil</a></code></li>
<li><code><a title="phi.math.channel" href="#phi.math.channel">channel</a></code></li>
<li><code><a title="phi.math.choose_backend" href="#phi.math.choose_backend">choose_backend</a></code></li>
<li><code><a title="phi.math.clip" href="#phi.math.clip">clip</a></code></li>
<li><code><a title="phi.math.close" href="#phi.math.close">close</a></code></li>
<li><code><a title="phi.math.closest_grid_values" href="#phi.math.closest_grid_values">closest_grid_values</a></code></li>
<li><code><a title="phi.math.concat" href="#phi.math.concat">concat</a></code></li>
<li><code><a title="phi.math.concat_shapes" href="#phi.math.concat_shapes">concat_shapes</a></code></li>
<li><code><a title="phi.math.conjugate" href="#phi.math.conjugate">conjugate</a></code></li>
<li><code><a title="phi.math.const_vec" href="#phi.math.const_vec">const_vec</a></code></li>
<li><code><a title="phi.math.convert" href="#phi.math.convert">convert</a></code></li>
<li><code><a title="phi.math.convolve" href="#phi.math.convolve">convolve</a></code></li>
<li><code><a title="phi.math.copy" href="#phi.math.copy">copy</a></code></li>
<li><code><a title="phi.math.copy_with" href="#phi.math.copy_with">copy_with</a></code></li>
<li><code><a title="phi.math.cos" href="#phi.math.cos">cos</a></code></li>
<li><code><a title="phi.math.cross_product" href="#phi.math.cross_product">cross_product</a></code></li>
<li><code><a title="phi.math.cumulative_sum" href="#phi.math.cumulative_sum">cumulative_sum</a></code></li>
<li><code><a title="phi.math.custom_gradient" href="#phi.math.custom_gradient">custom_gradient</a></code></li>
<li><code><a title="phi.math.degrees" href="#phi.math.degrees">degrees</a></code></li>
<li><code><a title="phi.math.dim_mask" href="#phi.math.dim_mask">dim_mask</a></code></li>
<li><code><a title="phi.math.divide_no_nan" href="#phi.math.divide_no_nan">divide_no_nan</a></code></li>
<li><code><a title="phi.math.dot" href="#phi.math.dot">dot</a></code></li>
<li><code><a title="phi.math.downsample2x" href="#phi.math.downsample2x">downsample2x</a></code></li>
<li><code><a title="phi.math.dtype" href="#phi.math.dtype">dtype</a></code></li>
<li><code><a title="phi.math.exp" href="#phi.math.exp">exp</a></code></li>
<li><code><a title="phi.math.expand" href="#phi.math.expand">expand</a></code></li>
<li><code><a title="phi.math.fft" href="#phi.math.fft">fft</a></code></li>
<li><code><a title="phi.math.fftfreq" href="#phi.math.fftfreq">fftfreq</a></code></li>
<li><code><a title="phi.math.finite_fill" href="#phi.math.finite_fill">finite_fill</a></code></li>
<li><code><a title="phi.math.finite_max" href="#phi.math.finite_max">finite_max</a></code></li>
<li><code><a title="phi.math.finite_mean" href="#phi.math.finite_mean">finite_mean</a></code></li>
<li><code><a title="phi.math.finite_min" href="#phi.math.finite_min">finite_min</a></code></li>
<li><code><a title="phi.math.finite_sum" href="#phi.math.finite_sum">finite_sum</a></code></li>
<li><code><a title="phi.math.flatten" href="#phi.math.flatten">flatten</a></code></li>
<li><code><a title="phi.math.floor" href="#phi.math.floor">floor</a></code></li>
<li><code><a title="phi.math.fourier_laplace" href="#phi.math.fourier_laplace">fourier_laplace</a></code></li>
<li><code><a title="phi.math.fourier_poisson" href="#phi.math.fourier_poisson">fourier_poisson</a></code></li>
<li><code><a title="phi.math.frequency_loss" href="#phi.math.frequency_loss">frequency_loss</a></code></li>
<li><code><a title="phi.math.from_dict" href="#phi.math.from_dict">from_dict</a></code></li>
<li><code><a title="phi.math.functional_gradient" href="#phi.math.functional_gradient">functional_gradient</a></code></li>
<li><code><a title="phi.math.gather" href="#phi.math.gather">gather</a></code></li>
<li><code><a title="phi.math.get_precision" href="#phi.math.get_precision">get_precision</a></code></li>
<li><code><a title="phi.math.gradient" href="#phi.math.gradient">gradient</a></code></li>
<li><code><a title="phi.math.grid_sample" href="#phi.math.grid_sample">grid_sample</a></code></li>
<li><code><a title="phi.math.hessian" href="#phi.math.hessian">hessian</a></code></li>
<li><code><a title="phi.math.ifft" href="#phi.math.ifft">ifft</a></code></li>
<li><code><a title="phi.math.imag" href="#phi.math.imag">imag</a></code></li>
<li><code><a title="phi.math.instance" href="#phi.math.instance">instance</a></code></li>
<li><code><a title="phi.math.is_finite" href="#phi.math.is_finite">is_finite</a></code></li>
<li><code><a title="phi.math.is_scalar" href="#phi.math.is_scalar">is_scalar</a></code></li>
<li><code><a title="phi.math.isfinite" href="#phi.math.isfinite">isfinite</a></code></li>
<li><code><a title="phi.math.iterate" href="#phi.math.iterate">iterate</a></code></li>
<li><code><a title="phi.math.jacobian" href="#phi.math.jacobian">jacobian</a></code></li>
<li><code><a title="phi.math.jit_compile" href="#phi.math.jit_compile">jit_compile</a></code></li>
<li><code><a title="phi.math.jit_compile_linear" href="#phi.math.jit_compile_linear">jit_compile_linear</a></code></li>
<li><code><a title="phi.math.l1_loss" href="#phi.math.l1_loss">l1_loss</a></code></li>
<li><code><a title="phi.math.l2_loss" href="#phi.math.l2_loss">l2_loss</a></code></li>
<li><code><a title="phi.math.laplace" href="#phi.math.laplace">laplace</a></code></li>
<li><code><a title="phi.math.layout" href="#phi.math.layout">layout</a></code></li>
<li><code><a title="phi.math.linspace" href="#phi.math.linspace">linspace</a></code></li>
<li><code><a title="phi.math.log" href="#phi.math.log">log</a></code></li>
<li><code><a title="phi.math.log10" href="#phi.math.log10">log10</a></code></li>
<li><code><a title="phi.math.log2" href="#phi.math.log2">log2</a></code></li>
<li><code><a title="phi.math.map" href="#phi.math.map">map</a></code></li>
<li><code><a title="phi.math.map_i2b" href="#phi.math.map_i2b">map_i2b</a></code></li>
<li><code><a title="phi.math.map_s2b" href="#phi.math.map_s2b">map_s2b</a></code></li>
<li><code><a title="phi.math.map_types" href="#phi.math.map_types">map_types</a></code></li>
<li><code><a title="phi.math.masked_fill" href="#phi.math.masked_fill">masked_fill</a></code></li>
<li><code><a title="phi.math.max" href="#phi.math.max">max</a></code></li>
<li><code><a title="phi.math.maximum" href="#phi.math.maximum">maximum</a></code></li>
<li><code><a title="phi.math.mean" href="#phi.math.mean">mean</a></code></li>
<li><code><a title="phi.math.median" href="#phi.math.median">median</a></code></li>
<li><code><a title="phi.math.merge_shapes" href="#phi.math.merge_shapes">merge_shapes</a></code></li>
<li><code><a title="phi.math.meshgrid" href="#phi.math.meshgrid">meshgrid</a></code></li>
<li><code><a title="phi.math.min" href="#phi.math.min">min</a></code></li>
<li><code><a title="phi.math.minimize" href="#phi.math.minimize">minimize</a></code></li>
<li><code><a title="phi.math.minimum" href="#phi.math.minimum">minimum</a></code></li>
<li><code><a title="phi.math.native" href="#phi.math.native">native</a></code></li>
<li><code><a title="phi.math.native_call" href="#phi.math.native_call">native_call</a></code></li>
<li><code><a title="phi.math.non_batch" href="#phi.math.non_batch">non_batch</a></code></li>
<li><code><a title="phi.math.non_channel" href="#phi.math.non_channel">non_channel</a></code></li>
<li><code><a title="phi.math.non_instance" href="#phi.math.non_instance">non_instance</a></code></li>
<li><code><a title="phi.math.non_spatial" href="#phi.math.non_spatial">non_spatial</a></code></li>
<li><code><a title="phi.math.nonzero" href="#phi.math.nonzero">nonzero</a></code></li>
<li><code><a title="phi.math.normalize_to" href="#phi.math.normalize_to">normalize_to</a></code></li>
<li><code><a title="phi.math.numpy" href="#phi.math.numpy">numpy</a></code></li>
<li><code><a title="phi.math.ones" href="#phi.math.ones">ones</a></code></li>
<li><code><a title="phi.math.ones_like" href="#phi.math.ones_like">ones_like</a></code></li>
<li><code><a title="phi.math.pack_dims" href="#phi.math.pack_dims">pack_dims</a></code></li>
<li><code><a title="phi.math.pad" href="#phi.math.pad">pad</a></code></li>
<li><code><a title="phi.math.precision" href="#phi.math.precision">precision</a></code></li>
<li><code><a title="phi.math.print" href="#phi.math.print">print</a></code></li>
<li><code><a title="phi.math.print_gradient" href="#phi.math.print_gradient">print_gradient</a></code></li>
<li><code><a title="phi.math.prod" href="#phi.math.prod">prod</a></code></li>
<li><code><a title="phi.math.quantile" href="#phi.math.quantile">quantile</a></code></li>
<li><code><a title="phi.math.random_normal" href="#phi.math.random_normal">random_normal</a></code></li>
<li><code><a title="phi.math.random_uniform" href="#phi.math.random_uniform">random_uniform</a></code></li>
<li><code><a title="phi.math.range" href="#phi.math.range">range</a></code></li>
<li><code><a title="phi.math.range_tensor" href="#phi.math.range_tensor">range_tensor</a></code></li>
<li><code><a title="phi.math.real" href="#phi.math.real">real</a></code></li>
<li><code><a title="phi.math.rename_dims" href="#phi.math.rename_dims">rename_dims</a></code></li>
<li><code><a title="phi.math.reshaped_native" href="#phi.math.reshaped_native">reshaped_native</a></code></li>
<li><code><a title="phi.math.reshaped_numpy" href="#phi.math.reshaped_numpy">reshaped_numpy</a></code></li>
<li><code><a title="phi.math.reshaped_tensor" href="#phi.math.reshaped_tensor">reshaped_tensor</a></code></li>
<li><code><a title="phi.math.rotate_vector" href="#phi.math.rotate_vector">rotate_vector</a></code></li>
<li><code><a title="phi.math.round" href="#phi.math.round">round</a></code></li>
<li><code><a title="phi.math.sample_subgrid" href="#phi.math.sample_subgrid">sample_subgrid</a></code></li>
<li><code><a title="phi.math.scatter" href="#phi.math.scatter">scatter</a></code></li>
<li><code><a title="phi.math.seed" href="#phi.math.seed">seed</a></code></li>
<li><code><a title="phi.math.set_global_precision" href="#phi.math.set_global_precision">set_global_precision</a></code></li>
<li><code><a title="phi.math.shape" href="#phi.math.shape">shape</a></code></li>
<li><code><a title="phi.math.shift" href="#phi.math.shift">shift</a></code></li>
<li><code><a title="phi.math.sigmoid" href="#phi.math.sigmoid">sigmoid</a></code></li>
<li><code><a title="phi.math.sign" href="#phi.math.sign">sign</a></code></li>
<li><code><a title="phi.math.sin" href="#phi.math.sin">sin</a></code></li>
<li><code><a title="phi.math.solve_linear" href="#phi.math.solve_linear">solve_linear</a></code></li>
<li><code><a title="phi.math.solve_nonlinear" href="#phi.math.solve_nonlinear">solve_nonlinear</a></code></li>
<li><code><a title="phi.math.spatial" href="#phi.math.spatial">spatial</a></code></li>
<li><code><a title="phi.math.spatial_gradient" href="#phi.math.spatial_gradient">spatial_gradient</a></code></li>
<li><code><a title="phi.math.sqrt" href="#phi.math.sqrt">sqrt</a></code></li>
<li><code><a title="phi.math.stack" href="#phi.math.stack">stack</a></code></li>
<li><code><a title="phi.math.std" href="#phi.math.std">std</a></code></li>
<li><code><a title="phi.math.stop_gradient" href="#phi.math.stop_gradient">stop_gradient</a></code></li>
<li><code><a title="phi.math.sum" href="#phi.math.sum">sum</a></code></li>
<li><code><a title="phi.math.tan" href="#phi.math.tan">tan</a></code></li>
<li><code><a title="phi.math.tensor" href="#phi.math.tensor">tensor</a></code></li>
<li><code><a title="phi.math.to_complex" href="#phi.math.to_complex">to_complex</a></code></li>
<li><code><a title="phi.math.to_dict" href="#phi.math.to_dict">to_dict</a></code></li>
<li><code><a title="phi.math.to_float" href="#phi.math.to_float">to_float</a></code></li>
<li><code><a title="phi.math.to_int32" href="#phi.math.to_int32">to_int32</a></code></li>
<li><code><a title="phi.math.to_int64" href="#phi.math.to_int64">to_int64</a></code></li>
<li><code><a title="phi.math.transpose" href="#phi.math.transpose">transpose</a></code></li>
<li><code><a title="phi.math.unpack_dim" href="#phi.math.unpack_dim">unpack_dim</a></code></li>
<li><code><a title="phi.math.unpack_dims" href="#phi.math.unpack_dims">unpack_dims</a></code></li>
<li><code><a title="phi.math.unstack" href="#phi.math.unstack">unstack</a></code></li>
<li><code><a title="phi.math.upsample2x" href="#phi.math.upsample2x">upsample2x</a></code></li>
<li><code><a title="phi.math.vec" href="#phi.math.vec">vec</a></code></li>
<li><code><a title="phi.math.vec_abs" href="#phi.math.vec_abs">vec_abs</a></code></li>
<li><code><a title="phi.math.vec_length" href="#phi.math.vec_length">vec_length</a></code></li>
<li><code><a title="phi.math.vec_normalize" href="#phi.math.vec_normalize">vec_normalize</a></code></li>
<li><code><a title="phi.math.vec_squared" href="#phi.math.vec_squared">vec_squared</a></code></li>
<li><code><a title="phi.math.where" href="#phi.math.where">where</a></code></li>
<li><code><a title="phi.math.wrap" href="#phi.math.wrap">wrap</a></code></li>
<li><code><a title="phi.math.zeros" href="#phi.math.zeros">zeros</a></code></li>
<li><code><a title="phi.math.zeros_like" href="#phi.math.zeros_like">zeros_like</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="phi.math.ConvergenceException" href="#phi.math.ConvergenceException">ConvergenceException</a></code></h4>
<ul class="">
<li><code><a title="phi.math.ConvergenceException.result" href="#phi.math.ConvergenceException.result">result</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.DType" href="#phi.math.DType">DType</a></code></h4>
<ul class="">
<li><code><a title="phi.math.DType.as_dtype" href="#phi.math.DType.as_dtype">as_dtype</a></code></li>
<li><code><a title="phi.math.DType.bits" href="#phi.math.DType.bits">bits</a></code></li>
<li><code><a title="phi.math.DType.itemsize" href="#phi.math.DType.itemsize">itemsize</a></code></li>
<li><code><a title="phi.math.DType.kind" href="#phi.math.DType.kind">kind</a></code></li>
<li><code><a title="phi.math.DType.precision" href="#phi.math.DType.precision">precision</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.Dict" href="#phi.math.Dict">Dict</a></code></h4>
<ul class="">
<li><code><a title="phi.math.Dict.copy" href="#phi.math.Dict.copy">copy</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.Diverged" href="#phi.math.Diverged">Diverged</a></code></h4>
</li>
<li>
<h4><code><a title="phi.math.IncompatibleShapes" href="#phi.math.IncompatibleShapes">IncompatibleShapes</a></code></h4>
</li>
<li>
<h4><code><a title="phi.math.LinearFunction" href="#phi.math.LinearFunction">LinearFunction</a></code></h4>
<ul class="">
<li><code><a title="phi.math.LinearFunction.sparse_matrix" href="#phi.math.LinearFunction.sparse_matrix">sparse_matrix</a></code></li>
<li><code><a title="phi.math.LinearFunction.sparse_matrix_and_bias" href="#phi.math.LinearFunction.sparse_matrix_and_bias">sparse_matrix_and_bias</a></code></li>
<li><code><a title="phi.math.LinearFunction.stencil_inspector" href="#phi.math.LinearFunction.stencil_inspector">stencil_inspector</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.NotConverged" href="#phi.math.NotConverged">NotConverged</a></code></h4>
</li>
<li>
<h4><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></h4>
<ul class="two-column">
<li><code><a title="phi.math.Shape.after_gather" href="#phi.math.Shape.after_gather">after_gather</a></code></li>
<li><code><a title="phi.math.Shape.after_pad" href="#phi.math.Shape.after_pad">after_pad</a></code></li>
<li><code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">batch</a></code></li>
<li><code><a title="phi.math.Shape.batch_rank" href="#phi.math.Shape.batch_rank">batch_rank</a></code></li>
<li><code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">channel</a></code></li>
<li><code><a title="phi.math.Shape.channel_rank" href="#phi.math.Shape.channel_rank">channel_rank</a></code></li>
<li><code><a title="phi.math.Shape.flipped" href="#phi.math.Shape.flipped">flipped</a></code></li>
<li><code><a title="phi.math.Shape.get_dim_type" href="#phi.math.Shape.get_dim_type">get_dim_type</a></code></li>
<li><code><a title="phi.math.Shape.get_item_names" href="#phi.math.Shape.get_item_names">get_item_names</a></code></li>
<li><code><a title="phi.math.Shape.get_size" href="#phi.math.Shape.get_size">get_size</a></code></li>
<li><code><a title="phi.math.Shape.get_sizes" href="#phi.math.Shape.get_sizes">get_sizes</a></code></li>
<li><code><a title="phi.math.Shape.get_type" href="#phi.math.Shape.get_type">get_type</a></code></li>
<li><code><a title="phi.math.Shape.get_types" href="#phi.math.Shape.get_types">get_types</a></code></li>
<li><code><a title="phi.math.Shape.index" href="#phi.math.Shape.index">index</a></code></li>
<li><code><a title="phi.math.Shape.indices" href="#phi.math.Shape.indices">indices</a></code></li>
<li><code><a title="phi.math.Shape.instance" href="#phi.math.Shape.instance">instance</a></code></li>
<li><code><a title="phi.math.Shape.instance_rank" href="#phi.math.Shape.instance_rank">instance_rank</a></code></li>
<li><code><a title="phi.math.Shape.is_empty" href="#phi.math.Shape.is_empty">is_empty</a></code></li>
<li><code><a title="phi.math.Shape.is_non_uniform" href="#phi.math.Shape.is_non_uniform">is_non_uniform</a></code></li>
<li><code><a title="phi.math.Shape.is_uniform" href="#phi.math.Shape.is_uniform">is_uniform</a></code></li>
<li><code><a title="phi.math.Shape.mask" href="#phi.math.Shape.mask">mask</a></code></li>
<li><code><a title="phi.math.Shape.meshgrid" href="#phi.math.Shape.meshgrid">meshgrid</a></code></li>
<li><code><a title="phi.math.Shape.name" href="#phi.math.Shape.name">name</a></code></li>
<li><code><a title="phi.math.Shape.names" href="#phi.math.Shape.names">names</a></code></li>
<li><code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">non_batch</a></code></li>
<li><code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">non_channel</a></code></li>
<li><code><a title="phi.math.Shape.non_instance" href="#phi.math.Shape.non_instance">non_instance</a></code></li>
<li><code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">non_singleton</a></code></li>
<li><code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">non_spatial</a></code></li>
<li><code><a title="phi.math.Shape.only" href="#phi.math.Shape.only">only</a></code></li>
<li><code><a title="phi.math.Shape.prepare_gather" href="#phi.math.Shape.prepare_gather">prepare_gather</a></code></li>
<li><code><a title="phi.math.Shape.rank" href="#phi.math.Shape.rank">rank</a></code></li>
<li><code><a title="phi.math.Shape.replace" href="#phi.math.Shape.replace">replace</a></code></li>
<li><code><a title="phi.math.Shape.reversed" href="#phi.math.Shape.reversed">reversed</a></code></li>
<li><code><a title="phi.math.Shape.shape" href="#phi.math.Shape.shape">shape</a></code></li>
<li><code><a title="phi.math.Shape.size" href="#phi.math.Shape.size">size</a></code></li>
<li><code><a title="phi.math.Shape.sizes" href="#phi.math.Shape.sizes">sizes</a></code></li>
<li><code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">spatial</a></code></li>
<li><code><a title="phi.math.Shape.spatial_rank" href="#phi.math.Shape.spatial_rank">spatial_rank</a></code></li>
<li><code><a title="phi.math.Shape.type" href="#phi.math.Shape.type">type</a></code></li>
<li><code><a title="phi.math.Shape.unstack" href="#phi.math.Shape.unstack">unstack</a></code></li>
<li><code><a title="phi.math.Shape.volume" href="#phi.math.Shape.volume">volume</a></code></li>
<li><code><a title="phi.math.Shape.well_defined" href="#phi.math.Shape.well_defined">well_defined</a></code></li>
<li><code><a title="phi.math.Shape.with_dim_size" href="#phi.math.Shape.with_dim_size">with_dim_size</a></code></li>
<li><code><a title="phi.math.Shape.with_size" href="#phi.math.Shape.with_size">with_size</a></code></li>
<li><code><a title="phi.math.Shape.with_sizes" href="#phi.math.Shape.with_sizes">with_sizes</a></code></li>
<li><code><a title="phi.math.Shape.without" href="#phi.math.Shape.without">without</a></code></li>
<li><code><a title="phi.math.Shape.without_sizes" href="#phi.math.Shape.without_sizes">without_sizes</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code></h4>
<ul class="two-column">
<li><code><a title="phi.math.Solve.absolute_tolerance" href="#phi.math.Solve.absolute_tolerance">absolute_tolerance</a></code></li>
<li><code><a title="phi.math.Solve.gradient_solve" href="#phi.math.Solve.gradient_solve">gradient_solve</a></code></li>
<li><code><a title="phi.math.Solve.max_iterations" href="#phi.math.Solve.max_iterations">max_iterations</a></code></li>
<li><code><a title="phi.math.Solve.method" href="#phi.math.Solve.method">method</a></code></li>
<li><code><a title="phi.math.Solve.preprocess_y" href="#phi.math.Solve.preprocess_y">preprocess_y</a></code></li>
<li><code><a title="phi.math.Solve.relative_tolerance" href="#phi.math.Solve.relative_tolerance">relative_tolerance</a></code></li>
<li><code><a title="phi.math.Solve.suppress" href="#phi.math.Solve.suppress">suppress</a></code></li>
<li><code><a title="phi.math.Solve.x0" href="#phi.math.Solve.x0">x0</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.SolveInfo" href="#phi.math.SolveInfo">SolveInfo</a></code></h4>
<ul class="">
<li><code><a title="phi.math.SolveInfo.converged" href="#phi.math.SolveInfo.converged">converged</a></code></li>
<li><code><a title="phi.math.SolveInfo.convergence_check" href="#phi.math.SolveInfo.convergence_check">convergence_check</a></code></li>
<li><code><a title="phi.math.SolveInfo.diverged" href="#phi.math.SolveInfo.diverged">diverged</a></code></li>
<li><code><a title="phi.math.SolveInfo.function_evaluations" href="#phi.math.SolveInfo.function_evaluations">function_evaluations</a></code></li>
<li><code><a title="phi.math.SolveInfo.iterations" href="#phi.math.SolveInfo.iterations">iterations</a></code></li>
<li><code><a title="phi.math.SolveInfo.method" href="#phi.math.SolveInfo.method">method</a></code></li>
<li><code><a title="phi.math.SolveInfo.msg" href="#phi.math.SolveInfo.msg">msg</a></code></li>
<li><code><a title="phi.math.SolveInfo.residual" href="#phi.math.SolveInfo.residual">residual</a></code></li>
<li><code><a title="phi.math.SolveInfo.snapshot" href="#phi.math.SolveInfo.snapshot">snapshot</a></code></li>
<li><code><a title="phi.math.SolveInfo.solve" href="#phi.math.SolveInfo.solve">solve</a></code></li>
<li><code><a title="phi.math.SolveInfo.solve_time" href="#phi.math.SolveInfo.solve_time">solve_time</a></code></li>
<li><code><a title="phi.math.SolveInfo.x" href="#phi.math.SolveInfo.x">x</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.SolveTape" href="#phi.math.SolveTape">SolveTape</a></code></h4>
</li>
<li>
<h4><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></h4>
<ul class="two-column">
<li><code><a title="phi.math.Tensor.all" href="#phi.math.Tensor.all">all</a></code></li>
<li><code><a title="phi.math.Tensor.any" href="#phi.math.Tensor.any">any</a></code></li>
<li><code><a title="phi.math.Tensor.available" href="#phi.math.Tensor.available">available</a></code></li>
<li><code><a title="phi.math.Tensor.default_backend" href="#phi.math.Tensor.default_backend">default_backend</a></code></li>
<li><code><a title="phi.math.Tensor.device" href="#phi.math.Tensor.device">device</a></code></li>
<li><code><a title="phi.math.Tensor.dimension" href="#phi.math.Tensor.dimension">dimension</a></code></li>
<li><code><a title="phi.math.Tensor.dtype" href="#phi.math.Tensor.dtype">dtype</a></code></li>
<li><code><a title="phi.math.Tensor.finite_max" href="#phi.math.Tensor.finite_max">finite_max</a></code></li>
<li><code><a title="phi.math.Tensor.finite_mean" href="#phi.math.Tensor.finite_mean">finite_mean</a></code></li>
<li><code><a title="phi.math.Tensor.finite_min" href="#phi.math.Tensor.finite_min">finite_min</a></code></li>
<li><code><a title="phi.math.Tensor.finite_sum" href="#phi.math.Tensor.finite_sum">finite_sum</a></code></li>
<li><code><a title="phi.math.Tensor.flip" href="#phi.math.Tensor.flip">flip</a></code></li>
<li><code><a title="phi.math.Tensor.imag" href="#phi.math.Tensor.imag">imag</a></code></li>
<li><code><a title="phi.math.Tensor.max" href="#phi.math.Tensor.max">max</a></code></li>
<li><code><a title="phi.math.Tensor.mean" href="#phi.math.Tensor.mean">mean</a></code></li>
<li><code><a title="phi.math.Tensor.min" href="#phi.math.Tensor.min">min</a></code></li>
<li><code><a title="phi.math.Tensor.native" href="#phi.math.Tensor.native">native</a></code></li>
<li><code><a title="phi.math.Tensor.numpy" href="#phi.math.Tensor.numpy">numpy</a></code></li>
<li><code><a title="phi.math.Tensor.pack" href="#phi.math.Tensor.pack">pack</a></code></li>
<li><code><a title="phi.math.Tensor.rank" href="#phi.math.Tensor.rank">rank</a></code></li>
<li><code><a title="phi.math.Tensor.real" href="#phi.math.Tensor.real">real</a></code></li>
<li><code><a title="phi.math.Tensor.shape" href="#phi.math.Tensor.shape">shape</a></code></li>
<li><code><a title="phi.math.Tensor.std" href="#phi.math.Tensor.std">std</a></code></li>
<li><code><a title="phi.math.Tensor.sum" href="#phi.math.Tensor.sum">sum</a></code></li>
<li><code><a title="phi.math.Tensor.unpack" href="#phi.math.Tensor.unpack">unpack</a></code></li>
<li><code><a title="phi.math.Tensor.unstack" href="#phi.math.Tensor.unstack">unstack</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>