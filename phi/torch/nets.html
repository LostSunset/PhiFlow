<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>phi.torch.nets API documentation</title>
<meta name="description" content="PyTorch implementation of the unified machine learning API.
Equivalent functions also exist for the other frameworks â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>phi.torch.nets</code></h1>
</header>
<section id="section-intro">
<p>PyTorch implementation of the unified machine learning API.
Equivalent functions also exist for the other frameworks.</p>
<p>For API documentation, see <a href="https://tum-pbs.github.io/PhiFlow/Network_API">https://tum-pbs.github.io/PhiFlow/Network_API</a> .</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
PyTorch implementation of the unified machine learning API.
Equivalent functions also exist for the other frameworks.

For API documentation, see https://tum-pbs.github.io/PhiFlow/Network_API .
&#34;&#34;&#34;
from typing import Callable, Union, Sequence

import numpy
import numpy as np
import torch
import torch.nn as nn
from torch import optim

from . import TORCH
from ._torch_backend import register_module_call
from .. import math
from ..math import channel


def parameter_count(model: nn.Module) -&gt; int:
    &#34;&#34;&#34;
    Counts the number of parameters in a model.

    Args:
        model: PyTorch model

    Returns:
        `int`
    &#34;&#34;&#34;
    total = 0
    for parameter in model.parameters():
        total += numpy.prod(parameter.shape)
    return int(total)


def get_parameters(net: nn.Module, wrap=True) -&gt; dict:
    if not wrap:
        return {name: param for name, param in net.named_parameters()}
    result = {}
    for name, param in net.named_parameters():
        if name.endswith(&#39;.weight&#39;):
            if param.ndim == 2:
                phi_tensor = math.wrap(param, channel(&#39;input,output&#39;))
            elif param.ndim == 3:
                phi_tensor = math.wrap(param, channel(&#39;x,input,output&#39;))
            elif param.ndim == 4:
                phi_tensor = math.wrap(param, channel(&#39;x,y,input,output&#39;))
            elif param.ndim == 5:
                phi_tensor = math.wrap(param, channel(&#39;x,y,z,input,output&#39;))
        elif name.endswith(&#39;.bias&#39;):
            phi_tensor = math.wrap(param, channel(&#39;output&#39;))
        else:
            raise NotImplementedError
        result[name] = phi_tensor
    return result


def save_state(obj: Union[nn.Module, optim.Optimizer], path: str):
    &#34;&#34;&#34;
    Write the state of a module or optimizer to a file.

    See Also:
        `load_state()`

    Args:
        obj: `torch.nn.Module or torch.optim.Optimizer`
        path: File path as `str`.
    &#34;&#34;&#34;
    if not path.endswith(&#39;.pth&#39;):
        path += &#39;.pth&#39;
    torch.save(obj.state_dict(), path)


def load_state(obj: Union[nn.Module, optim.Optimizer], path: str):
    &#34;&#34;&#34;
    Read the state of a module or optimizer from a file.

    See Also:
        `save_state()`

    Args:
        obj: `torch.nn.Module or torch.optim.Optimizer`
        path: File path as `str`.
    &#34;&#34;&#34;
    if not path.endswith(&#39;.pth&#39;):
        path += &#39;.pth&#39;
    obj.load_state_dict(torch.load(path))


def update_weights(net: nn.Module, optimizer: optim.Optimizer, loss_function: Callable, *loss_args, check_nan=False, **loss_kwargs):
    &#34;&#34;&#34;
    Computes the gradients of `loss_function` w.r.t. the parameters of `net` and updates its weights using `optimizer`.

    This is the PyTorch version. Analogue functions exist for other learning frameworks.

    Args:
        net: Learning model.
        optimizer: Optimizer.
        loss_function: Loss function, called as `loss_function(*loss_args, **loss_kwargs)`.
        *loss_args: Arguments given to `loss_function`.
        **loss_kwargs: Keyword arguments given to `loss_function`.

    Returns:
        Output of `loss_function`.
    &#34;&#34;&#34;
    optimizer.zero_grad()
    output = loss_function(*loss_args, **loss_kwargs)
    loss = output[0] if isinstance(output, tuple) else output
    loss.sum.backward()
    if isinstance(optimizer, optim.LBFGS):
        def closure():
            result = loss_function(*loss_args, **loss_kwargs)
            loss_val = result[0] if isinstance(result, tuple) else result
            return loss_val.sum
        optimizer.step(closure=closure)
    else:
        if check_nan:
            for p in net.parameters():
                if not torch.all(torch.isfinite(p.grad)):
                    raise RuntimeError(f&#34;NaN in network gradient detected. Parameter: {p}&#34;)
        optimizer.step()
    return output


def adam(net: nn.Module, learning_rate: float = 1e-3, betas=(0.9, 0.999), epsilon=1e-07):
    &#34;&#34;&#34;
    Creates an Adam optimizer for `net`, alias for [`torch.optim.Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html).
    Analogue functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return optim.Adam(net.parameters(), learning_rate, betas, epsilon)


def sgd(net: nn.Module, learning_rate: float = 1e-3, momentum=0, dampening=0, weight_decay=0, nesterov=False):
    &#34;&#34;&#34;
    Creates an SGD optimizer for &#39;net&#39;, alias for [&#39;torch.optim.SGD&#39;](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)
    Analogue functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return optim.SGD(net.parameters(), learning_rate, momentum, dampening, weight_decay, nesterov)


def adagrad(net: nn.Module, learning_rate: float = 1e-3, lr_decay=0, weight_decay=0, initial_accumulator_value=0,
            eps=1e-10):
    &#34;&#34;&#34;
    Creates an Adagrad optimizer for &#39;net&#39;, alias for [&#39;torch.optim.Adagrad&#39;](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html)
    Analogue functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return optim.Adagrad(net.parameters(), learning_rate, lr_decay, weight_decay, initial_accumulator_value, eps)


def rmsprop(net: nn.Module, learning_rate: float = 1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0,
            centered=False):
    &#34;&#34;&#34;
    Creates an RMSProp optimizer for &#39;net&#39;, alias for [&#39;torch.optim.RMSprop&#39;](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html)
    Analogue functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return optim.RMSprop(net.parameters(), learning_rate, alpha, eps, weight_decay, momentum, centered)


def _bias0(conv):
    def initialize(*args, **kwargs):
        module = conv(*args, **kwargs)
        module.bias.data.fill_(0)
        return module
    return initialize


CONV = [None, _bias0(nn.Conv1d), _bias0(nn.Conv2d), _bias0(nn.Conv3d)]
NORM = [None, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d]
ACTIVATIONS = {&#39;ReLU&#39;: nn.ReLU, &#39;Sigmoid&#39;: nn.Sigmoid, &#39;tanh&#39;: nn.Tanh, &#39;SiLU&#39;: nn.SiLU, &#39;GeLU&#39;: nn.GELU}


def dense_net(in_channels: int,
              out_channels: int,
              layers: Sequence[int],
              batch_norm=False,
              activation: Union[str, Callable] = &#39;ReLU&#39;,
              softmax=False) -&gt; nn.Module:
    &#34;&#34;&#34;
    Fully-connected neural networks are available in Î¦-Flow via dense_net().
    Arguments:
        in_channels : size of input layer, int
        out_channels = size of output layer, int
        layers : tuple of linear layers between input and output neurons, list or tuple
        activation : activation function used within the layers, string
        batch_norm : use of batch norm after each linear layer, bool

    Returns:
        Dense net model as specified by input arguments
    &#34;&#34;&#34;
    layers = [in_channels, *layers, out_channels]
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    net = DenseNet(layers, activation, batch_norm, softmax)
    return net.to(TORCH.get_default_device().ref)


class DenseNet(nn.Module):

    def __init__(self,
                 layers: list,
                 activation: type,
                 batch_norm: bool,
                 use_softmax: bool):
        super(DenseNet, self).__init__()
        self._layers = layers
        self._activation = activation
        self._batch_norm = batch_norm
        for i, (s1, s2) in enumerate(zip(layers[:-2], layers[1:-1])):
            self.add_module(f&#39;linear{i}&#39;, _bias0(nn.Linear)(s1, s2, bias=True))
            if batch_norm:
                self.add_module(f&#39;norm{i}&#39;, nn.BatchNorm1d(s2))
        self.add_module(f&#39;linear_out&#39;, _bias0(nn.Linear)(layers[-2], layers[-1], bias=True))
        self.softmax = nn.Softmax() if use_softmax else None

    def forward(self, x):
        register_module_call(self)
        x = TORCH.as_tensor(x)
        for i in range(len(self._layers) - 2):
            x = self._activation()(getattr(self, f&#39;linear{i}&#39;)(x))
            if self._batch_norm:
                x = getattr(self, f&#39;norm{i}&#39;)(x)
        x = getattr(self, f&#39;linear_out&#39;)(x)
        if self.softmax:
            x = self.softmax(x)
        return x


def u_net(in_channels: int,
          out_channels: int,
          levels: int = 4,
          filters: Union[int, tuple, list] = 16,
          batch_norm: bool = True,
          activation: Union[str, type] = &#39;ReLU&#39;,
          in_spatial: Union[tuple, int] = 2,
          periodic=False,
          use_res_blocks: bool = False,
          **kwargs) -&gt; nn.Module:
    &#34;&#34;&#34;
    Î¦Flow provides a built-in U-net architecture, classically popular for Semantic Segmentation in Computer Vision, composed of downsampling and upsampling layers.

    Arguments:

        in_channels: input channels of the feature map, dtype : int
        out_channels : output channels of the feature map, dtype : int
        levels : number of levels of down-sampling and upsampling, dtype : int
        filters : filter sizes at each down/up sampling convolutional layer, if the input is integer all conv layers have the same filter size,
        activation : activation function used within the layers, dtype : string
        batch_norm : use of batchnorm after each conv layer, dtype : bool
        in_spatial : spatial dimensions of the input feature map, dtype : int
        use_res_blocks : use convolutional blocks with skip connections instead of regular convolutional blocks, dtype : bool

    Returns:

        U-net model as specified by input arguments

    &#34;&#34;&#34;
    if isinstance(filters, (tuple, list)):
        assert len(filters) == levels, f&#34;List of filters has length {len(filters)} but u-net has {levels} levels.&#34;
    else:
        filters = (filters,) * levels
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(in_spatial, int):
        d = in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    net = UNet(d, in_channels, out_channels, filters, batch_norm, activation, periodic, use_res_blocks)
    return net.to(TORCH.get_default_device().ref)


class UNet(nn.Module):

    def __init__(self, d: int, in_channels: int, out_channels: int, filters: tuple, batch_norm: bool, activation: type, periodic: bool, use_res_blocks: bool):
        super(UNet, self).__init__()
        self._levels = len(filters)
        self._spatial_rank = d
        if use_res_blocks:
            self.add_module(&#39;inc&#39;, resnet_block(d, in_channels, filters[0], batch_norm, activation, periodic))
        else:
            self.add_module(&#39;inc&#39;, DoubleConv(d, in_channels, filters[0], filters[0], batch_norm, activation, periodic))
        for i in range(1, self._levels):
            self.add_module(f&#39;down{i}&#39;, Down(d, filters[i - 1], filters[i], batch_norm, activation, periodic, use_res_blocks))
            self.add_module(f&#39;up{i}&#39;, Up(d, filters[i] + filters[i - 1], filters[i - 1], batch_norm, activation, periodic, use_res_blocks))
        self.add_module(&#39;outc&#39;, CONV[d](filters[0], out_channels, kernel_size=1))

    def forward(self, x):
        register_module_call(self)
        x = TORCH.as_tensor(x)
        x = self.inc(x)
        xs = [x]
        for i in range(1, self._levels):
            x = getattr(self, f&#39;down{i}&#39;)(x)
            xs.insert(0, x)
        for i in range(1, self._levels):
            x = getattr(self, f&#39;up{i}&#39;)(x, xs[i])
        x = self.outc(x)
        return x


class DoubleConv(nn.Module):
    &#34;&#34;&#34;(convolution =&gt; [BN] =&gt; ReLU) * 2&#34;&#34;&#34;

    def __init__(self, d: int, in_channels: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: type, periodic: bool):
        super().__init__()
        self.add_module(&#39;double_conv&#39;, nn.Sequential(
            CONV[d](in_channels, mid_channels, kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
            NORM[d](mid_channels) if batch_norm else nn.Identity(),
            activation(),
            CONV[d](mid_channels, out_channels, kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
            NORM[d](out_channels) if batch_norm else nn.Identity(),
            nn.ReLU(inplace=True)
        ))

    def forward(self, x):
        return self.double_conv(x)


MAX_POOL = [None, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d]


class Down(nn.Module):
    &#34;&#34;&#34;Downscaling with maxpool then double conv or resnet_block&#34;&#34;&#34;

    def __init__(self, d: int, in_channels: int, out_channels: int, batch_norm: bool, activation: Union[str, type], use_res_blocks: bool, periodic):
        super().__init__()
        self.add_module(&#39;maxpool&#39;, MAX_POOL[d](2))
        if use_res_blocks:
            self.add_module(&#39;conv&#39;, resnet_block(d, in_channels, out_channels, batch_norm, activation, periodic))
        else:
            self.add_module(&#39;conv&#39;, DoubleConv(d, in_channels, out_channels, out_channels, batch_norm, activation, periodic))

    def forward(self, x):
        x = self.maxpool(x)
        return self.conv(x)


class Up(nn.Module):
    &#34;&#34;&#34;Upscaling then double conv&#34;&#34;&#34;

    _MODES = [None, &#39;linear&#39;, &#39;bilinear&#39;, &#39;trilinear&#39;]

    def __init__(self, d: int, in_channels: int, out_channels: int, batch_norm: bool, activation: type, periodic: bool, use_res_blocks: bool):
        super().__init__()
        up = nn.Upsample(scale_factor=2, mode=Up._MODES[d])
        if use_res_blocks:
            conv = resnet_block(d, in_channels, out_channels, batch_norm, activation, periodic)
        else:
            conv = DoubleConv(d, in_channels, out_channels, in_channels // 2, batch_norm, activation, periodic)
        self.add_module(&#39;up&#39;, up)
        self.add_module(&#39;conv&#39;, conv)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        # input is CHW
        # diff = [x2.size()[i] - x1.size()[i] for i in range(2, len(x1.shape))]
        # x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
        #                 diffY // 2, diffY - diffY // 2])
        # if you have padding issues, see
        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a
        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)


class ConvNet(nn.Module):

    def __init__(self, in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic: bool):
        super(ConvNet, self).__init__()
        activation = ACTIVATIONS[activation]
        if len(layers) &lt; 1:
            layers.append(out_channels)
        self.layers = layers
        self.add_module(f&#39;Conv_in&#39;, nn.Sequential(
            CONV[in_spatial](in_channels, layers[0], kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
            NORM[in_spatial](layers[0]) if batch_norm else nn.Identity(),
            activation()))
        for i in range(1, len(layers)):
            self.add_module(f&#39;Conv{i}&#39;, nn.Sequential(
                CONV[in_spatial](layers[i - 1], layers[i], kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
                NORM[in_spatial](layers[i]) if batch_norm else nn.Identity(),
                activation()))
        self.add_module(f&#39;Conv_out&#39;, CONV[in_spatial](layers[len(layers) - 1], out_channels, kernel_size=1))

    def forward(self, x):
        x = getattr(self, f&#39;Conv_in&#39;)(x)
        for i in range(1, len(self.layers)):
            x = getattr(self, f&#39;Conv{i}&#39;)(x)
        x = getattr(self, f&#39;Conv_out&#39;)(x)
        return x


def conv_net(in_channels: int,
             out_channels: int,
             layers: Sequence[int],
             batch_norm: bool = False,
             activation: Union[str, type] = &#39;ReLU&#39;,
             in_spatial: Union[int, tuple] = 2,
             periodic=False,
             **kwargs) -&gt; nn.Module:
    &#34;&#34;&#34;
    Built in Conv-Nets are also provided. Contrary to the classical convolutional neural networks, the feature map spatial size remains the same throughout the layers. Each layer of the network is essentially a convolutional block comprising of two conv layers. A filter size of 3 is used in the convolutional layers.
    Arguments:

        in_channels : input channels of the feature map, dtype : int
        out_channels : output channels of the feature map, dtype : int
        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
        activation : activation function used within the layers, dtype : string
        batch_norm : use of batchnorm after each conv layer, dtype : bool
        in_spatial : spatial dimensions of the input feature map, dtype : int

    Returns:

        Conv-net model as specified by input arguments
    &#34;&#34;&#34;
    if isinstance(in_spatial, int):
        d = in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    net = ConvNet(d, in_channels, out_channels, layers, batch_norm, activation, periodic)
    net = net.to(TORCH.get_default_device().ref)
    return net


class resnet_block(nn.Module):

    def __init__(self, in_spatial, in_channels, out_channels, batch_norm, activation, periodic: bool):
        # Since in_channels and out_channels might be different
        # we need a sampling layer for up/down sampling input
        # in order to add it as a skip connection
        super(resnet_block, self).__init__()
        if in_channels != out_channels:
            self.sample_input = CONV[in_spatial](in_channels, out_channels, kernel_size=1, padding=0)
            self.bn_sample = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
        else:
            self.sample_input = nn.Identity()
            self.bn_sample = nn.Identity()
        self.activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
        self.bn1 = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
        self.conv1 = CONV[in_spatial](in_channels, out_channels, kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;)
        self.bn2 = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
        self.conv2 = CONV[in_spatial](out_channels, out_channels, kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;)

    def forward(self, x):
        x = TORCH.as_tensor(x)
        out = self.activation()(self.bn1(self.conv1(x)))
        out = self.activation()(self.bn2(self.conv2(out)))
        out = (out + self.bn_sample(self.sample_input(x)))
        return out


class Dense_resnet_block(nn.Module):

    def __init__(self, in_channels, mid_channels, batch_norm, activation):
        super(Dense_resnet_block, self).__init__()
        self.activation = activation
        self.bn1 = NORM[1](in_channels) if batch_norm else nn.Identity()
        self.linear1 = _bias0(nn.Linear)(in_channels, mid_channels)
        self.bn2 = NORM[1](mid_channels) if batch_norm else nn.Identity()
        self.linear2 = _bias0(nn.Linear)(mid_channels, in_channels)

    def forward(self, x):
        x = TORCH.as_tensor(x)
        out = self.activation()(self.bn1(self.linear1(x)))
        out = self.activation()(self.bn2(self.linear2(out)))
        out = out + x
        return out


def get_mask(inputs, reverse_mask, data_format=&#39;NHWC&#39;):
    &#34;&#34;&#34; Compute mask for slicing input feature map for Invertible Nets &#34;&#34;&#34;
    shape = inputs.shape
    if len(shape) == 2:
        N = shape[-1]
        range_n = torch.arange(0, N)
        even_ind = range_n % 2
        checker = torch.reshape(even_ind, (-1, N))
    elif len(shape) == 4:
        H = shape[2] if data_format == &#39;NCHW&#39; else shape[1]
        W = shape[3] if data_format == &#39;NCHW&#39; else shape[2]

        range_h = torch.arange(0, H)
        range_w = torch.arange(0, W)

        even_ind_h = range_h % 2
        even_ind_w = range_w % 2

        ind_h = even_ind_h.unsqueeze(-1).repeat(1, W)
        ind_w = even_ind_w.unsqueeze(0).repeat(H, 1)

        checker = torch.logical_xor(ind_h, ind_w)

        checker = checker.reshape(1, 1, H, W) if data_format == &#39;NCHW&#39; else checker.reshape(1, H, W, 1)
        checker = checker.long()

    else:
        raise ValueError(&#39;Invalid tensor shape. Dimension of the tensor shape must be &#39;
                         &#39;2 (NxD) or 4 (NxCxHxW or NxHxWxC), got {}.&#39;.format(inputs.get_shape().as_list()))

    if reverse_mask:
        checker = 1 - checker

    return checker.to(TORCH.get_default_device().ref)


class ResNet(nn.Module):

    def __init__(self, in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic: bool):
        super(ResNet, self).__init__()
        self.layers = layers
        if len(self.layers) &lt; 1:
            layers.append(out_channels)
        self.add_module(&#39;Res_in&#39;, resnet_block(in_spatial, in_channels, layers[0], batch_norm, activation, periodic))
        for i in range(1, len(layers)):
            self.add_module(f&#39;Res{i}&#39;, resnet_block(in_spatial, layers[i - 1], layers[i], batch_norm, activation, periodic))
        self.add_module(&#39;Res_out&#39;, CONV[in_spatial](layers[len(layers) - 1], out_channels, kernel_size=1))

    def forward(self, x):
        x = TORCH.as_tensor(x)
        x = getattr(self, &#39;Res_in&#39;)(x)
        for i in range(1, len(self.layers)):
            x = getattr(self, f&#39;Res{i}&#39;)(x)
        x = getattr(self, &#39;Res_out&#39;)(x)
        return x


def res_net(in_channels: int,
            out_channels: int,
            layers: Sequence[int],
            batch_norm: bool = False,
            activation: Union[str, type] = &#39;ReLU&#39;,
            in_spatial: Union[int, tuple] = 2,
            periodic=False,
            **kwargs) -&gt; nn.Module:
    &#34;&#34;&#34;
    Built in Res-Nets are provided in the Î¦Flow framework. Similar to the conv-net, the feature map spatial size remains the same throughout the layers.
    These networks use residual blocks composed of two conv layers with a skip connection added from the input to the output feature map.
    A default filter size of 3 is used in the convolutional layers.

    Arguments:

        in_channels : input channels of the feature map, dtype : int
        out_channels : output channels of the feature map, dtype : int
        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
        activation : activation function used within the layers, dtype : string
        batch_norm : use of batchnorm after each conv layer, dtype : bool
        in_spatial : spatial dimensions of the input feature map, dtype : int

    Returns:

        Res-net model as specified by input arguments

    &#34;&#34;&#34;
    if (isinstance(in_spatial, int)):
        d = in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    net = ResNet(d, in_channels, out_channels, layers, batch_norm, activation, periodic)
    net = net.to(TORCH.get_default_device().ref)
    return net


def conv_classifier(in_features: int,
                    in_spatial: Union[tuple, list],
                    num_classes: int,
                    blocks=(64, 128, 256, 256, 512, 512),
                    block_sizes=(2, 2, 3, 3, 3),
                    dense_layers=(4096, 4096, 100),
                    batch_norm=True,
                    activation=&#39;ReLU&#39;,
                    softmax=True,
                    periodic=False):
    &#34;&#34;&#34;
    Based on VGG16.
    &#34;&#34;&#34;
    assert isinstance(in_spatial, (tuple, list))
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    net = ConvClassifier(in_features, in_spatial, num_classes, batch_norm, softmax, blocks, block_sizes, dense_layers, periodic, activation)
    return net.to(TORCH.get_default_device().ref)


class ConvClassifier(nn.Module):

    def __init__(self, in_features, in_spatial: list, num_classes: int, batch_norm: bool, use_softmax: bool, blocks: tuple, block_sizes: tuple, dense_layers: tuple, periodic: bool, activation):
        super(ConvClassifier, self).__init__()
        d = len(in_spatial)
        self.in_spatial = in_spatial
        self._blocks = blocks
        self.add_module(&#39;maxpool&#39;, MAX_POOL[d](2))
        for i, (prev, next) in enumerate(zip((in_features,) + tuple(blocks[:-1]), blocks)):
            block_size = block_sizes[i]
            layers = []
            for j in range(block_size):
                layers.append(CONV[d](prev if j == 0 else next, next, kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;))
                layers.append(NORM[d](next) if batch_norm else nn.Identity())
                layers.append(activation())
            self.add_module(f&#39;conv{i+1}&#39;, nn.Sequential(*layers))
        flat_size = int(np.prod(in_spatial) * blocks[-1] / (2**d) ** len(blocks))
        self.dense_net = dense_net(flat_size, num_classes, dense_layers, batch_norm, activation, use_softmax)
        self.flatten = nn.Flatten()

    def forward(self, x):
        for i in range(len(self._blocks)):
            x = getattr(self, f&#39;conv{i+1}&#39;)(x)
            x = self.maxpool(x)
        x = self.flatten(x)
        x = self.dense_net(x)
        return x


NET = {&#39;u_net&#39;: u_net, &#39;res_net&#39;: res_net, &#39;conv_net&#39;: conv_net}


class CouplingLayer(nn.Module):

    def __init__(self, in_channels, activation, batch_norm, in_spatial, net, reverse_mask):
        super(CouplingLayer, self).__init__()

        self.activation = activation
        self.batch_norm = batch_norm
        self.reverse_mask = reverse_mask

        if in_spatial == 0:  # for in_spatial = 0, use dense layers
            self.s1 = nn.Sequential(Dense_resnet_block(in_channels, in_channels, batch_norm, activation),
                                    torch.nn.Tanh())
            self.t1 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)

            self.s2 = nn.Sequential(Dense_resnet_block(in_channels, in_channels, batch_norm, activation),
                                    torch.nn.Tanh())
            self.t2 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
        else:
            self.s1 = nn.Sequential(NET[net](in_channels=in_channels, out_channels=in_channels,
                                             layers=[], batch_norm=batch_norm, activation=activation,
                                             in_spatial=in_spatial), torch.nn.Tanh())
            self.t1 = NET[net](in_channels=in_channels, out_channels=in_channels,
                               layers=[], batch_norm=batch_norm, activation=activation,
                               in_spatial=in_spatial)
            self.s2 = nn.Sequential(NET[net](in_channels=in_channels, out_channels=in_channels,
                                             layers=[], batch_norm=batch_norm, activation=activation,
                                             in_spatial=in_spatial), torch.nn.Tanh())
            self.t2 = NET[net](in_channels=in_channels, out_channels=in_channels,
                               layers=[], batch_norm=batch_norm, activation=activation,
                               in_spatial=in_spatial)

    def forward(self, x, invert=False):
        x = TORCH.as_tensor(x)
        mask = get_mask(x, self.reverse_mask, &#39;NCHW&#39;)
        if invert:
            v1 = x * mask
            v2 = x * (1 - mask)
            u2 = (1 - mask) * (v2 - self.t1(v1)) * torch.exp(-self.s1(v1))
            u1 = mask * (v1 - self.t2(u2)) * torch.exp(-self.s2(u2))
            return u1 + u2
        else:
            u1 = x * mask
            u2 = x * (1 - mask)
            v1 = mask * (u1 * torch.exp(self.s2(u2)) + self.t2(u2))
            v2 = (1 - mask) * (u2 * torch.exp(self.s1(v1)) + self.t1(v1))
            return v1 + v2


class InvertibleNet(nn.Module):
    def __init__(self, in_channels, num_blocks, activation, batch_norm, in_spatial, net):
        super(InvertibleNet, self).__init__()
        self.num_blocks = num_blocks
        for i in range(num_blocks):
            self.add_module(f&#39;coupling_block{i + 1}&#39;,
                            CouplingLayer(in_channels, activation,
                                          batch_norm, in_spatial, net, (i % 2 == 0)))

    def forward(self, x, backward=False):
        if backward:
            for i in range(self.num_blocks, 0, -1):
                x = getattr(self, f&#39;coupling_block{i}&#39;)(x, backward)
        else:
            for i in range(1, self.num_blocks + 1):
                x = getattr(self, f&#39;coupling_block{i}&#39;)(x, backward)
        return x


def invertible_net(in_channels: int,
                   num_blocks: int,
                   batch_norm: bool = False,
                   net: str = &#39;u_net&#39;,
                   activation: Union[str, type] = &#39;ReLU&#39;,
                   in_spatial: Union[tuple, int] = 2, **kwargs):
    &#34;&#34;&#34;
    Phiflow also provides invertible neural networks that are capable of inverting the output tensor back to the input tensor initially passed.\ These networks have far reaching applications in predicting input parameters of a problem given its observations.\ Invertible nets are composed of multiple concatenated coupling blocks wherein each such block consists of arbitrary neural networks.

    Currently, these arbitrary neural networks could be set to u_net(default), conv_net, res_net or dense_net blocks with in_channels = out_channels.
    The architecture used is popularized by [&#34;Real NVP&#34;](https://arxiv.org/abs/1605.08803).

    Arguments:

        in_channels : input channels of the feature map, dtype : int
        num_blocks : number of coupling blocks inside the invertible net, dtype : int
        activation : activation function used within the layers, dtype : string
        batch_norm : use of batchnorm after each layer, dtype : bool
        in_spatial : spatial dimensions of the input feature map, dtype : int
        net : type of neural network blocks used in coupling layers, dtype : str
        **kwargs : placeholder for arguments not supported by the function

    Returns:

        Invertible Net model as specified by input arguments

    Note: Currently supported values for net are &#39;u_net&#39;(default), &#39;conv_net&#39; and &#39;res_net&#39;.
    For choosing &#39;dense_net&#39; as the network block in coupling layers in_spatial must be set to zero.
    &#34;&#34;&#34;
    if isinstance(in_spatial, tuple):
        in_spatial = len(in_spatial)

    return InvertibleNet(in_channels, num_blocks, activation, batch_norm, in_spatial, net).to(TORCH.get_default_device().ref)


def coupling_layer(in_channels: int,
                   activation: Union[str, type] = &#39;ReLU&#39;,
                   batch_norm=False,
                   reverse_mask=False,
                   in_spatial: Union[tuple, int] = 2):
    if isinstance(in_spatial, tuple):
        in_spatial = len(in_spatial)

    net = CouplingLayer(in_channels, activation, batch_norm, in_spatial, reverse_mask)
    net = net.to(TORCH.get_default_device().ref)
    return net


##################################################################################################################
#  Fourier Neural Operators
#  source: https://github.com/zongyi-li/fourier_neural_operator
###################################################################################################################

class SpectralConv(nn.Module):

    def __init__(self, in_channels, out_channels, modes, in_spatial):

        super(SpectralConv, self).__init__()

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.in_spatial = in_spatial
        assert in_spatial &gt;= 1 and in_spatial &lt;= 3
        if isinstance(modes, int):
            mode = modes
            modes = [mode for i in range(in_spatial)]

        self.scale = 1 / (in_channels * out_channels)

        self.modes = {i + 1: modes[i] for i in range(len(modes))}
        self.weights = {}

        rand_shape = [in_channels, out_channels]
        rand_shape += [self.modes[i] for i in range(1, in_spatial + 1)]

        for i in range(2 ** (in_spatial - 1)):
            self.weights[f&#39;w{i + 1}&#39;] = nn.Parameter(self.scale * torch.randn(rand_shape, dtype=torch.cfloat))
            # print(&#39;TORCH self.weights:&#39;, self.weights_[f&#39;w{i + 1}&#39;].shape)
            # print(self.weights[f&#39;w{i + 1}&#39;].shape)

    def complex_mul(self, input, weights):

        # print(input.shape)
        # print(weights.shape)
        # exit(1)
        if self.in_spatial == 1:
            return torch.einsum(&#34;bix,iox-&gt;box&#34;, input, weights)
        elif self.in_spatial == 2:
            return torch.einsum(&#34;bixy,ioxy-&gt;boxy&#34;, input, weights)
        elif self.in_spatial == 3:
            return torch.einsum(&#34;bixyz,ioxyz-&gt;boxyz&#34;, input, weights)

    def forward(self, x):
        batch_size = x.shape[0]

        # print(&#39;x.shape:&#39;, x.shape)
        ##Convert to Fourier space
        dims = [-i for i in range(self.in_spatial, 0, -1)]
        x_ft = torch.fft.rfftn(x, dim=dims)
        # print(&#39;After RFFT torch&#39;, x_ft.shape)
        outft_dims = [batch_size, self.out_channels] + \
                     [x.size(-i) for i in range(self.in_spatial, 1, -1)] + [x.size(-1) // 2 + 1]
        out_ft = torch.zeros(outft_dims, dtype=torch.cfloat, device=x.device)
        # print(&#39;outft shape before&#39;, out_ft.shape)
        ##Multiply relevant fourier modes
        if self.in_spatial == 1:
            out_ft[:, :, :self.modes[1]] = \
                self.complex_mul(x_ft[:, :, :self.modes[1]],
                                 self.weights[&#39;w1&#39;].to(x_ft.device))
        elif self.in_spatial == 2:
            out_ft[:, :, :self.modes[1], :self.modes[2]] = \
                self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2]],
                                 self.weights[&#39;w1&#39;].to(x_ft.device))
            out_ft[:, :, -self.modes[1]:, :self.modes[2]] = \
                self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2]],
                                 self.weights[&#39;w2&#39;].to(x_ft.device))
        elif self.in_spatial == 3:
            out_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]] = \
                self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]],
                                 self.weights[&#39;w1&#39;].to(x_ft.device))
            out_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]] = \
                self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]],
                                 self.weights[&#39;w2&#39;].to(x_ft.device))
            out_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]] = \
                self.complex_mul(x_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]],
                                 self.weights[&#39;w3&#39;].to(x_ft.device))
            out_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]] = \
                self.complex_mul(x_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]],
                                 self.weights[&#39;w4&#39;].to(x_ft.device))

        ##Return to Physical Space
        x = torch.fft.irfftn(out_ft, s=[x.size(-i) for i in range(self.in_spatial, 0, -1)])
        return x


class FNO(nn.Module):

    def __init__(self, in_channels, out_channels, width, modes, activation, batch_norm, in_spatial):
        super(FNO, self).__init__()

        &#34;&#34;&#34;
        The overall network contains 4 layers of the [&#34;Fourier layer&#34;](https://github.com/zongyi-li/fourier_neural_operator).
        1. Lift the input to the desire channel dimension by self.fc0 .
        2. 4 layers of the integral operators u&#39; = (W + K)(u).
            W defined by self.w; K defined by self.conv .
        3. Project from the channel space to the output space by self.fc1 and self.fc2.
        
        input shape and output shape: (batchsize b, channels c, *spatial)
        &#34;&#34;&#34;

        self.activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
        self.width = width
        self.in_spatial = in_spatial

        self.fc0 = _bias0(nn.Linear)(in_channels + in_spatial, self.width)

        for i in range(4):
            self.add_module(f&#39;conv{i}&#39;, SpectralConv(self.width, self.width, modes, in_spatial))
            self.add_module(f&#39;w{i}&#39;, CONV[in_spatial](self.width, self.width, kernel_size=1))
            self.add_module(f&#39;bn{i}&#39;, NORM[in_spatial](self.width) if batch_norm else nn.Identity())

        self.fc1 = _bias0(nn.Linear)(self.width, 128)
        self.fc2 = _bias0(nn.Linear)(128, out_channels)

    # Adding extra spatial channels eg. x, y, z, .... to input x
    def get_grid(self, shape, device):
        batch_size = shape[0]
        grid_channel_sizes = shape[2:]  # shape =  (batch_size, channels, *spatial)
        self.grid_channels = {}
        for i in range(self.in_spatial):
            self.grid_channels[f&#39;dim{i}&#39;] = torch.tensor(torch.linspace(0, 1, grid_channel_sizes[i]),
                                                         dtype=torch.float)
            reshape_dim_tuple = [1, 1] + [1 if i != j else grid_channel_sizes[j] for j in range(self.in_spatial)]
            repeat_dim_tuple = [batch_size, 1] + [1 if i == j else grid_channel_sizes[j] for j in
                                                  range(self.in_spatial)]
            self.grid_channels[f&#39;dim{i}&#39;] = self.grid_channels[f&#39;dim{i}&#39;].reshape(reshape_dim_tuple) \
                .repeat(repeat_dim_tuple)

        return torch.cat([self.grid_channels[f&#39;dim{i}&#39;] for i in range(self.in_spatial)], dim=1).to(device)

    def forward(self, x):
        grid = self.get_grid(x.shape, x.device)
        x = torch.cat([x, grid], dim=1)

        permute_tuple = [0] + [2 + i for i in range(self.in_spatial)] + [1]
        permute_tuple_reverse = [0] + [self.in_spatial + 1] + [i + 1 for i in range(self.in_spatial)]

        # Transpose x such that channels shape lies at the end to pass it through linear layers
        x = x.permute(permute_tuple)

        x = self.fc0(x)

        # Transpose x back to its original shape to pass it through convolutional layers
        x = x.permute(permute_tuple_reverse)

        for i in range(4):
            x1 = getattr(self, f&#39;w{i}&#39;)(x)
            x2 = getattr(self, f&#39;conv{i}&#39;)(x)
            x = getattr(self, f&#39;bn{i}&#39;)(x1) + getattr(self, f&#39;bn{i}&#39;)(x2)
            x = self.activation()(x)

        x = x.permute(permute_tuple)
        x = self.activation()(self.fc1(x))
        x = self.fc2(x)

        x = x.permute(permute_tuple_reverse)

        return x


def fno(in_channels: int,
        out_channels: int,
        mid_channels: int,
        modes: Sequence[int],
        activation: Union[str, type] = &#39;ReLU&#39;,
        batch_norm: bool = False,
        in_spatial: int = 2):
    &#34;&#34;&#34;
    [&#34;Fourier Neural Operator&#34;](https://github.com/zongyi-li/fourier_neural_operator) network contains 4 layers of the Fourier layer.
    1. Lift the input to the desire channel dimension by self.fc0 .
    2. 4 layers of the integral operators u&#39; = (W + K)(u). W defined by self.w; K defined by self.conv .
    3. Project from the channel space to the output space by self.fc1 and self.fc2.

    Arguments:

        in_channels : input channels of the feature map, dtype : int
        out_channels : output channels of the feature map, dtype : int
        mid_channels : channels used in Spectral Convolution Layers, dtype : int
        modes : Fourier modes for each spatial channel, dtype : List[int] or int (in case all number modes are to be the same for each spatial channel)
        activation : activation function used within the layers, dtype : string
        batch_norm : use of batchnorm after each conv layer, dtype : bool
        in_spatial : spatial dimensions of the input feature map, dtype : int

    Returns:

        Fourier Neural Operator model as specified by input arguments.
    &#34;&#34;&#34;
    net = FNO(in_channels, out_channels, mid_channels, modes, activation, batch_norm, in_spatial)
    net = net.to(TORCH.get_default_device().ref)
    return net</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="phi.torch.nets.adagrad"><code class="name flex">
<span>def <span class="ident">adagrad</span></span>(<span>net:Â torch.nn.modules.module.Module, learning_rate:Â floatÂ =Â 0.001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an Adagrad optimizer for 'net', alias for <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html">'torch.optim.Adagrad'</a>
Analogue functions exist for other learning frameworks.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adagrad(net: nn.Module, learning_rate: float = 1e-3, lr_decay=0, weight_decay=0, initial_accumulator_value=0,
            eps=1e-10):
    &#34;&#34;&#34;
    Creates an Adagrad optimizer for &#39;net&#39;, alias for [&#39;torch.optim.Adagrad&#39;](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html)
    Analogue functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return optim.Adagrad(net.parameters(), learning_rate, lr_decay, weight_decay, initial_accumulator_value, eps)</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.adam"><code class="name flex">
<span>def <span class="ident">adam</span></span>(<span>net:Â torch.nn.modules.module.Module, learning_rate:Â floatÂ =Â 0.001, betas=(0.9, 0.999), epsilon=1e-07)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an Adam optimizer for <code>net</code>, alias for <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"><code>torch.optim.Adam</code></a>.
Analogue functions exist for other learning frameworks.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adam(net: nn.Module, learning_rate: float = 1e-3, betas=(0.9, 0.999), epsilon=1e-07):
    &#34;&#34;&#34;
    Creates an Adam optimizer for `net`, alias for [`torch.optim.Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html).
    Analogue functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return optim.Adam(net.parameters(), learning_rate, betas, epsilon)</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.conv_classifier"><code class="name flex">
<span>def <span class="ident">conv_classifier</span></span>(<span>in_features:Â int, in_spatial:Â Union[tuple,Â list], num_classes:Â int, blocks=(64, 128, 256, 256, 512, 512), block_sizes=(2, 2, 3, 3, 3), dense_layers=(4096, 4096, 100), batch_norm=True, activation='ReLU', softmax=True, periodic=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Based on VGG16.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv_classifier(in_features: int,
                    in_spatial: Union[tuple, list],
                    num_classes: int,
                    blocks=(64, 128, 256, 256, 512, 512),
                    block_sizes=(2, 2, 3, 3, 3),
                    dense_layers=(4096, 4096, 100),
                    batch_norm=True,
                    activation=&#39;ReLU&#39;,
                    softmax=True,
                    periodic=False):
    &#34;&#34;&#34;
    Based on VGG16.
    &#34;&#34;&#34;
    assert isinstance(in_spatial, (tuple, list))
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    net = ConvClassifier(in_features, in_spatial, num_classes, batch_norm, softmax, blocks, block_sizes, dense_layers, periodic, activation)
    return net.to(TORCH.get_default_device().ref)</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.conv_net"><code class="name flex">
<span>def <span class="ident">conv_net</span></span>(<span>in_channels:Â int, out_channels:Â int, layers:Â Sequence[int], batch_norm:Â boolÂ =Â False, activation:Â Union[str,Â type]Â =Â 'ReLU', in_spatial:Â Union[int,Â tuple]Â =Â 2, periodic=False, **kwargs) â€‘>Â torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<div class="desc"><p>Built in Conv-Nets are also provided. Contrary to the classical convolutional neural networks, the feature map spatial size remains the same throughout the layers. Each layer of the network is essentially a convolutional block comprising of two conv layers. A filter size of 3 is used in the convolutional layers.</p>
<h2 id="arguments">Arguments</h2>
<p>in_channels : input channels of the feature map, dtype : int
out_channels : output channels of the feature map, dtype : int
layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
activation : activation function used within the layers, dtype : string
batch_norm : use of batchnorm after each conv layer, dtype : bool
in_spatial : spatial dimensions of the input feature map, dtype : int</p>
<h2 id="returns">Returns</h2>
<p>Conv-net model as specified by input arguments</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv_net(in_channels: int,
             out_channels: int,
             layers: Sequence[int],
             batch_norm: bool = False,
             activation: Union[str, type] = &#39;ReLU&#39;,
             in_spatial: Union[int, tuple] = 2,
             periodic=False,
             **kwargs) -&gt; nn.Module:
    &#34;&#34;&#34;
    Built in Conv-Nets are also provided. Contrary to the classical convolutional neural networks, the feature map spatial size remains the same throughout the layers. Each layer of the network is essentially a convolutional block comprising of two conv layers. A filter size of 3 is used in the convolutional layers.
    Arguments:

        in_channels : input channels of the feature map, dtype : int
        out_channels : output channels of the feature map, dtype : int
        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
        activation : activation function used within the layers, dtype : string
        batch_norm : use of batchnorm after each conv layer, dtype : bool
        in_spatial : spatial dimensions of the input feature map, dtype : int

    Returns:

        Conv-net model as specified by input arguments
    &#34;&#34;&#34;
    if isinstance(in_spatial, int):
        d = in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    net = ConvNet(d, in_channels, out_channels, layers, batch_norm, activation, periodic)
    net = net.to(TORCH.get_default_device().ref)
    return net</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.coupling_layer"><code class="name flex">
<span>def <span class="ident">coupling_layer</span></span>(<span>in_channels:Â int, activation:Â Union[str,Â type]Â =Â 'ReLU', batch_norm=False, reverse_mask=False, in_spatial:Â Union[int,Â tuple]Â =Â 2)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coupling_layer(in_channels: int,
                   activation: Union[str, type] = &#39;ReLU&#39;,
                   batch_norm=False,
                   reverse_mask=False,
                   in_spatial: Union[tuple, int] = 2):
    if isinstance(in_spatial, tuple):
        in_spatial = len(in_spatial)

    net = CouplingLayer(in_channels, activation, batch_norm, in_spatial, reverse_mask)
    net = net.to(TORCH.get_default_device().ref)
    return net</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.dense_net"><code class="name flex">
<span>def <span class="ident">dense_net</span></span>(<span>in_channels:Â int, out_channels:Â int, layers:Â Sequence[int], batch_norm=False, activation:Â Union[str,Â Callable]Â =Â 'ReLU', softmax=False) â€‘>Â torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<div class="desc"><p>Fully-connected neural networks are available in Î¦-Flow via dense_net().</p>
<h2 id="arguments">Arguments</h2>
<p>in_channels : size of input layer, int
out_channels = size of output layer, int
layers : tuple of linear layers between input and output neurons, list or tuple
activation : activation function used within the layers, string
batch_norm : use of batch norm after each linear layer, bool</p>
<h2 id="returns">Returns</h2>
<p>Dense net model as specified by input arguments</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dense_net(in_channels: int,
              out_channels: int,
              layers: Sequence[int],
              batch_norm=False,
              activation: Union[str, Callable] = &#39;ReLU&#39;,
              softmax=False) -&gt; nn.Module:
    &#34;&#34;&#34;
    Fully-connected neural networks are available in Î¦-Flow via dense_net().
    Arguments:
        in_channels : size of input layer, int
        out_channels = size of output layer, int
        layers : tuple of linear layers between input and output neurons, list or tuple
        activation : activation function used within the layers, string
        batch_norm : use of batch norm after each linear layer, bool

    Returns:
        Dense net model as specified by input arguments
    &#34;&#34;&#34;
    layers = [in_channels, *layers, out_channels]
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    net = DenseNet(layers, activation, batch_norm, softmax)
    return net.to(TORCH.get_default_device().ref)</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.fno"><code class="name flex">
<span>def <span class="ident">fno</span></span>(<span>in_channels:Â int, out_channels:Â int, mid_channels:Â int, modes:Â Sequence[int], activation:Â Union[str,Â type]Â =Â 'ReLU', batch_norm:Â boolÂ =Â False, in_spatial:Â intÂ =Â 2)</span>
</code></dt>
<dd>
<div class="desc"><p><a href="https://github.com/zongyi-li/fourier_neural_operator">"Fourier Neural Operator"</a> network contains 4 layers of the Fourier layer.
1. Lift the input to the desire channel dimension by self.fc0 .
2. 4 layers of the integral operators u' = (W + K)(u). W defined by self.w; K defined by self.conv .
3. Project from the channel space to the output space by self.fc1 and self.fc2.</p>
<h2 id="arguments">Arguments</h2>
<p>in_channels : input channels of the feature map, dtype : int
out_channels : output channels of the feature map, dtype : int
mid_channels : channels used in Spectral Convolution Layers, dtype : int
modes : Fourier modes for each spatial channel, dtype : List[int] or int (in case all number modes are to be the same for each spatial channel)
activation : activation function used within the layers, dtype : string
batch_norm : use of batchnorm after each conv layer, dtype : bool
in_spatial : spatial dimensions of the input feature map, dtype : int</p>
<h2 id="returns">Returns</h2>
<p>Fourier Neural Operator model as specified by input arguments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fno(in_channels: int,
        out_channels: int,
        mid_channels: int,
        modes: Sequence[int],
        activation: Union[str, type] = &#39;ReLU&#39;,
        batch_norm: bool = False,
        in_spatial: int = 2):
    &#34;&#34;&#34;
    [&#34;Fourier Neural Operator&#34;](https://github.com/zongyi-li/fourier_neural_operator) network contains 4 layers of the Fourier layer.
    1. Lift the input to the desire channel dimension by self.fc0 .
    2. 4 layers of the integral operators u&#39; = (W + K)(u). W defined by self.w; K defined by self.conv .
    3. Project from the channel space to the output space by self.fc1 and self.fc2.

    Arguments:

        in_channels : input channels of the feature map, dtype : int
        out_channels : output channels of the feature map, dtype : int
        mid_channels : channels used in Spectral Convolution Layers, dtype : int
        modes : Fourier modes for each spatial channel, dtype : List[int] or int (in case all number modes are to be the same for each spatial channel)
        activation : activation function used within the layers, dtype : string
        batch_norm : use of batchnorm after each conv layer, dtype : bool
        in_spatial : spatial dimensions of the input feature map, dtype : int

    Returns:

        Fourier Neural Operator model as specified by input arguments.
    &#34;&#34;&#34;
    net = FNO(in_channels, out_channels, mid_channels, modes, activation, batch_norm, in_spatial)
    net = net.to(TORCH.get_default_device().ref)
    return net</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.get_mask"><code class="name flex">
<span>def <span class="ident">get_mask</span></span>(<span>inputs, reverse_mask, data_format='NHWC')</span>
</code></dt>
<dd>
<div class="desc"><p>Compute mask for slicing input feature map for Invertible Nets</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_mask(inputs, reverse_mask, data_format=&#39;NHWC&#39;):
    &#34;&#34;&#34; Compute mask for slicing input feature map for Invertible Nets &#34;&#34;&#34;
    shape = inputs.shape
    if len(shape) == 2:
        N = shape[-1]
        range_n = torch.arange(0, N)
        even_ind = range_n % 2
        checker = torch.reshape(even_ind, (-1, N))
    elif len(shape) == 4:
        H = shape[2] if data_format == &#39;NCHW&#39; else shape[1]
        W = shape[3] if data_format == &#39;NCHW&#39; else shape[2]

        range_h = torch.arange(0, H)
        range_w = torch.arange(0, W)

        even_ind_h = range_h % 2
        even_ind_w = range_w % 2

        ind_h = even_ind_h.unsqueeze(-1).repeat(1, W)
        ind_w = even_ind_w.unsqueeze(0).repeat(H, 1)

        checker = torch.logical_xor(ind_h, ind_w)

        checker = checker.reshape(1, 1, H, W) if data_format == &#39;NCHW&#39; else checker.reshape(1, H, W, 1)
        checker = checker.long()

    else:
        raise ValueError(&#39;Invalid tensor shape. Dimension of the tensor shape must be &#39;
                         &#39;2 (NxD) or 4 (NxCxHxW or NxHxWxC), got {}.&#39;.format(inputs.get_shape().as_list()))

    if reverse_mask:
        checker = 1 - checker

    return checker.to(TORCH.get_default_device().ref)</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.get_parameters"><code class="name flex">
<span>def <span class="ident">get_parameters</span></span>(<span>net:Â torch.nn.modules.module.Module, wrap=True) â€‘>Â dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_parameters(net: nn.Module, wrap=True) -&gt; dict:
    if not wrap:
        return {name: param for name, param in net.named_parameters()}
    result = {}
    for name, param in net.named_parameters():
        if name.endswith(&#39;.weight&#39;):
            if param.ndim == 2:
                phi_tensor = math.wrap(param, channel(&#39;input,output&#39;))
            elif param.ndim == 3:
                phi_tensor = math.wrap(param, channel(&#39;x,input,output&#39;))
            elif param.ndim == 4:
                phi_tensor = math.wrap(param, channel(&#39;x,y,input,output&#39;))
            elif param.ndim == 5:
                phi_tensor = math.wrap(param, channel(&#39;x,y,z,input,output&#39;))
        elif name.endswith(&#39;.bias&#39;):
            phi_tensor = math.wrap(param, channel(&#39;output&#39;))
        else:
            raise NotImplementedError
        result[name] = phi_tensor
    return result</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.invertible_net"><code class="name flex">
<span>def <span class="ident">invertible_net</span></span>(<span>in_channels:Â int, num_blocks:Â int, batch_norm:Â boolÂ =Â False, net:Â strÂ =Â 'u_net', activation:Â Union[str,Â type]Â =Â 'ReLU', in_spatial:Â Union[int,Â tuple]Â =Â 2, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Phiflow also provides invertible neural networks that are capable of inverting the output tensor back to the input tensor initially passed.\ These networks have far reaching applications in predicting input parameters of a problem given its observations.\ Invertible nets are composed of multiple concatenated coupling blocks wherein each such block consists of arbitrary neural networks.</p>
<p>Currently, these arbitrary neural networks could be set to u_net(default), conv_net, res_net or dense_net blocks with in_channels = out_channels.
The architecture used is popularized by <a href="https://arxiv.org/abs/1605.08803">"Real NVP"</a>.</p>
<h2 id="arguments">Arguments</h2>
<p>in_channels : input channels of the feature map, dtype : int
num_blocks : number of coupling blocks inside the invertible net, dtype : int
activation : activation function used within the layers, dtype : string
batch_norm : use of batchnorm after each layer, dtype : bool
in_spatial : spatial dimensions of the input feature map, dtype : int
net : type of neural network blocks used in coupling layers, dtype : str
**kwargs : placeholder for arguments not supported by the function</p>
<h2 id="returns">Returns</h2>
<p>Invertible Net model as specified by input arguments
Note: Currently supported values for net are 'u_net'(default), 'conv_net' and 'res_net'.
For choosing 'dense_net' as the network block in coupling layers in_spatial must be set to zero.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def invertible_net(in_channels: int,
                   num_blocks: int,
                   batch_norm: bool = False,
                   net: str = &#39;u_net&#39;,
                   activation: Union[str, type] = &#39;ReLU&#39;,
                   in_spatial: Union[tuple, int] = 2, **kwargs):
    &#34;&#34;&#34;
    Phiflow also provides invertible neural networks that are capable of inverting the output tensor back to the input tensor initially passed.\ These networks have far reaching applications in predicting input parameters of a problem given its observations.\ Invertible nets are composed of multiple concatenated coupling blocks wherein each such block consists of arbitrary neural networks.

    Currently, these arbitrary neural networks could be set to u_net(default), conv_net, res_net or dense_net blocks with in_channels = out_channels.
    The architecture used is popularized by [&#34;Real NVP&#34;](https://arxiv.org/abs/1605.08803).

    Arguments:

        in_channels : input channels of the feature map, dtype : int
        num_blocks : number of coupling blocks inside the invertible net, dtype : int
        activation : activation function used within the layers, dtype : string
        batch_norm : use of batchnorm after each layer, dtype : bool
        in_spatial : spatial dimensions of the input feature map, dtype : int
        net : type of neural network blocks used in coupling layers, dtype : str
        **kwargs : placeholder for arguments not supported by the function

    Returns:

        Invertible Net model as specified by input arguments

    Note: Currently supported values for net are &#39;u_net&#39;(default), &#39;conv_net&#39; and &#39;res_net&#39;.
    For choosing &#39;dense_net&#39; as the network block in coupling layers in_spatial must be set to zero.
    &#34;&#34;&#34;
    if isinstance(in_spatial, tuple):
        in_spatial = len(in_spatial)

    return InvertibleNet(in_channels, num_blocks, activation, batch_norm, in_spatial, net).to(TORCH.get_default_device().ref)</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.load_state"><code class="name flex">
<span>def <span class="ident">load_state</span></span>(<span>obj:Â Union[torch.nn.modules.module.Module,Â torch.optim.optimizer.Optimizer], path:Â str)</span>
</code></dt>
<dd>
<div class="desc"><p>Read the state of a module or optimizer from a file.</p>
<p>See Also:
<code><a title="phi.torch.nets.save_state" href="#phi.torch.nets.save_state">save_state()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code>torch.nn.Module or torch.optim.Optimizer</code></dd>
<dt><strong><code>path</code></strong></dt>
<dd>File path as <code>str</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_state(obj: Union[nn.Module, optim.Optimizer], path: str):
    &#34;&#34;&#34;
    Read the state of a module or optimizer from a file.

    See Also:
        `save_state()`

    Args:
        obj: `torch.nn.Module or torch.optim.Optimizer`
        path: File path as `str`.
    &#34;&#34;&#34;
    if not path.endswith(&#39;.pth&#39;):
        path += &#39;.pth&#39;
    obj.load_state_dict(torch.load(path))</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.parameter_count"><code class="name flex">
<span>def <span class="ident">parameter_count</span></span>(<span>model:Â torch.nn.modules.module.Module) â€‘>Â int</span>
</code></dt>
<dd>
<div class="desc"><p>Counts the number of parameters in a model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>PyTorch model</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>int</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parameter_count(model: nn.Module) -&gt; int:
    &#34;&#34;&#34;
    Counts the number of parameters in a model.

    Args:
        model: PyTorch model

    Returns:
        `int`
    &#34;&#34;&#34;
    total = 0
    for parameter in model.parameters():
        total += numpy.prod(parameter.shape)
    return int(total)</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.res_net"><code class="name flex">
<span>def <span class="ident">res_net</span></span>(<span>in_channels:Â int, out_channels:Â int, layers:Â Sequence[int], batch_norm:Â boolÂ =Â False, activation:Â Union[str,Â type]Â =Â 'ReLU', in_spatial:Â Union[int,Â tuple]Â =Â 2, periodic=False, **kwargs) â€‘>Â torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<div class="desc"><p>Built in Res-Nets are provided in the Î¦Flow framework. Similar to the conv-net, the feature map spatial size remains the same throughout the layers.
These networks use residual blocks composed of two conv layers with a skip connection added from the input to the output feature map.
A default filter size of 3 is used in the convolutional layers.</p>
<h2 id="arguments">Arguments</h2>
<p>in_channels : input channels of the feature map, dtype : int
out_channels : output channels of the feature map, dtype : int
layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
activation : activation function used within the layers, dtype : string
batch_norm : use of batchnorm after each conv layer, dtype : bool
in_spatial : spatial dimensions of the input feature map, dtype : int</p>
<h2 id="returns">Returns</h2>
<p>Res-net model as specified by input arguments</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def res_net(in_channels: int,
            out_channels: int,
            layers: Sequence[int],
            batch_norm: bool = False,
            activation: Union[str, type] = &#39;ReLU&#39;,
            in_spatial: Union[int, tuple] = 2,
            periodic=False,
            **kwargs) -&gt; nn.Module:
    &#34;&#34;&#34;
    Built in Res-Nets are provided in the Î¦Flow framework. Similar to the conv-net, the feature map spatial size remains the same throughout the layers.
    These networks use residual blocks composed of two conv layers with a skip connection added from the input to the output feature map.
    A default filter size of 3 is used in the convolutional layers.

    Arguments:

        in_channels : input channels of the feature map, dtype : int
        out_channels : output channels of the feature map, dtype : int
        layers : list or tuple of output channels for each intermediate layer between the input and final output channels, dtype : list or tuple
        activation : activation function used within the layers, dtype : string
        batch_norm : use of batchnorm after each conv layer, dtype : bool
        in_spatial : spatial dimensions of the input feature map, dtype : int

    Returns:

        Res-net model as specified by input arguments

    &#34;&#34;&#34;
    if (isinstance(in_spatial, int)):
        d = in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    net = ResNet(d, in_channels, out_channels, layers, batch_norm, activation, periodic)
    net = net.to(TORCH.get_default_device().ref)
    return net</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.rmsprop"><code class="name flex">
<span>def <span class="ident">rmsprop</span></span>(<span>net:Â torch.nn.modules.module.Module, learning_rate:Â floatÂ =Â 0.001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an RMSProp optimizer for 'net', alias for <a href="https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html">'torch.optim.RMSprop'</a>
Analogue functions exist for other learning frameworks.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rmsprop(net: nn.Module, learning_rate: float = 1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0,
            centered=False):
    &#34;&#34;&#34;
    Creates an RMSProp optimizer for &#39;net&#39;, alias for [&#39;torch.optim.RMSprop&#39;](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html)
    Analogue functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return optim.RMSprop(net.parameters(), learning_rate, alpha, eps, weight_decay, momentum, centered)</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.save_state"><code class="name flex">
<span>def <span class="ident">save_state</span></span>(<span>obj:Â Union[torch.nn.modules.module.Module,Â torch.optim.optimizer.Optimizer], path:Â str)</span>
</code></dt>
<dd>
<div class="desc"><p>Write the state of a module or optimizer to a file.</p>
<p>See Also:
<code><a title="phi.torch.nets.load_state" href="#phi.torch.nets.load_state">load_state()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code>torch.nn.Module or torch.optim.Optimizer</code></dd>
<dt><strong><code>path</code></strong></dt>
<dd>File path as <code>str</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_state(obj: Union[nn.Module, optim.Optimizer], path: str):
    &#34;&#34;&#34;
    Write the state of a module or optimizer to a file.

    See Also:
        `load_state()`

    Args:
        obj: `torch.nn.Module or torch.optim.Optimizer`
        path: File path as `str`.
    &#34;&#34;&#34;
    if not path.endswith(&#39;.pth&#39;):
        path += &#39;.pth&#39;
    torch.save(obj.state_dict(), path)</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.sgd"><code class="name flex">
<span>def <span class="ident">sgd</span></span>(<span>net:Â torch.nn.modules.module.Module, learning_rate:Â floatÂ =Â 0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an SGD optimizer for 'net', alias for <a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">'torch.optim.SGD'</a>
Analogue functions exist for other learning frameworks.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sgd(net: nn.Module, learning_rate: float = 1e-3, momentum=0, dampening=0, weight_decay=0, nesterov=False):
    &#34;&#34;&#34;
    Creates an SGD optimizer for &#39;net&#39;, alias for [&#39;torch.optim.SGD&#39;](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)
    Analogue functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return optim.SGD(net.parameters(), learning_rate, momentum, dampening, weight_decay, nesterov)</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.u_net"><code class="name flex">
<span>def <span class="ident">u_net</span></span>(<span>in_channels:Â int, out_channels:Â int, levels:Â intÂ =Â 4, filters:Â Union[int,Â tuple,Â list]Â =Â 16, batch_norm:Â boolÂ =Â True, activation:Â Union[str,Â type]Â =Â 'ReLU', in_spatial:Â Union[int,Â tuple]Â =Â 2, periodic=False, use_res_blocks:Â boolÂ =Â False, **kwargs) â€‘>Â torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<div class="desc"><p>Î¦Flow provides a built-in U-net architecture, classically popular for Semantic Segmentation in Computer Vision, composed of downsampling and upsampling layers.</p>
<h2 id="arguments">Arguments</h2>
<p>in_channels: input channels of the feature map, dtype : int
out_channels : output channels of the feature map, dtype : int
levels : number of levels of down-sampling and upsampling, dtype : int
filters : filter sizes at each down/up sampling convolutional layer, if the input is integer all conv layers have the same filter size,
activation : activation function used within the layers, dtype : string
batch_norm : use of batchnorm after each conv layer, dtype : bool
in_spatial : spatial dimensions of the input feature map, dtype : int
use_res_blocks : use convolutional blocks with skip connections instead of regular convolutional blocks, dtype : bool</p>
<h2 id="returns">Returns</h2>
<p>U-net model as specified by input arguments</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def u_net(in_channels: int,
          out_channels: int,
          levels: int = 4,
          filters: Union[int, tuple, list] = 16,
          batch_norm: bool = True,
          activation: Union[str, type] = &#39;ReLU&#39;,
          in_spatial: Union[tuple, int] = 2,
          periodic=False,
          use_res_blocks: bool = False,
          **kwargs) -&gt; nn.Module:
    &#34;&#34;&#34;
    Î¦Flow provides a built-in U-net architecture, classically popular for Semantic Segmentation in Computer Vision, composed of downsampling and upsampling layers.

    Arguments:

        in_channels: input channels of the feature map, dtype : int
        out_channels : output channels of the feature map, dtype : int
        levels : number of levels of down-sampling and upsampling, dtype : int
        filters : filter sizes at each down/up sampling convolutional layer, if the input is integer all conv layers have the same filter size,
        activation : activation function used within the layers, dtype : string
        batch_norm : use of batchnorm after each conv layer, dtype : bool
        in_spatial : spatial dimensions of the input feature map, dtype : int
        use_res_blocks : use convolutional blocks with skip connections instead of regular convolutional blocks, dtype : bool

    Returns:

        U-net model as specified by input arguments

    &#34;&#34;&#34;
    if isinstance(filters, (tuple, list)):
        assert len(filters) == levels, f&#34;List of filters has length {len(filters)} but u-net has {levels} levels.&#34;
    else:
        filters = (filters,) * levels
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(in_spatial, int):
        d = in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    net = UNet(d, in_channels, out_channels, filters, batch_norm, activation, periodic, use_res_blocks)
    return net.to(TORCH.get_default_device().ref)</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.update_weights"><code class="name flex">
<span>def <span class="ident">update_weights</span></span>(<span>net:Â torch.nn.modules.module.Module, optimizer:Â torch.optim.optimizer.Optimizer, loss_function:Â Callable, *loss_args, check_nan=False, **loss_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the gradients of <code>loss_function</code> w.r.t. the parameters of <code>net</code> and updates its weights using <code>optimizer</code>.</p>
<p>This is the PyTorch version. Analogue functions exist for other learning frameworks.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>net</code></strong></dt>
<dd>Learning model.</dd>
<dt><strong><code>optimizer</code></strong></dt>
<dd>Optimizer.</dd>
<dt><strong><code>loss_function</code></strong></dt>
<dd>Loss function, called as <code>loss_function(*loss_args, **loss_kwargs)</code>.</dd>
<dt><strong><code>*loss_args</code></strong></dt>
<dd>Arguments given to <code>loss_function</code>.</dd>
<dt><strong><code>**loss_kwargs</code></strong></dt>
<dd>Keyword arguments given to <code>loss_function</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Output of <code>loss_function</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_weights(net: nn.Module, optimizer: optim.Optimizer, loss_function: Callable, *loss_args, check_nan=False, **loss_kwargs):
    &#34;&#34;&#34;
    Computes the gradients of `loss_function` w.r.t. the parameters of `net` and updates its weights using `optimizer`.

    This is the PyTorch version. Analogue functions exist for other learning frameworks.

    Args:
        net: Learning model.
        optimizer: Optimizer.
        loss_function: Loss function, called as `loss_function(*loss_args, **loss_kwargs)`.
        *loss_args: Arguments given to `loss_function`.
        **loss_kwargs: Keyword arguments given to `loss_function`.

    Returns:
        Output of `loss_function`.
    &#34;&#34;&#34;
    optimizer.zero_grad()
    output = loss_function(*loss_args, **loss_kwargs)
    loss = output[0] if isinstance(output, tuple) else output
    loss.sum.backward()
    if isinstance(optimizer, optim.LBFGS):
        def closure():
            result = loss_function(*loss_args, **loss_kwargs)
            loss_val = result[0] if isinstance(result, tuple) else result
            return loss_val.sum
        optimizer.step(closure=closure)
    else:
        if check_nan:
            for p in net.parameters():
                if not torch.all(torch.isfinite(p.grad)):
                    raise RuntimeError(f&#34;NaN in network gradient detected. Parameter: {p}&#34;)
        optimizer.step()
    return output</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="phi.torch.nets.ConvClassifier"><code class="flex name class">
<span>class <span class="ident">ConvClassifier</span></span>
<span>(</span><span>in_features, in_spatial:Â list, num_classes:Â int, batch_norm:Â bool, use_softmax:Â bool, blocks:Â tuple, block_sizes:Â tuple, dense_layers:Â tuple, periodic:Â bool, activation)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConvClassifier(nn.Module):

    def __init__(self, in_features, in_spatial: list, num_classes: int, batch_norm: bool, use_softmax: bool, blocks: tuple, block_sizes: tuple, dense_layers: tuple, periodic: bool, activation):
        super(ConvClassifier, self).__init__()
        d = len(in_spatial)
        self.in_spatial = in_spatial
        self._blocks = blocks
        self.add_module(&#39;maxpool&#39;, MAX_POOL[d](2))
        for i, (prev, next) in enumerate(zip((in_features,) + tuple(blocks[:-1]), blocks)):
            block_size = block_sizes[i]
            layers = []
            for j in range(block_size):
                layers.append(CONV[d](prev if j == 0 else next, next, kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;))
                layers.append(NORM[d](next) if batch_norm else nn.Identity())
                layers.append(activation())
            self.add_module(f&#39;conv{i+1}&#39;, nn.Sequential(*layers))
        flat_size = int(np.prod(in_spatial) * blocks[-1] / (2**d) ** len(blocks))
        self.dense_net = dense_net(flat_size, num_classes, dense_layers, batch_norm, activation, use_softmax)
        self.flatten = nn.Flatten()

    def forward(self, x):
        for i in range(len(self._blocks)):
            x = getattr(self, f&#39;conv{i+1}&#39;)(x)
            x = self.maxpool(x)
        x = self.flatten(x)
        x = self.dense_net(x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phi.torch.nets.ConvClassifier.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.ConvClassifier.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.ConvClassifier.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.torch.nets.ConvClassifier.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    for i in range(len(self._blocks)):
        x = getattr(self, f&#39;conv{i+1}&#39;)(x)
        x = self.maxpool(x)
    x = self.flatten(x)
    x = self.dense_net(x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.torch.nets.ConvNet"><code class="flex name class">
<span>class <span class="ident">ConvNet</span></span>
<span>(</span><span>in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConvNet(nn.Module):

    def __init__(self, in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic: bool):
        super(ConvNet, self).__init__()
        activation = ACTIVATIONS[activation]
        if len(layers) &lt; 1:
            layers.append(out_channels)
        self.layers = layers
        self.add_module(f&#39;Conv_in&#39;, nn.Sequential(
            CONV[in_spatial](in_channels, layers[0], kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
            NORM[in_spatial](layers[0]) if batch_norm else nn.Identity(),
            activation()))
        for i in range(1, len(layers)):
            self.add_module(f&#39;Conv{i}&#39;, nn.Sequential(
                CONV[in_spatial](layers[i - 1], layers[i], kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
                NORM[in_spatial](layers[i]) if batch_norm else nn.Identity(),
                activation()))
        self.add_module(f&#39;Conv_out&#39;, CONV[in_spatial](layers[len(layers) - 1], out_channels, kernel_size=1))

    def forward(self, x):
        x = getattr(self, f&#39;Conv_in&#39;)(x)
        for i in range(1, len(self.layers)):
            x = getattr(self, f&#39;Conv{i}&#39;)(x)
        x = getattr(self, f&#39;Conv_out&#39;)(x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phi.torch.nets.ConvNet.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.ConvNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.ConvNet.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.torch.nets.ConvNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = getattr(self, f&#39;Conv_in&#39;)(x)
    for i in range(1, len(self.layers)):
        x = getattr(self, f&#39;Conv{i}&#39;)(x)
    x = getattr(self, f&#39;Conv_out&#39;)(x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.torch.nets.CouplingLayer"><code class="flex name class">
<span>class <span class="ident">CouplingLayer</span></span>
<span>(</span><span>in_channels, activation, batch_norm, in_spatial, net, reverse_mask)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CouplingLayer(nn.Module):

    def __init__(self, in_channels, activation, batch_norm, in_spatial, net, reverse_mask):
        super(CouplingLayer, self).__init__()

        self.activation = activation
        self.batch_norm = batch_norm
        self.reverse_mask = reverse_mask

        if in_spatial == 0:  # for in_spatial = 0, use dense layers
            self.s1 = nn.Sequential(Dense_resnet_block(in_channels, in_channels, batch_norm, activation),
                                    torch.nn.Tanh())
            self.t1 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)

            self.s2 = nn.Sequential(Dense_resnet_block(in_channels, in_channels, batch_norm, activation),
                                    torch.nn.Tanh())
            self.t2 = Dense_resnet_block(in_channels, in_channels, batch_norm, activation)
        else:
            self.s1 = nn.Sequential(NET[net](in_channels=in_channels, out_channels=in_channels,
                                             layers=[], batch_norm=batch_norm, activation=activation,
                                             in_spatial=in_spatial), torch.nn.Tanh())
            self.t1 = NET[net](in_channels=in_channels, out_channels=in_channels,
                               layers=[], batch_norm=batch_norm, activation=activation,
                               in_spatial=in_spatial)
            self.s2 = nn.Sequential(NET[net](in_channels=in_channels, out_channels=in_channels,
                                             layers=[], batch_norm=batch_norm, activation=activation,
                                             in_spatial=in_spatial), torch.nn.Tanh())
            self.t2 = NET[net](in_channels=in_channels, out_channels=in_channels,
                               layers=[], batch_norm=batch_norm, activation=activation,
                               in_spatial=in_spatial)

    def forward(self, x, invert=False):
        x = TORCH.as_tensor(x)
        mask = get_mask(x, self.reverse_mask, &#39;NCHW&#39;)
        if invert:
            v1 = x * mask
            v2 = x * (1 - mask)
            u2 = (1 - mask) * (v2 - self.t1(v1)) * torch.exp(-self.s1(v1))
            u1 = mask * (v1 - self.t2(u2)) * torch.exp(-self.s2(u2))
            return u1 + u2
        else:
            u1 = x * mask
            u2 = x * (1 - mask)
            v1 = mask * (u1 * torch.exp(self.s2(u2)) + self.t2(u2))
            v2 = (1 - mask) * (u2 * torch.exp(self.s1(v1)) + self.t1(v1))
            return v1 + v2</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phi.torch.nets.CouplingLayer.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.CouplingLayer.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.CouplingLayer.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.torch.nets.CouplingLayer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x, invert=False) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x, invert=False):
    x = TORCH.as_tensor(x)
    mask = get_mask(x, self.reverse_mask, &#39;NCHW&#39;)
    if invert:
        v1 = x * mask
        v2 = x * (1 - mask)
        u2 = (1 - mask) * (v2 - self.t1(v1)) * torch.exp(-self.s1(v1))
        u1 = mask * (v1 - self.t2(u2)) * torch.exp(-self.s2(u2))
        return u1 + u2
    else:
        u1 = x * mask
        u2 = x * (1 - mask)
        v1 = mask * (u1 * torch.exp(self.s2(u2)) + self.t2(u2))
        v2 = (1 - mask) * (u2 * torch.exp(self.s1(v1)) + self.t1(v1))
        return v1 + v2</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.torch.nets.DenseNet"><code class="flex name class">
<span>class <span class="ident">DenseNet</span></span>
<span>(</span><span>layers:Â list, activation:Â type, batch_norm:Â bool, use_softmax:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DenseNet(nn.Module):

    def __init__(self,
                 layers: list,
                 activation: type,
                 batch_norm: bool,
                 use_softmax: bool):
        super(DenseNet, self).__init__()
        self._layers = layers
        self._activation = activation
        self._batch_norm = batch_norm
        for i, (s1, s2) in enumerate(zip(layers[:-2], layers[1:-1])):
            self.add_module(f&#39;linear{i}&#39;, _bias0(nn.Linear)(s1, s2, bias=True))
            if batch_norm:
                self.add_module(f&#39;norm{i}&#39;, nn.BatchNorm1d(s2))
        self.add_module(f&#39;linear_out&#39;, _bias0(nn.Linear)(layers[-2], layers[-1], bias=True))
        self.softmax = nn.Softmax() if use_softmax else None

    def forward(self, x):
        register_module_call(self)
        x = TORCH.as_tensor(x)
        for i in range(len(self._layers) - 2):
            x = self._activation()(getattr(self, f&#39;linear{i}&#39;)(x))
            if self._batch_norm:
                x = getattr(self, f&#39;norm{i}&#39;)(x)
        x = getattr(self, f&#39;linear_out&#39;)(x)
        if self.softmax:
            x = self.softmax(x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phi.torch.nets.DenseNet.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.DenseNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.DenseNet.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.torch.nets.DenseNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    register_module_call(self)
    x = TORCH.as_tensor(x)
    for i in range(len(self._layers) - 2):
        x = self._activation()(getattr(self, f&#39;linear{i}&#39;)(x))
        if self._batch_norm:
            x = getattr(self, f&#39;norm{i}&#39;)(x)
    x = getattr(self, f&#39;linear_out&#39;)(x)
    if self.softmax:
        x = self.softmax(x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.torch.nets.Dense_resnet_block"><code class="flex name class">
<span>class <span class="ident">Dense_resnet_block</span></span>
<span>(</span><span>in_channels, mid_channels, batch_norm, activation)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dense_resnet_block(nn.Module):

    def __init__(self, in_channels, mid_channels, batch_norm, activation):
        super(Dense_resnet_block, self).__init__()
        self.activation = activation
        self.bn1 = NORM[1](in_channels) if batch_norm else nn.Identity()
        self.linear1 = _bias0(nn.Linear)(in_channels, mid_channels)
        self.bn2 = NORM[1](mid_channels) if batch_norm else nn.Identity()
        self.linear2 = _bias0(nn.Linear)(mid_channels, in_channels)

    def forward(self, x):
        x = TORCH.as_tensor(x)
        out = self.activation()(self.bn1(self.linear1(x)))
        out = self.activation()(self.bn2(self.linear2(out)))
        out = out + x
        return out</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phi.torch.nets.Dense_resnet_block.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.Dense_resnet_block.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.Dense_resnet_block.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.torch.nets.Dense_resnet_block.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = TORCH.as_tensor(x)
    out = self.activation()(self.bn1(self.linear1(x)))
    out = self.activation()(self.bn2(self.linear2(out)))
    out = out + x
    return out</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.torch.nets.DoubleConv"><code class="flex name class">
<span>class <span class="ident">DoubleConv</span></span>
<span>(</span><span>d:Â int, in_channels:Â int, out_channels:Â int, mid_channels:Â int, batch_norm:Â bool, activation:Â type, periodic:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>(convolution =&gt; [BN] =&gt; ReLU) * 2</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DoubleConv(nn.Module):
    &#34;&#34;&#34;(convolution =&gt; [BN] =&gt; ReLU) * 2&#34;&#34;&#34;

    def __init__(self, d: int, in_channels: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: type, periodic: bool):
        super().__init__()
        self.add_module(&#39;double_conv&#39;, nn.Sequential(
            CONV[d](in_channels, mid_channels, kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
            NORM[d](mid_channels) if batch_norm else nn.Identity(),
            activation(),
            CONV[d](mid_channels, out_channels, kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;),
            NORM[d](out_channels) if batch_norm else nn.Identity(),
            nn.ReLU(inplace=True)
        ))

    def forward(self, x):
        return self.double_conv(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phi.torch.nets.DoubleConv.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.DoubleConv.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.DoubleConv.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.torch.nets.DoubleConv.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    return self.double_conv(x)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.torch.nets.Down"><code class="flex name class">
<span>class <span class="ident">Down</span></span>
<span>(</span><span>d:Â int, in_channels:Â int, out_channels:Â int, batch_norm:Â bool, activation:Â Union[str,Â type], use_res_blocks:Â bool, periodic)</span>
</code></dt>
<dd>
<div class="desc"><p>Downscaling with maxpool then double conv or resnet_block</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Down(nn.Module):
    &#34;&#34;&#34;Downscaling with maxpool then double conv or resnet_block&#34;&#34;&#34;

    def __init__(self, d: int, in_channels: int, out_channels: int, batch_norm: bool, activation: Union[str, type], use_res_blocks: bool, periodic):
        super().__init__()
        self.add_module(&#39;maxpool&#39;, MAX_POOL[d](2))
        if use_res_blocks:
            self.add_module(&#39;conv&#39;, resnet_block(d, in_channels, out_channels, batch_norm, activation, periodic))
        else:
            self.add_module(&#39;conv&#39;, DoubleConv(d, in_channels, out_channels, out_channels, batch_norm, activation, periodic))

    def forward(self, x):
        x = self.maxpool(x)
        return self.conv(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phi.torch.nets.Down.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.Down.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.Down.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.torch.nets.Down.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = self.maxpool(x)
    return self.conv(x)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.torch.nets.FNO"><code class="flex name class">
<span>class <span class="ident">FNO</span></span>
<span>(</span><span>in_channels, out_channels, width, modes, activation, batch_norm, in_spatial)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FNO(nn.Module):

    def __init__(self, in_channels, out_channels, width, modes, activation, batch_norm, in_spatial):
        super(FNO, self).__init__()

        &#34;&#34;&#34;
        The overall network contains 4 layers of the [&#34;Fourier layer&#34;](https://github.com/zongyi-li/fourier_neural_operator).
        1. Lift the input to the desire channel dimension by self.fc0 .
        2. 4 layers of the integral operators u&#39; = (W + K)(u).
            W defined by self.w; K defined by self.conv .
        3. Project from the channel space to the output space by self.fc1 and self.fc2.
        
        input shape and output shape: (batchsize b, channels c, *spatial)
        &#34;&#34;&#34;

        self.activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
        self.width = width
        self.in_spatial = in_spatial

        self.fc0 = _bias0(nn.Linear)(in_channels + in_spatial, self.width)

        for i in range(4):
            self.add_module(f&#39;conv{i}&#39;, SpectralConv(self.width, self.width, modes, in_spatial))
            self.add_module(f&#39;w{i}&#39;, CONV[in_spatial](self.width, self.width, kernel_size=1))
            self.add_module(f&#39;bn{i}&#39;, NORM[in_spatial](self.width) if batch_norm else nn.Identity())

        self.fc1 = _bias0(nn.Linear)(self.width, 128)
        self.fc2 = _bias0(nn.Linear)(128, out_channels)

    # Adding extra spatial channels eg. x, y, z, .... to input x
    def get_grid(self, shape, device):
        batch_size = shape[0]
        grid_channel_sizes = shape[2:]  # shape =  (batch_size, channels, *spatial)
        self.grid_channels = {}
        for i in range(self.in_spatial):
            self.grid_channels[f&#39;dim{i}&#39;] = torch.tensor(torch.linspace(0, 1, grid_channel_sizes[i]),
                                                         dtype=torch.float)
            reshape_dim_tuple = [1, 1] + [1 if i != j else grid_channel_sizes[j] for j in range(self.in_spatial)]
            repeat_dim_tuple = [batch_size, 1] + [1 if i == j else grid_channel_sizes[j] for j in
                                                  range(self.in_spatial)]
            self.grid_channels[f&#39;dim{i}&#39;] = self.grid_channels[f&#39;dim{i}&#39;].reshape(reshape_dim_tuple) \
                .repeat(repeat_dim_tuple)

        return torch.cat([self.grid_channels[f&#39;dim{i}&#39;] for i in range(self.in_spatial)], dim=1).to(device)

    def forward(self, x):
        grid = self.get_grid(x.shape, x.device)
        x = torch.cat([x, grid], dim=1)

        permute_tuple = [0] + [2 + i for i in range(self.in_spatial)] + [1]
        permute_tuple_reverse = [0] + [self.in_spatial + 1] + [i + 1 for i in range(self.in_spatial)]

        # Transpose x such that channels shape lies at the end to pass it through linear layers
        x = x.permute(permute_tuple)

        x = self.fc0(x)

        # Transpose x back to its original shape to pass it through convolutional layers
        x = x.permute(permute_tuple_reverse)

        for i in range(4):
            x1 = getattr(self, f&#39;w{i}&#39;)(x)
            x2 = getattr(self, f&#39;conv{i}&#39;)(x)
            x = getattr(self, f&#39;bn{i}&#39;)(x1) + getattr(self, f&#39;bn{i}&#39;)(x2)
            x = self.activation()(x)

        x = x.permute(permute_tuple)
        x = self.activation()(self.fc1(x))
        x = self.fc2(x)

        x = x.permute(permute_tuple_reverse)

        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phi.torch.nets.FNO.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.FNO.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.FNO.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.torch.nets.FNO.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    grid = self.get_grid(x.shape, x.device)
    x = torch.cat([x, grid], dim=1)

    permute_tuple = [0] + [2 + i for i in range(self.in_spatial)] + [1]
    permute_tuple_reverse = [0] + [self.in_spatial + 1] + [i + 1 for i in range(self.in_spatial)]

    # Transpose x such that channels shape lies at the end to pass it through linear layers
    x = x.permute(permute_tuple)

    x = self.fc0(x)

    # Transpose x back to its original shape to pass it through convolutional layers
    x = x.permute(permute_tuple_reverse)

    for i in range(4):
        x1 = getattr(self, f&#39;w{i}&#39;)(x)
        x2 = getattr(self, f&#39;conv{i}&#39;)(x)
        x = getattr(self, f&#39;bn{i}&#39;)(x1) + getattr(self, f&#39;bn{i}&#39;)(x2)
        x = self.activation()(x)

    x = x.permute(permute_tuple)
    x = self.activation()(self.fc1(x))
    x = self.fc2(x)

    x = x.permute(permute_tuple_reverse)

    return x</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.FNO.get_grid"><code class="name flex">
<span>def <span class="ident">get_grid</span></span>(<span>self, shape, device)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_grid(self, shape, device):
    batch_size = shape[0]
    grid_channel_sizes = shape[2:]  # shape =  (batch_size, channels, *spatial)
    self.grid_channels = {}
    for i in range(self.in_spatial):
        self.grid_channels[f&#39;dim{i}&#39;] = torch.tensor(torch.linspace(0, 1, grid_channel_sizes[i]),
                                                     dtype=torch.float)
        reshape_dim_tuple = [1, 1] + [1 if i != j else grid_channel_sizes[j] for j in range(self.in_spatial)]
        repeat_dim_tuple = [batch_size, 1] + [1 if i == j else grid_channel_sizes[j] for j in
                                              range(self.in_spatial)]
        self.grid_channels[f&#39;dim{i}&#39;] = self.grid_channels[f&#39;dim{i}&#39;].reshape(reshape_dim_tuple) \
            .repeat(repeat_dim_tuple)

    return torch.cat([self.grid_channels[f&#39;dim{i}&#39;] for i in range(self.in_spatial)], dim=1).to(device)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.torch.nets.InvertibleNet"><code class="flex name class">
<span>class <span class="ident">InvertibleNet</span></span>
<span>(</span><span>in_channels, num_blocks, activation, batch_norm, in_spatial, net)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InvertibleNet(nn.Module):
    def __init__(self, in_channels, num_blocks, activation, batch_norm, in_spatial, net):
        super(InvertibleNet, self).__init__()
        self.num_blocks = num_blocks
        for i in range(num_blocks):
            self.add_module(f&#39;coupling_block{i + 1}&#39;,
                            CouplingLayer(in_channels, activation,
                                          batch_norm, in_spatial, net, (i % 2 == 0)))

    def forward(self, x, backward=False):
        if backward:
            for i in range(self.num_blocks, 0, -1):
                x = getattr(self, f&#39;coupling_block{i}&#39;)(x, backward)
        else:
            for i in range(1, self.num_blocks + 1):
                x = getattr(self, f&#39;coupling_block{i}&#39;)(x, backward)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phi.torch.nets.InvertibleNet.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.InvertibleNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.InvertibleNet.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.torch.nets.InvertibleNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x, backward=False) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x, backward=False):
    if backward:
        for i in range(self.num_blocks, 0, -1):
            x = getattr(self, f&#39;coupling_block{i}&#39;)(x, backward)
    else:
        for i in range(1, self.num_blocks + 1):
            x = getattr(self, f&#39;coupling_block{i}&#39;)(x, backward)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.torch.nets.ResNet"><code class="flex name class">
<span>class <span class="ident">ResNet</span></span>
<span>(</span><span>in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ResNet(nn.Module):

    def __init__(self, in_spatial, in_channels, out_channels, layers, batch_norm, activation, periodic: bool):
        super(ResNet, self).__init__()
        self.layers = layers
        if len(self.layers) &lt; 1:
            layers.append(out_channels)
        self.add_module(&#39;Res_in&#39;, resnet_block(in_spatial, in_channels, layers[0], batch_norm, activation, periodic))
        for i in range(1, len(layers)):
            self.add_module(f&#39;Res{i}&#39;, resnet_block(in_spatial, layers[i - 1], layers[i], batch_norm, activation, periodic))
        self.add_module(&#39;Res_out&#39;, CONV[in_spatial](layers[len(layers) - 1], out_channels, kernel_size=1))

    def forward(self, x):
        x = TORCH.as_tensor(x)
        x = getattr(self, &#39;Res_in&#39;)(x)
        for i in range(1, len(self.layers)):
            x = getattr(self, f&#39;Res{i}&#39;)(x)
        x = getattr(self, &#39;Res_out&#39;)(x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phi.torch.nets.ResNet.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.ResNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.ResNet.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.torch.nets.ResNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = TORCH.as_tensor(x)
    x = getattr(self, &#39;Res_in&#39;)(x)
    for i in range(1, len(self.layers)):
        x = getattr(self, f&#39;Res{i}&#39;)(x)
    x = getattr(self, &#39;Res_out&#39;)(x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.torch.nets.SpectralConv"><code class="flex name class">
<span>class <span class="ident">SpectralConv</span></span>
<span>(</span><span>in_channels, out_channels, modes, in_spatial)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SpectralConv(nn.Module):

    def __init__(self, in_channels, out_channels, modes, in_spatial):

        super(SpectralConv, self).__init__()

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.in_spatial = in_spatial
        assert in_spatial &gt;= 1 and in_spatial &lt;= 3
        if isinstance(modes, int):
            mode = modes
            modes = [mode for i in range(in_spatial)]

        self.scale = 1 / (in_channels * out_channels)

        self.modes = {i + 1: modes[i] for i in range(len(modes))}
        self.weights = {}

        rand_shape = [in_channels, out_channels]
        rand_shape += [self.modes[i] for i in range(1, in_spatial + 1)]

        for i in range(2 ** (in_spatial - 1)):
            self.weights[f&#39;w{i + 1}&#39;] = nn.Parameter(self.scale * torch.randn(rand_shape, dtype=torch.cfloat))
            # print(&#39;TORCH self.weights:&#39;, self.weights_[f&#39;w{i + 1}&#39;].shape)
            # print(self.weights[f&#39;w{i + 1}&#39;].shape)

    def complex_mul(self, input, weights):

        # print(input.shape)
        # print(weights.shape)
        # exit(1)
        if self.in_spatial == 1:
            return torch.einsum(&#34;bix,iox-&gt;box&#34;, input, weights)
        elif self.in_spatial == 2:
            return torch.einsum(&#34;bixy,ioxy-&gt;boxy&#34;, input, weights)
        elif self.in_spatial == 3:
            return torch.einsum(&#34;bixyz,ioxyz-&gt;boxyz&#34;, input, weights)

    def forward(self, x):
        batch_size = x.shape[0]

        # print(&#39;x.shape:&#39;, x.shape)
        ##Convert to Fourier space
        dims = [-i for i in range(self.in_spatial, 0, -1)]
        x_ft = torch.fft.rfftn(x, dim=dims)
        # print(&#39;After RFFT torch&#39;, x_ft.shape)
        outft_dims = [batch_size, self.out_channels] + \
                     [x.size(-i) for i in range(self.in_spatial, 1, -1)] + [x.size(-1) // 2 + 1]
        out_ft = torch.zeros(outft_dims, dtype=torch.cfloat, device=x.device)
        # print(&#39;outft shape before&#39;, out_ft.shape)
        ##Multiply relevant fourier modes
        if self.in_spatial == 1:
            out_ft[:, :, :self.modes[1]] = \
                self.complex_mul(x_ft[:, :, :self.modes[1]],
                                 self.weights[&#39;w1&#39;].to(x_ft.device))
        elif self.in_spatial == 2:
            out_ft[:, :, :self.modes[1], :self.modes[2]] = \
                self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2]],
                                 self.weights[&#39;w1&#39;].to(x_ft.device))
            out_ft[:, :, -self.modes[1]:, :self.modes[2]] = \
                self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2]],
                                 self.weights[&#39;w2&#39;].to(x_ft.device))
        elif self.in_spatial == 3:
            out_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]] = \
                self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]],
                                 self.weights[&#39;w1&#39;].to(x_ft.device))
            out_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]] = \
                self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]],
                                 self.weights[&#39;w2&#39;].to(x_ft.device))
            out_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]] = \
                self.complex_mul(x_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]],
                                 self.weights[&#39;w3&#39;].to(x_ft.device))
            out_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]] = \
                self.complex_mul(x_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]],
                                 self.weights[&#39;w4&#39;].to(x_ft.device))

        ##Return to Physical Space
        x = torch.fft.irfftn(out_ft, s=[x.size(-i) for i in range(self.in_spatial, 0, -1)])
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phi.torch.nets.SpectralConv.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.SpectralConv.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.SpectralConv.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.torch.nets.SpectralConv.complex_mul"><code class="name flex">
<span>def <span class="ident">complex_mul</span></span>(<span>self, input, weights)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def complex_mul(self, input, weights):

    # print(input.shape)
    # print(weights.shape)
    # exit(1)
    if self.in_spatial == 1:
        return torch.einsum(&#34;bix,iox-&gt;box&#34;, input, weights)
    elif self.in_spatial == 2:
        return torch.einsum(&#34;bixy,ioxy-&gt;boxy&#34;, input, weights)
    elif self.in_spatial == 3:
        return torch.einsum(&#34;bixyz,ioxyz-&gt;boxyz&#34;, input, weights)</code></pre>
</details>
</dd>
<dt id="phi.torch.nets.SpectralConv.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    batch_size = x.shape[0]

    # print(&#39;x.shape:&#39;, x.shape)
    ##Convert to Fourier space
    dims = [-i for i in range(self.in_spatial, 0, -1)]
    x_ft = torch.fft.rfftn(x, dim=dims)
    # print(&#39;After RFFT torch&#39;, x_ft.shape)
    outft_dims = [batch_size, self.out_channels] + \
                 [x.size(-i) for i in range(self.in_spatial, 1, -1)] + [x.size(-1) // 2 + 1]
    out_ft = torch.zeros(outft_dims, dtype=torch.cfloat, device=x.device)
    # print(&#39;outft shape before&#39;, out_ft.shape)
    ##Multiply relevant fourier modes
    if self.in_spatial == 1:
        out_ft[:, :, :self.modes[1]] = \
            self.complex_mul(x_ft[:, :, :self.modes[1]],
                             self.weights[&#39;w1&#39;].to(x_ft.device))
    elif self.in_spatial == 2:
        out_ft[:, :, :self.modes[1], :self.modes[2]] = \
            self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2]],
                             self.weights[&#39;w1&#39;].to(x_ft.device))
        out_ft[:, :, -self.modes[1]:, :self.modes[2]] = \
            self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2]],
                             self.weights[&#39;w2&#39;].to(x_ft.device))
    elif self.in_spatial == 3:
        out_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]] = \
            self.complex_mul(x_ft[:, :, :self.modes[1], :self.modes[2], :self.modes[3]],
                             self.weights[&#39;w1&#39;].to(x_ft.device))
        out_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]] = \
            self.complex_mul(x_ft[:, :, -self.modes[1]:, :self.modes[2], :self.modes[3]],
                             self.weights[&#39;w2&#39;].to(x_ft.device))
        out_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]] = \
            self.complex_mul(x_ft[:, :, :self.modes[1], -self.modes[2]:, :self.modes[3]],
                             self.weights[&#39;w3&#39;].to(x_ft.device))
        out_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]] = \
            self.complex_mul(x_ft[:, :, -self.modes[1]:, -self.modes[2]:, :self.modes[3]],
                             self.weights[&#39;w4&#39;].to(x_ft.device))

    ##Return to Physical Space
    x = torch.fft.irfftn(out_ft, s=[x.size(-i) for i in range(self.in_spatial, 0, -1)])
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.torch.nets.UNet"><code class="flex name class">
<span>class <span class="ident">UNet</span></span>
<span>(</span><span>d:Â int, in_channels:Â int, out_channels:Â int, filters:Â tuple, batch_norm:Â bool, activation:Â type, periodic:Â bool, use_res_blocks:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UNet(nn.Module):

    def __init__(self, d: int, in_channels: int, out_channels: int, filters: tuple, batch_norm: bool, activation: type, periodic: bool, use_res_blocks: bool):
        super(UNet, self).__init__()
        self._levels = len(filters)
        self._spatial_rank = d
        if use_res_blocks:
            self.add_module(&#39;inc&#39;, resnet_block(d, in_channels, filters[0], batch_norm, activation, periodic))
        else:
            self.add_module(&#39;inc&#39;, DoubleConv(d, in_channels, filters[0], filters[0], batch_norm, activation, periodic))
        for i in range(1, self._levels):
            self.add_module(f&#39;down{i}&#39;, Down(d, filters[i - 1], filters[i], batch_norm, activation, periodic, use_res_blocks))
            self.add_module(f&#39;up{i}&#39;, Up(d, filters[i] + filters[i - 1], filters[i - 1], batch_norm, activation, periodic, use_res_blocks))
        self.add_module(&#39;outc&#39;, CONV[d](filters[0], out_channels, kernel_size=1))

    def forward(self, x):
        register_module_call(self)
        x = TORCH.as_tensor(x)
        x = self.inc(x)
        xs = [x]
        for i in range(1, self._levels):
            x = getattr(self, f&#39;down{i}&#39;)(x)
            xs.insert(0, x)
        for i in range(1, self._levels):
            x = getattr(self, f&#39;up{i}&#39;)(x, xs[i])
        x = self.outc(x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phi.torch.nets.UNet.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.UNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.UNet.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.torch.nets.UNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    register_module_call(self)
    x = TORCH.as_tensor(x)
    x = self.inc(x)
    xs = [x]
    for i in range(1, self._levels):
        x = getattr(self, f&#39;down{i}&#39;)(x)
        xs.insert(0, x)
    for i in range(1, self._levels):
        x = getattr(self, f&#39;up{i}&#39;)(x, xs[i])
    x = self.outc(x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.torch.nets.Up"><code class="flex name class">
<span>class <span class="ident">Up</span></span>
<span>(</span><span>d:Â int, in_channels:Â int, out_channels:Â int, batch_norm:Â bool, activation:Â type, periodic:Â bool, use_res_blocks:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Upscaling then double conv</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Up(nn.Module):
    &#34;&#34;&#34;Upscaling then double conv&#34;&#34;&#34;

    _MODES = [None, &#39;linear&#39;, &#39;bilinear&#39;, &#39;trilinear&#39;]

    def __init__(self, d: int, in_channels: int, out_channels: int, batch_norm: bool, activation: type, periodic: bool, use_res_blocks: bool):
        super().__init__()
        up = nn.Upsample(scale_factor=2, mode=Up._MODES[d])
        if use_res_blocks:
            conv = resnet_block(d, in_channels, out_channels, batch_norm, activation, periodic)
        else:
            conv = DoubleConv(d, in_channels, out_channels, in_channels // 2, batch_norm, activation, periodic)
        self.add_module(&#39;up&#39;, up)
        self.add_module(&#39;conv&#39;, conv)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        # input is CHW
        # diff = [x2.size()[i] - x1.size()[i] for i in range(2, len(x1.shape))]
        # x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
        #                 diffY // 2, diffY - diffY // 2])
        # if you have padding issues, see
        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a
        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phi.torch.nets.Up.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.Up.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.Up.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.torch.nets.Up.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x1, x2) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x1, x2):
    x1 = self.up(x1)
    # input is CHW
    # diff = [x2.size()[i] - x1.size()[i] for i in range(2, len(x1.shape))]
    # x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
    #                 diffY // 2, diffY - diffY // 2])
    # if you have padding issues, see
    # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a
    # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd
    x = torch.cat([x2, x1], dim=1)
    return self.conv(x)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.torch.nets.resnet_block"><code class="flex name class">
<span>class <span class="ident">resnet_block</span></span>
<span>(</span><span>in_spatial, in_channels, out_channels, batch_norm, activation, periodic:Â bool)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class resnet_block(nn.Module):

    def __init__(self, in_spatial, in_channels, out_channels, batch_norm, activation, periodic: bool):
        # Since in_channels and out_channels might be different
        # we need a sampling layer for up/down sampling input
        # in order to add it as a skip connection
        super(resnet_block, self).__init__()
        if in_channels != out_channels:
            self.sample_input = CONV[in_spatial](in_channels, out_channels, kernel_size=1, padding=0)
            self.bn_sample = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
        else:
            self.sample_input = nn.Identity()
            self.bn_sample = nn.Identity()
        self.activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
        self.bn1 = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
        self.conv1 = CONV[in_spatial](in_channels, out_channels, kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;)
        self.bn2 = NORM[in_spatial](out_channels) if batch_norm else nn.Identity()
        self.conv2 = CONV[in_spatial](out_channels, out_channels, kernel_size=3, padding=1, padding_mode=&#39;circular&#39; if periodic else &#39;zeros&#39;)

    def forward(self, x):
        x = TORCH.as_tensor(x)
        out = self.activation()(self.bn1(self.conv1(x)))
        out = self.activation()(self.bn2(self.conv2(out)))
        out = (out + self.bn_sample(self.sample_input(x)))
        return out</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="phi.torch.nets.resnet_block.call_super_init"><code class="name">var <span class="ident">call_super_init</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.resnet_block.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="phi.torch.nets.resnet_block.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.torch.nets.resnet_block.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = TORCH.as_tensor(x)
    out = self.activation()(self.bn1(self.conv1(x)))
    out = self.activation()(self.bn2(self.conv2(out)))
    out = (out + self.bn_sample(self.sample_input(x)))
    return out</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="phi.torch" href="index.html">phi.torch</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="phi.torch.nets.adagrad" href="#phi.torch.nets.adagrad">adagrad</a></code></li>
<li><code><a title="phi.torch.nets.adam" href="#phi.torch.nets.adam">adam</a></code></li>
<li><code><a title="phi.torch.nets.conv_classifier" href="#phi.torch.nets.conv_classifier">conv_classifier</a></code></li>
<li><code><a title="phi.torch.nets.conv_net" href="#phi.torch.nets.conv_net">conv_net</a></code></li>
<li><code><a title="phi.torch.nets.coupling_layer" href="#phi.torch.nets.coupling_layer">coupling_layer</a></code></li>
<li><code><a title="phi.torch.nets.dense_net" href="#phi.torch.nets.dense_net">dense_net</a></code></li>
<li><code><a title="phi.torch.nets.fno" href="#phi.torch.nets.fno">fno</a></code></li>
<li><code><a title="phi.torch.nets.get_mask" href="#phi.torch.nets.get_mask">get_mask</a></code></li>
<li><code><a title="phi.torch.nets.get_parameters" href="#phi.torch.nets.get_parameters">get_parameters</a></code></li>
<li><code><a title="phi.torch.nets.invertible_net" href="#phi.torch.nets.invertible_net">invertible_net</a></code></li>
<li><code><a title="phi.torch.nets.load_state" href="#phi.torch.nets.load_state">load_state</a></code></li>
<li><code><a title="phi.torch.nets.parameter_count" href="#phi.torch.nets.parameter_count">parameter_count</a></code></li>
<li><code><a title="phi.torch.nets.res_net" href="#phi.torch.nets.res_net">res_net</a></code></li>
<li><code><a title="phi.torch.nets.rmsprop" href="#phi.torch.nets.rmsprop">rmsprop</a></code></li>
<li><code><a title="phi.torch.nets.save_state" href="#phi.torch.nets.save_state">save_state</a></code></li>
<li><code><a title="phi.torch.nets.sgd" href="#phi.torch.nets.sgd">sgd</a></code></li>
<li><code><a title="phi.torch.nets.u_net" href="#phi.torch.nets.u_net">u_net</a></code></li>
<li><code><a title="phi.torch.nets.update_weights" href="#phi.torch.nets.update_weights">update_weights</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="phi.torch.nets.ConvClassifier" href="#phi.torch.nets.ConvClassifier">ConvClassifier</a></code></h4>
<ul class="">
<li><code><a title="phi.torch.nets.ConvClassifier.call_super_init" href="#phi.torch.nets.ConvClassifier.call_super_init">call_super_init</a></code></li>
<li><code><a title="phi.torch.nets.ConvClassifier.dump_patches" href="#phi.torch.nets.ConvClassifier.dump_patches">dump_patches</a></code></li>
<li><code><a title="phi.torch.nets.ConvClassifier.forward" href="#phi.torch.nets.ConvClassifier.forward">forward</a></code></li>
<li><code><a title="phi.torch.nets.ConvClassifier.training" href="#phi.torch.nets.ConvClassifier.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.torch.nets.ConvNet" href="#phi.torch.nets.ConvNet">ConvNet</a></code></h4>
<ul class="">
<li><code><a title="phi.torch.nets.ConvNet.call_super_init" href="#phi.torch.nets.ConvNet.call_super_init">call_super_init</a></code></li>
<li><code><a title="phi.torch.nets.ConvNet.dump_patches" href="#phi.torch.nets.ConvNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="phi.torch.nets.ConvNet.forward" href="#phi.torch.nets.ConvNet.forward">forward</a></code></li>
<li><code><a title="phi.torch.nets.ConvNet.training" href="#phi.torch.nets.ConvNet.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.torch.nets.CouplingLayer" href="#phi.torch.nets.CouplingLayer">CouplingLayer</a></code></h4>
<ul class="">
<li><code><a title="phi.torch.nets.CouplingLayer.call_super_init" href="#phi.torch.nets.CouplingLayer.call_super_init">call_super_init</a></code></li>
<li><code><a title="phi.torch.nets.CouplingLayer.dump_patches" href="#phi.torch.nets.CouplingLayer.dump_patches">dump_patches</a></code></li>
<li><code><a title="phi.torch.nets.CouplingLayer.forward" href="#phi.torch.nets.CouplingLayer.forward">forward</a></code></li>
<li><code><a title="phi.torch.nets.CouplingLayer.training" href="#phi.torch.nets.CouplingLayer.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.torch.nets.DenseNet" href="#phi.torch.nets.DenseNet">DenseNet</a></code></h4>
<ul class="">
<li><code><a title="phi.torch.nets.DenseNet.call_super_init" href="#phi.torch.nets.DenseNet.call_super_init">call_super_init</a></code></li>
<li><code><a title="phi.torch.nets.DenseNet.dump_patches" href="#phi.torch.nets.DenseNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="phi.torch.nets.DenseNet.forward" href="#phi.torch.nets.DenseNet.forward">forward</a></code></li>
<li><code><a title="phi.torch.nets.DenseNet.training" href="#phi.torch.nets.DenseNet.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.torch.nets.Dense_resnet_block" href="#phi.torch.nets.Dense_resnet_block">Dense_resnet_block</a></code></h4>
<ul class="">
<li><code><a title="phi.torch.nets.Dense_resnet_block.call_super_init" href="#phi.torch.nets.Dense_resnet_block.call_super_init">call_super_init</a></code></li>
<li><code><a title="phi.torch.nets.Dense_resnet_block.dump_patches" href="#phi.torch.nets.Dense_resnet_block.dump_patches">dump_patches</a></code></li>
<li><code><a title="phi.torch.nets.Dense_resnet_block.forward" href="#phi.torch.nets.Dense_resnet_block.forward">forward</a></code></li>
<li><code><a title="phi.torch.nets.Dense_resnet_block.training" href="#phi.torch.nets.Dense_resnet_block.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.torch.nets.DoubleConv" href="#phi.torch.nets.DoubleConv">DoubleConv</a></code></h4>
<ul class="">
<li><code><a title="phi.torch.nets.DoubleConv.call_super_init" href="#phi.torch.nets.DoubleConv.call_super_init">call_super_init</a></code></li>
<li><code><a title="phi.torch.nets.DoubleConv.dump_patches" href="#phi.torch.nets.DoubleConv.dump_patches">dump_patches</a></code></li>
<li><code><a title="phi.torch.nets.DoubleConv.forward" href="#phi.torch.nets.DoubleConv.forward">forward</a></code></li>
<li><code><a title="phi.torch.nets.DoubleConv.training" href="#phi.torch.nets.DoubleConv.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.torch.nets.Down" href="#phi.torch.nets.Down">Down</a></code></h4>
<ul class="">
<li><code><a title="phi.torch.nets.Down.call_super_init" href="#phi.torch.nets.Down.call_super_init">call_super_init</a></code></li>
<li><code><a title="phi.torch.nets.Down.dump_patches" href="#phi.torch.nets.Down.dump_patches">dump_patches</a></code></li>
<li><code><a title="phi.torch.nets.Down.forward" href="#phi.torch.nets.Down.forward">forward</a></code></li>
<li><code><a title="phi.torch.nets.Down.training" href="#phi.torch.nets.Down.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.torch.nets.FNO" href="#phi.torch.nets.FNO">FNO</a></code></h4>
<ul class="">
<li><code><a title="phi.torch.nets.FNO.call_super_init" href="#phi.torch.nets.FNO.call_super_init">call_super_init</a></code></li>
<li><code><a title="phi.torch.nets.FNO.dump_patches" href="#phi.torch.nets.FNO.dump_patches">dump_patches</a></code></li>
<li><code><a title="phi.torch.nets.FNO.forward" href="#phi.torch.nets.FNO.forward">forward</a></code></li>
<li><code><a title="phi.torch.nets.FNO.get_grid" href="#phi.torch.nets.FNO.get_grid">get_grid</a></code></li>
<li><code><a title="phi.torch.nets.FNO.training" href="#phi.torch.nets.FNO.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.torch.nets.InvertibleNet" href="#phi.torch.nets.InvertibleNet">InvertibleNet</a></code></h4>
<ul class="">
<li><code><a title="phi.torch.nets.InvertibleNet.call_super_init" href="#phi.torch.nets.InvertibleNet.call_super_init">call_super_init</a></code></li>
<li><code><a title="phi.torch.nets.InvertibleNet.dump_patches" href="#phi.torch.nets.InvertibleNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="phi.torch.nets.InvertibleNet.forward" href="#phi.torch.nets.InvertibleNet.forward">forward</a></code></li>
<li><code><a title="phi.torch.nets.InvertibleNet.training" href="#phi.torch.nets.InvertibleNet.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.torch.nets.ResNet" href="#phi.torch.nets.ResNet">ResNet</a></code></h4>
<ul class="">
<li><code><a title="phi.torch.nets.ResNet.call_super_init" href="#phi.torch.nets.ResNet.call_super_init">call_super_init</a></code></li>
<li><code><a title="phi.torch.nets.ResNet.dump_patches" href="#phi.torch.nets.ResNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="phi.torch.nets.ResNet.forward" href="#phi.torch.nets.ResNet.forward">forward</a></code></li>
<li><code><a title="phi.torch.nets.ResNet.training" href="#phi.torch.nets.ResNet.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.torch.nets.SpectralConv" href="#phi.torch.nets.SpectralConv">SpectralConv</a></code></h4>
<ul class="">
<li><code><a title="phi.torch.nets.SpectralConv.call_super_init" href="#phi.torch.nets.SpectralConv.call_super_init">call_super_init</a></code></li>
<li><code><a title="phi.torch.nets.SpectralConv.complex_mul" href="#phi.torch.nets.SpectralConv.complex_mul">complex_mul</a></code></li>
<li><code><a title="phi.torch.nets.SpectralConv.dump_patches" href="#phi.torch.nets.SpectralConv.dump_patches">dump_patches</a></code></li>
<li><code><a title="phi.torch.nets.SpectralConv.forward" href="#phi.torch.nets.SpectralConv.forward">forward</a></code></li>
<li><code><a title="phi.torch.nets.SpectralConv.training" href="#phi.torch.nets.SpectralConv.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.torch.nets.UNet" href="#phi.torch.nets.UNet">UNet</a></code></h4>
<ul class="">
<li><code><a title="phi.torch.nets.UNet.call_super_init" href="#phi.torch.nets.UNet.call_super_init">call_super_init</a></code></li>
<li><code><a title="phi.torch.nets.UNet.dump_patches" href="#phi.torch.nets.UNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="phi.torch.nets.UNet.forward" href="#phi.torch.nets.UNet.forward">forward</a></code></li>
<li><code><a title="phi.torch.nets.UNet.training" href="#phi.torch.nets.UNet.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.torch.nets.Up" href="#phi.torch.nets.Up">Up</a></code></h4>
<ul class="">
<li><code><a title="phi.torch.nets.Up.call_super_init" href="#phi.torch.nets.Up.call_super_init">call_super_init</a></code></li>
<li><code><a title="phi.torch.nets.Up.dump_patches" href="#phi.torch.nets.Up.dump_patches">dump_patches</a></code></li>
<li><code><a title="phi.torch.nets.Up.forward" href="#phi.torch.nets.Up.forward">forward</a></code></li>
<li><code><a title="phi.torch.nets.Up.training" href="#phi.torch.nets.Up.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.torch.nets.resnet_block" href="#phi.torch.nets.resnet_block">resnet_block</a></code></h4>
<ul class="">
<li><code><a title="phi.torch.nets.resnet_block.call_super_init" href="#phi.torch.nets.resnet_block.call_super_init">call_super_init</a></code></li>
<li><code><a title="phi.torch.nets.resnet_block.dump_patches" href="#phi.torch.nets.resnet_block.dump_patches">dump_patches</a></code></li>
<li><code><a title="phi.torch.nets.resnet_block.forward" href="#phi.torch.nets.resnet_block.forward">forward</a></code></li>
<li><code><a title="phi.torch.nets.resnet_block.training" href="#phi.torch.nets.resnet_block.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>